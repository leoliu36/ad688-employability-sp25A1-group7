[
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Conclusion",
    "section": "",
    "text": "The job market in 2024 is undergoing a major transformation with the rise of artificial intelligence and widespread acceptance of long-distance/remote work. This analysis will examine the following key topics to uncover the factors that influence salary trends:\nThe restructuring of compensation due to AI, inflation, and remote work.\nThe traditional pay structure is being reshaped as companies adapt to automation, rising costs of living, and a distributed workforce. These forces are pushing employers to rethink how they reward skills, experience, and location.\nGrowing pay disparities across regions, industries, and job types in 2024.\nHigh-paying jobs are increasingly concentrated in specific tech-driven sectors and urban hubs. Meanwhile, many essential or service-based roles are seeing slower wage growth, deepening inequality across the workforce.\nShifting salary patterns based on remote flexibility, job type, and sector growth.\nRemote roles now often offer comparable or even higher compensation due to talent shortages and broader applicant pools. Industries like tech and healthcare are setting new standards, while others struggle to keep pace."
  },
  {
    "objectID": "final_report.html#introduction-research-rationale",
    "href": "final_report.html#introduction-research-rationale",
    "title": "Conclusion",
    "section": "",
    "text": "The job market in 2024 is undergoing a major transformation with the rise of artificial intelligence and widespread acceptance of long-distance/remote work. This analysis will examine the following key topics to uncover the factors that influence salary trends:\nThe restructuring of compensation due to AI, inflation, and remote work.\nThe traditional pay structure is being reshaped as companies adapt to automation, rising costs of living, and a distributed workforce. These forces are pushing employers to rethink how they reward skills, experience, and location.\nGrowing pay disparities across regions, industries, and job types in 2024.\nHigh-paying jobs are increasingly concentrated in specific tech-driven sectors and urban hubs. Meanwhile, many essential or service-based roles are seeing slower wage growth, deepening inequality across the workforce.\nShifting salary patterns based on remote flexibility, job type, and sector growth.\nRemote roles now often offer comparable or even higher compensation due to talent shortages and broader applicant pools. Industries like tech and healthcare are setting new standards, while others struggle to keep pace."
  },
  {
    "objectID": "final_report.html#literature-review-summary",
    "href": "final_report.html#literature-review-summary",
    "title": "Conclusion",
    "section": "0.2 Literature Review Summary",
    "text": "0.2 Literature Review Summary\nThe rise of Artificial Intelligence (AI) has reshaped the job market across the US, with demand for AI-related skills increasing dramatically. According to a study by PWC, jobs that require AI specialist skills are growing 3.5 times faster than all other job markets, with skilled AI workers being paid up to 25% more in some sectors (PwC (2024)). However, this increase in compensation is not limited to workers specialized in AI. Non-AI roles that require complementary skills such as digital literacy, analytical thinking, and teamwork are also seeing a 5–10% wage increase (Mäkelä and Stephany (2024)). According to US job vacancy data from 2018–2024, AI-related jobs are significantly more likely to include non-monetary benefits as part of the compensation package, including parental leave and remote working options (Stephany, Mira, and Bone (2025)).\nBeyond the benefit of not having to commute and the ability to work from anywhere, long-distance and remote roles are often sought after by job seekers due to faster wage growth. In a study comparing the pay trends of remote versus in-office workers in the same occupation, remote workers experienced 4.4% faster annual wage growth, especially in professional and technical sectors (Pabilonia and Vernon (2025)). Furthermore, workers who transitioned into remote roles with the same employer saw up to 16 percentage points higher wage growth than their counterparts who remained local (Romem (2024)). This demonstrates that switching into remote work can lead to significantly higher wage growth, even within the same company and/or job category."
  },
  {
    "objectID": "final_report.html#references",
    "href": "final_report.html#references",
    "title": "Conclusion",
    "section": "0.3 References",
    "text": "0.3 References\n\ntitle: “Data Cleaning & Exploration” execute: kernel: ad688-venv echo: false\nwarning: false\nmessage: false\nauthor:\n- name: Angelina McKim affiliations: - id: bu name: Boston University city: Boston state: MA - name: Devin Blanchard affiliations: - ref: bu - name: Leo Liu affiliations: - ref: bu date: 2025-09-10 date-modified: 2025-10-10 bibliography: references.bib csl: csl/econometrica.csl format: html: toc: true number-sections: true df-print: paged —"
  },
  {
    "objectID": "final_report.html#remove-unnecessary-columns",
    "href": "final_report.html#remove-unnecessary-columns",
    "title": "Conclusion",
    "section": "1.1 Remove Unnecessary Columns",
    "text": "1.1 Remove Unnecessary Columns\nFirstly, columns containing redundant and irrelevant information were excluded from the dataset. Since the scope of this analysis is focused on job market trends in 2024, it is best practice to remove any outdated NAICS/SOC fields to prevent confusion and duplication. Similarly, metadata fields or duplicate fields that could introduce ambiguity and do not add any meaningful to downstream analysis are excluded. To summarize, unnecessary columns containing the following information were dropped:\n\nMeta/tracking\nDuplicated location info\nRaw/duplicate title & body\nDuplicated employment info\nEducation code columns\nRedundant NAICS/SOC versions\nLOT/V6 occupation hierarchy\nONET & CIP codes\nSectors"
  },
  {
    "objectID": "final_report.html#handling-of-missing-values",
    "href": "final_report.html#handling-of-missing-values",
    "title": "Conclusion",
    "section": "1.2 Handling of missing values",
    "text": "1.2 Handling of missing values\nWe addressed missing values based on field type and the amount of data missing per field:\n\n1.2.1 Numerical Fields\nMissing values in key numerical fields were imputed with the median of each respective fields. Since these numerical fields are key for grouping and visualizing trends, simply removing empty rows could reduce the diversity of the dataset and introduce biases. The rationale for imputing the median rather than the mean is that the latter tends to be influenced by outliers and skewed distributions, which is often exhibited in fields like “SALARY. Based on the downstream EDA, additional filtering may be applied to exclude imputed values altogether to prevent distorition and ensure statistical integrity in the trends observed.\n\n\n1.2.2 Categorical Fields\nMissing values in key categorical fields were imputed with the placeholder value “Unknown” in order to prevent those rows of data being dropped, leading to unnecessary data loss. By imputing a neutral label like “Unknown”, this strategy ensures that data integrity is retained without introducing false assumptions and biases into the downstream analysis.\n\n\n1.2.3 Columns containing Majority Missing Data\nWhile the above addressed the rationale for imputing missing values, having to impute the majority of a column’s values can also create noise, which provides insignificant information and analytical value to the downstream analysis. Therefore, columns containing more than 50% missing values were excluded from the dataset.\n\n\n\nNon-null Data"
  },
  {
    "objectID": "final_report.html#remove-duplicates",
    "href": "final_report.html#remove-duplicates",
    "title": "Conclusion",
    "section": "1.3 Remove Duplicates",
    "text": "1.3 Remove Duplicates\nTo eliminate true duplicates from the dataset, job listings that have identical values in all the fields listed below were removed to prevent distortion and ensure statistical integrity:\n\nJob Title\nCompany Name\nLocation\nPosting Date\nSkill requirements\nEmployment type\n\n\n\n\n\n\n\n\n\n\n                         Column  Missing %\n           MAX_YEARS_EXPERIENCE  88.372093\n             MAX_EDULEVELS_NAME  77.495931\n                         SALARY  57.505035\n                      SALARY_TO  55.311871\n            ORIGINAL_PAY_PERIOD  55.311871\n                    SALARY_FROM  55.311871\n                       DURATION  37.678281\n           MIN_YEARS_EXPERIENCE  31.926398\n               MODELED_DURATION  26.607631\n                MODELED_EXPIRED  21.264035\n                        EXPIRED  10.819609\n                       MSA_NAME   5.447047\n                    COMPANY_RAW   0.746227\n                    TITLE_CLEAN   0.193109\n                     SOC_5_NAME   0.060691\n             SPECIALIZED_SKILLS   0.060691\n        SPECIALIZED_SKILLS_NAME   0.060691\n                 CERTIFICATIONS   0.060691\nLOT_SPECIALIZED_OCCUPATION_NAME   0.060691\n            CERTIFICATIONS_NAME   0.060691\n     LOT_SPECIALIZED_OCCUPATION   0.060691\n             COMMON_SKILLS_NAME   0.060691\n                    SKILLS_NAME   0.060691\n                SOFTWARE_SKILLS   0.060691\n           SOFTWARE_SKILLS_NAME   0.060691\n                  COMMON_SKILLS   0.060691\n              NAICS_2022_6_NAME   0.060691\n                         SKILLS   0.060691\n                          TITLE   0.060691\n                     STATE_NAME   0.060691\n                      CITY_NAME   0.060691\n               REMOTE_TYPE_NAME   0.060691\n                    REMOTE_TYPE   0.060691\n                  IS_INTERNSHIP   0.060691\n             MIN_EDULEVELS_NAME   0.060691\n            COMPANY_IS_STAFFING   0.060691\n                   COMPANY_NAME   0.060691\n                        COMPANY   0.060691\n                           BODY   0.060691\n                         POSTED   0.030346\n\n\n\n\n(55656, 33)\n\n\n\n{{&lt; embed skill_gap_analysis.ipynb &gt;}}"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Methodology",
    "section": "",
    "text": "Below are the steps we took to prepare the dataset to ensure accuracy and statistical integrity in our exploratory data analysis (EDA)."
  },
  {
    "objectID": "data_cleaning.html#remove-unnecessary-columns",
    "href": "data_cleaning.html#remove-unnecessary-columns",
    "title": "Methodology",
    "section": "1 Remove Unnecessary Columns",
    "text": "1 Remove Unnecessary Columns\nFirstly, columns containing redundant and irrelevant information were excluded from the dataset. Since the scope of this analysis is focused on job market trends in 2024, it is best practice to remove any outdated NAICS/SOC fields to prevent confusion and duplication. Similarly, metadata fields or duplicate fields that could introduce ambiguity and do not add any meaningful to downstream analysis are excluded. To summarize, unnecessary columns containing the following information were dropped:\n\nMeta/tracking\nDuplicated location info\nRaw/duplicate title & body\nDuplicated employment info\nEducation code columns\nRedundant NAICS/SOC versions\nLOT/V6 occupation hierarchy\nONET & CIP codes\nSectors"
  },
  {
    "objectID": "data_cleaning.html#handling-of-missing-values",
    "href": "data_cleaning.html#handling-of-missing-values",
    "title": "Methodology",
    "section": "2 Handling of missing values",
    "text": "2 Handling of missing values\nWe addressed missing values based on field type and the amount of data missing per field:\n\n2.1 Numerical Fields\nMissing values in key numerical fields were imputed with the median of each respective fields. Since these numerical fields are key for grouping and visualizing trends, simply removing empty rows could reduce the diversity of the dataset and introduce biases. The rationale for imputing the median rather than the mean is that the latter tends to be influenced by outliers and skewed distributions, which is often exhibited in fields like “SALARY. Based on the downstream EDA, additional filtering may be applied to exclude imputed values altogether to prevent distorition and ensure statistical integrity in the trends observed.\n\n\n2.2 Categorical Fields\nMissing values in key categorical fields were imputed with the placeholder value “Unknown” in order to prevent those rows of data being dropped, leading to unnecessary data loss. By imputing a neutral label like “Unknown”, this strategy ensures that data integrity is retained without introducing false assumptions and biases into the downstream analysis.\n\n\n2.3 Columns containing Majority Missing Data\nWhile the above addressed the rationale for imputing missing values, having to impute the majority of a column’s values can also create noise, which provides insignificant information and analytical value to the downstream analysis. Therefore, columns containing more than 50% missing values were excluded from the dataset.\n\n\n\nFigure 1: Non-null Data\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load lightcast_job_postings.csv \ndf = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\n# Show the first 5 rows \n#print(df.head(5).to_string())\n\n# Drop columns\ncolumns_to_drop = [\n    # Meta/tracking\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\", \"URL\",\n    \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Duplicated location info\n    \"LOCATION\", \"CITY\", \"STATE\", \"COUNTY\", \"COUNTY_NAME\",\n    \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_NAME_OUTGOING\", \"MSA_INCOMING\", \"MSA_NAME_INCOMING\",\n\n    # Raw/duplicate title & body\n    \"TITLE_RAW\", \"TITLE_NAME\",\n\n    # Duplicated employment info\n    \"EMPLOYMENT_TYPE\", \"EMPLOYMENT_TYPE_NAME\",\n\n    # Education code columns\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\", \"MIN_EDULEVELS\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS/SOC versions\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\",\n    \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_4\", \"SOC_4_NAME\",\n    \"SOC_5\",  # keep SOC_5_NAME, drop code\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\",\n    \"SOC_2021_4\", \"SOC_2021_4_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n\n    # LOT/V6 occupation hierarchy (keep only 1 specialized name field)\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n\n    # ONET & CIP codes \n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"CIP2\", \"CIP2_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP6\", \"CIP6_NAME\",\n\n    # Sectors\n    \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\",\n\n    # NAICS 2022 lower-level codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"NAICS_2022_4\", \"NAICS_2022_4_NAME\", \"NAICS_2022_5\", \"NAICS_2022_5_NAME\",\n    \"NAICS_2022_6\",  # drop code, keep name\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n#df.info()\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Identify columns that have a significant amount of missing values and sort df by the percentage of missing values\nmissing_percent = df.isnull().mean().sort_values(ascending=False)*100\ndf_sorted = df[missing_percent.index]\n\n# Visualize missing data using missingno bar chart \nplt.figure(figsize=(12, 6))\nmsno.bar(df_sorted)\nplt.title(\"Non-null Data Bar Chart\")\nplt.tight_layout()\nout_path = \"figures/non-null_data.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\nmissing_values_pct = (missing_percent.reset_index().rename(columns={\"index\": \"Column\", 0: \"Missing %\"}))\nprint(missing_values_pct.to_string(index=False))\n\n\n                         Column  Missing %\n           MAX_YEARS_EXPERIENCE  88.372093\n             MAX_EDULEVELS_NAME  77.495931\n                         SALARY  57.505035\n                      SALARY_TO  55.311871\n            ORIGINAL_PAY_PERIOD  55.311871\n                    SALARY_FROM  55.311871\n                       DURATION  37.678281\n           MIN_YEARS_EXPERIENCE  31.926398\n               MODELED_DURATION  26.607631\n                MODELED_EXPIRED  21.264035\n                        EXPIRED  10.819609\n                       MSA_NAME   5.447047\n                    COMPANY_RAW   0.746227\n                    TITLE_CLEAN   0.193109\n                     SOC_5_NAME   0.060691\n             SPECIALIZED_SKILLS   0.060691\n        SPECIALIZED_SKILLS_NAME   0.060691\n                 CERTIFICATIONS   0.060691\nLOT_SPECIALIZED_OCCUPATION_NAME   0.060691\n            CERTIFICATIONS_NAME   0.060691\n     LOT_SPECIALIZED_OCCUPATION   0.060691\n             COMMON_SKILLS_NAME   0.060691\n                    SKILLS_NAME   0.060691\n                SOFTWARE_SKILLS   0.060691\n           SOFTWARE_SKILLS_NAME   0.060691\n                  COMMON_SKILLS   0.060691\n              NAICS_2022_6_NAME   0.060691\n                         SKILLS   0.060691\n                          TITLE   0.060691\n                     STATE_NAME   0.060691\n                      CITY_NAME   0.060691\n               REMOTE_TYPE_NAME   0.060691\n                    REMOTE_TYPE   0.060691\n                  IS_INTERNSHIP   0.060691\n             MIN_EDULEVELS_NAME   0.060691\n            COMPANY_IS_STAFFING   0.060691\n                   COMPANY_NAME   0.060691\n                        COMPANY   0.060691\n                           BODY   0.060691\n                         POSTED   0.030346"
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Methodology",
    "section": "3 Remove Duplicates",
    "text": "3 Remove Duplicates\nTo eliminate true duplicates from the dataset, job listings that have identical values in all the fields listed below were removed to prevent distortion and ensure statistical integrity:\n\nJob Title\nCompany Name\nLocation\nPosting Date\nSkill requirements\nEmployment type\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ncols_to_drop_missing = [\n    \"MAX_YEARS_EXPERIENCE\",\n    \"MAX_EDULEVELS_NAME\",\n    \"SALARY_FROM\",\n    \"SALARY_TO\",\n    \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_DURATION\",\n    \"MODELED_EXPIRED\",\n    \"EXPIRED\"\n]\ndf.drop(columns=cols_to_drop_missing, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\nfill_col_unk = [\n    # Company info\n    \"COMPANY_NAME\", \"COMPANY_IS_STAFFING\",\n    \n    # Job titles\n    \"TITLE\", \"TITLE_CLEAN\",\n    \n    # Occupation/industry (kept name fields)\n    \"SOC_5_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \"NAICS_2022_6_NAME\",\n    \n    # Remote type\n    \"REMOTE_TYPE_NAME\",\n    \n    # Education level (names, not codes)\n    \"MIN_EDULEVELS_NAME\", \n    \n    # Location info\n    \"STATE_NAME\", \"CITY_NAME\", \"MSA_NAME\",\n\n    # Skills/certifications (optional — only if you plan to analyze skills)\n    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \"COMMON_SKILLS_NAME\", \"CERTIFICATIONS_NAME\"\n]\n\n# Loop through and fill missing values\nfor col in fill_col_unk:\n    df[col] = df[col].fillna(\"Unknown\")\n\n# Create a cleaned version for SALARY with median imputation\ndf[\"SALARY_CLEANED\"] = df[\"SALARY\"].copy()\nmedian_salary = df[\"SALARY\"].median()\ndf[\"SALARY_CLEANED\"] = df[\"SALARY_CLEANED\"].fillna(median_salary)\n\n# Remove duplicate\ndf=df.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"POSTED\", \"REMOTE_TYPE_NAME\", \"SKILLS_NAME\"], keep=\"first\")\n\n# Preview new df\ndf.shape\n\nimport os\n# Ensure the data directory exists\nos.makedirs(\"data\", exist_ok=True)\n\n# Export cleaned dataset for reuse in other analyses\noutput_path = \"data/cleaned_job_postings.csv\"\ndf.to_csv(output_path, index=False)\n\n#print(f\"Cleaned dataset saved successfully: {output_path}\")\n#qprint(f\"Total rows: {len(df):,}, columns: {len(df.columns)}\")"
  },
  {
    "objectID": "data_analysis.html#each-file-that-contributes-to-the-overall-quarto-website-will-contain-the-text-and-visual-outputs-only.",
    "href": "data_analysis.html#each-file-that-contributes-to-the-overall-quarto-website-will-contain-the-text-and-visual-outputs-only.",
    "title": "Project Working File",
    "section": "0.2 Each file that contributes to the overall quarto website will contain the text and visual outputs only.",
    "text": "0.2 Each file that contributes to the overall quarto website will contain the text and visual outputs only.\n\n0.2.1 Code block for data_cleaning.qmd:\n\n\nCode\n# ADDING IN MORE LIBRARIES\n# -------------------------------------------------------------------\n# Global Plotly Styling — applies to all visualizations in this report\n# -------------------------------------------------------------------\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# pio.templates.default = \"plotly_white\"\n# pio.templates[\"plotly_white\"].layout.font.family = \"Arial\"\n# pio.templates[\"plotly_white\"].layout.font.size = 14\n# pio.templates[\"plotly_white\"].layout.title.font.size = 18\n\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\", color=\"#000000\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2,\n        xaxis=dict(\n            showgrid=False,\n            gridcolor=\"#c0c0c0\",  # same as Matplotlib grid\n            zeroline=False,\n            linecolor=\"black\",\n            mirror=True,\n            ticks=\"outside\",\n            title_font=dict(size=13, family=\"Arial\"),\n            tickfont=dict(size=11, family=\"Arial\")\n        ),\n        yaxis=dict(\n            showgrid=True,\n            gridcolor=\"#c0c0c0\",\n            zeroline=False,\n            linecolor=\"black\",\n            mirror=True,\n            ticks=\"outside\",\n            title_font=dict(size=13, family=\"Arial\"),\n            tickfont=dict(size=11, family=\"Arial\")\n        )\n    )\n)\n\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-v0_8-whitegrid\") \nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"axes.edgecolor\": \"black\",\n    \"axes.linewidth\": 0.8,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150,\n    \"axes.grid\": True,\n    \"grid.color\": \"#c0c0c0\"\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load lightcast_job_postings.csv \ndf = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\n# Show the first 5 rows \n#print(df.head(5).to_string())\n\n\n\n\nCode\n# Drop columns\ncolumns_to_drop = [\n    # Meta/tracking\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\", \"URL\",\n    \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Duplicated location info\n    \"LOCATION\", \"CITY\", \"STATE\", \"COUNTY\", \"COUNTY_NAME\",\n    \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_NAME_OUTGOING\", \"MSA_INCOMING\", \"MSA_NAME_INCOMING\",\n\n    # Raw/duplicate title & body\n    \"TITLE_RAW\", \"TITLE_NAME\",\n\n    # Duplicated employment info\n    \"EMPLOYMENT_TYPE\", \"EMPLOYMENT_TYPE_NAME\",\n\n    # Education code columns\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\", \"MIN_EDULEVELS\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS/SOC versions\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\",\n    \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_4\", \"SOC_4_NAME\",\n    \"SOC_5\",  # keep SOC_5_NAME, drop code\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\",\n    \"SOC_2021_4\", \"SOC_2021_4_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n\n    # LOT/V6 occupation hierarchy (keep only 1 specialized name field)\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n\n    # ONET & CIP codes \n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"CIP2\", \"CIP2_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP6\", \"CIP6_NAME\",\n\n    # Sectors\n    \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\",\n\n    # NAICS 2022 lower-level codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"NAICS_2022_4\", \"NAICS_2022_4_NAME\", \"NAICS_2022_5\", \"NAICS_2022_5_NAME\",\n    \"NAICS_2022_6\",  # drop code, keep name\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n#df.info()\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Identify columns that have a significant amount of missing values and sort df by the percentage of missing values\nmissing_percent = df.isnull().mean().sort_values(ascending=False)*100\ndf_sorted = df[missing_percent.index]\n\n# Visualize missing data using missingno bar chart \nplt.figure(figsize=(12, 6))\nmsno.bar(df_sorted)\nplt.title(\"Non-null Data Bar Chart\")\nplt.tight_layout()\nout_path = \"figures/non-null_data.png\"\nplt.savefig(out_path, dpi=150)\nplt.show()\n\nmissing_values_pct = (missing_percent.reset_index().rename(columns={\"index\": \"Column\", 0: \"Missing %\"}))\nprint(missing_values_pct.to_string(index=False))\n\n\n\n\n\n\n\n\n\n                         Column  Missing %\n           MAX_YEARS_EXPERIENCE  88.372093\n             MAX_EDULEVELS_NAME  77.495931\n                         SALARY  57.505035\n                      SALARY_TO  55.311871\n            ORIGINAL_PAY_PERIOD  55.311871\n                    SALARY_FROM  55.311871\n                       DURATION  37.678281\n           MIN_YEARS_EXPERIENCE  31.926398\n               MODELED_DURATION  26.607631\n                MODELED_EXPIRED  21.264035\n                        EXPIRED  10.819609\n                       MSA_NAME   5.447047\n                    COMPANY_RAW   0.746227\n                    TITLE_CLEAN   0.193109\n                     SOC_5_NAME   0.060691\n             SPECIALIZED_SKILLS   0.060691\n        SPECIALIZED_SKILLS_NAME   0.060691\n                 CERTIFICATIONS   0.060691\nLOT_SPECIALIZED_OCCUPATION_NAME   0.060691\n            CERTIFICATIONS_NAME   0.060691\n     LOT_SPECIALIZED_OCCUPATION   0.060691\n             COMMON_SKILLS_NAME   0.060691\n                    SKILLS_NAME   0.060691\n                SOFTWARE_SKILLS   0.060691\n           SOFTWARE_SKILLS_NAME   0.060691\n                  COMMON_SKILLS   0.060691\n              NAICS_2022_6_NAME   0.060691\n                         SKILLS   0.060691\n                          TITLE   0.060691\n                     STATE_NAME   0.060691\n                      CITY_NAME   0.060691\n               REMOTE_TYPE_NAME   0.060691\n                    REMOTE_TYPE   0.060691\n                  IS_INTERNSHIP   0.060691\n             MIN_EDULEVELS_NAME   0.060691\n            COMPANY_IS_STAFFING   0.060691\n                   COMPANY_NAME   0.060691\n                        COMPANY   0.060691\n                           BODY   0.060691\n                         POSTED   0.030346\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ncols_to_drop_missing = [\n    \"MAX_YEARS_EXPERIENCE\",\n    \"MAX_EDULEVELS_NAME\",\n    \"SALARY_FROM\",\n    \"SALARY_TO\",\n    \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_DURATION\",\n    \"MODELED_EXPIRED\",\n    \"EXPIRED\"\n]\ndf.drop(columns=cols_to_drop_missing, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\nfill_col_unk = [\n    # Company info\n    \"COMPANY_NAME\", \"COMPANY_IS_STAFFING\",\n    \n    # Job titles\n    \"TITLE\", \"TITLE_CLEAN\",\n    \n    # Occupation/industry (kept name fields)\n    \"SOC_5_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \"NAICS_2022_6_NAME\",\n    \n    # Remote type\n    \"REMOTE_TYPE_NAME\",\n    \n    # Education level (names, not codes)\n    \"MIN_EDULEVELS_NAME\", \n    \n    # Location info\n    \"STATE_NAME\", \"CITY_NAME\", \"MSA_NAME\",\n    \n    # Skills/certifications (optional — only if you plan to analyze skills)\n    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \"COMMON_SKILLS_NAME\", \"CERTIFICATIONS_NAME\"\n]\n\n# Loop through and fill missing values\nfor col in fill_col_unk:\n    df[col] = df[col].fillna(\"Unknown\")\n\n# Create a cleaned version for SALARY with median imputation\ndf[\"SALARY_CLEANED\"] = df[\"SALARY\"].copy()\nmedian_salary = df[\"SALARY\"].median()\ndf[\"SALARY_CLEANED\"] = df[\"SALARY_CLEANED\"].fillna(median_salary)\n\n# Drop columns with &gt;50% missing values\n#df.info()\n\n\n\n\nCode\n# Remove duplicate\ndf=df.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"POSTED\", \"REMOTE_TYPE_NAME\", \"SKILLS_NAME\"], keep=\"first\")\n\n# Preview new df\ndf.shape\n\n\n(55656, 33)\n\n\n\n\n0.2.2 Code block for eda.qmd:\n\n0.2.2.1 5.1.1 Salary by Remote Work Type\n\n\nCode\n# 5.1.1 Visual - Compensation\nimport plotly.express as px\nimport pandas as pd\n\nvalues_to_exclude = ['Unknown', '[None]']\ndf_filtered = df[~df['REMOTE_TYPE_NAME'].isin(values_to_exclude)]\n\nfig1 = px.box(\n    df_filtered,\n    x=\"REMOTE_TYPE_NAME\",\n    y=\"SALARY\",\n    title=\"Salary Distribution by Work Arrangement\",\n    labels={\"REMOTE_TYPE_NAME\": \"Work Arrangement\", \"SALARY\": \"Annual Salary ($)\"},\n    width=800,\n    height=600,\n    color=\"REMOTE_TYPE_NAME\",                         \n    color_discrete_sequence=px.colors.qualitative.Set2\n)\nfig1.show()\nfig1.write_image(\"figures/salary_by_work_arrangement.png\", scale=2)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n0.2.2.2 5.1.2 Top Skills vs. Average Salary\n\n\nCode\n# 5.1.2 Visual - Skills vs. Salary\nimport plotly.express as px\nimport ast\n\n# This function safely converts the string of a list into an actual list\ndef parse_skills(skill_list_str):\n    try:\n        return ast.literal_eval(skill_list_str)\n    except (ValueError, SyntaxError):\n        return []\n\n# Create a new column with the cleaned lists of skills\ndf['SKILLS_LIST'] = df['SKILLS_NAME'].apply(parse_skills)\n\n# Create a new DataFrame where each skill gets its own row\ndf_skills_exploded = df.explode('SKILLS_LIST')\n\n# --- Now, create the chart using the cleaned data ---\ntop_10_skills_by_count = df_skills_exploded['SKILLS_LIST'].value_counts().nlargest(10).index\ndf_top_skills = df_skills_exploded[df_skills_exploded['SKILLS_LIST'].isin(top_10_skills_by_count)]\navg_salary_for_top_skills = df_top_skills.groupby('SKILLS_LIST')['SALARY'].mean().reset_index()\n\nfig2 = px.bar(\n    avg_salary_for_top_skills,\n    x='SKILLS_LIST',\n    y=\"SALARY\",\n    title=\"Average Salary for Top 10 Skills In Demand\",\n    labels={'SKILLS_LIST': \"Skill\", \"SALARY\": \"Average Annual Salary ($)\"},\n    width=800,\n    height=600,                       \n    color_discrete_sequence=px.colors.qualitative.Set2,\n    opacity=0.7\n)\nfig2.update_layout(\n    margin=dict(l=80, r=50, t=80, b=200),  # increase bottom margin\n    xaxis_tickangle=-45                    # tilt labels for readability\n)\nfig2.show()\nfig2.write_image(\"figures/topskills_salary.png\", scale=2)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n0.2.2.3 5.1.3 Salary Distribution by Top Industries\nRationale: The purpose of this EDA is to explore how compensation varies across economic sectors and identify which industries offer the highest earning potential. Using the 2024 job posting data and grouping by industry (NAICS 2022 Level-6 codes), this analysis aims to compare the median salary and salary distribution across highest-paying industries within the job market.\nKey Insights: The analysis revealed that\nThe analysis revealed that information- and technology-driven sectors—such as Web Search Portals and All Other Information Services and Computing Infrastructure Providers, Data Processing, Web Hosting, and Related Services—offer the highest median salaries, exceeding $160,000 in some cases. Professional services fields like Management Consulting and Certified Public Accountants also ranked among the top, reflecting the premium placed on specialized expertise. The boxplot visualization further illustrates the wide salary dispersion in these industries, indicating high variance between entry- and senior-level roles. Conversely, industries such as Office Administrative Services and Miscellaneous Retailers showed lower median pay and narrower ranges, suggesting more standardized compensation structures. Overall, the findings highlight the concentration of top-paying opportunities in technology, finance, and professional consulting sectors, consistent with broader labor-market trends favoring digital and analytical skills.\n\n\nCode\n## Query Setup\n# Convert the POSTED date from string to date format\ndf[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\n# Filter for job postings from 2024, specifically looking at Salary and Industry. Exclude unknowns, nulls, and zeros. Exclude 'Unclassified Industry' \ndf_jp_2024 = df[\n  (df[\"POSTED\"].dt.year==2024) & \n  (df[\"SALARY\"] &gt; 0) & \n  (df[\"SALARY\"].notnull()) &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unknown\") &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unclassified Industry\")\n]\n\n## Further filter to exclude industries that have an insignificant number of job postings\n# count the number of rows per industry  \nindustry_jp_count = df_jp_2024[\"NAICS_2022_6_NAME\"].value_counts()\n\n# summarize the distribution of job counts per industry\nindustry_jp_count.describe()\n\n# Set minimum threshold at 100 job postings to ensure statistical significance\ntop_jp_industries = industry_jp_count[industry_jp_count &gt; 100].index\n\n# Update df to only show top job posting industries\ndf_jp_2024 = df_jp_2024[df_jp_2024[\"NAICS_2022_6_NAME\"].isin(top_jp_industries)]\n\n\n\n\nCode\n## Plot: Analyze Median Salary by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# group by industry name and calculate median salary, sort by descending order\ntop_industry_salary_order = (\n    df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"]\n    .median()\n    .sort_values(ascending=False)\n    .head(12)\n)\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax1=sns.barplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    hue=\"NAICS_2022_6_NAME\",\n    order=industry_order,\n    orient='h',\n    palette=\"Set3\",\n    estimator=np.median,\n    errorbar=None,\n    legend=False,\n    alpha=0.8\n)\n\nfor patch in ax1.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Median Salary by Industry\")\nplt.xlabel(\"Median Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\nout_path = \"figures/median_salary_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Plot: Analyze Salary Distribution by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# # determine IQRs by industry:\n# q25 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.25)\n# q75 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.75)\n# # sort by the middle 50% (Q3 - Q1) and name that as the new sorting order\n# iqr = (q75 - q25).sort_values(ascending=False).head(12)\n\n#iqr_order = iqr.index.to_list()\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax2 = sns.boxplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    order=industry_order,\n    palette=\"Set3\",\n    width=0.6,\n    fliersize=2.5,  \n    linewidth=0.8,\n)\n\nfor patch in ax2.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution by Industry\")\nplt.xlabel(\"Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\n\n# Save and display the figure\nout_path = \"figures/salary_distribution_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n0.2.2.4 5.1.4 AI vs. Non-AI Job Salary Comparison\n\n\nCode\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a lowercase version of the BODY column for keyword searching\ndf[\"BODY\"] = df[\"BODY\"].astype(str).str.lower()\n\n# identify AI related keywords \nai_keywords = [\n    \"machine learn\",  # matches 'machine learning', 'machine learner'\n    \"data scien\",     # matches 'data scientist', 'data science'\n    \"artificial intel\",  # matches 'artificial intelligence'\n    \"deep learn\",  \n    \"ml engineer\",            \n    \"data engineer\",\n    \"computer vision\", \n    \"natural language\", \n    \"nlp\",\n    \"big data\",\n    \"cloud data\"\n]\n\n# Create a regex pattern that matches any of the keywords, case-insensitive\nai_pattern = re.compile(r\"|\".join([re.escape(k) for k in ai_keywords]), flags=re.IGNORECASE)\n\n# Assign a new column is_ai_job to label job postings with AI or Non-AI based on keyword presence in the BODY text\ndf[\"is_ai_job\"] = df[\"BODY\"].apply(lambda text: \"AI\" if ai_pattern.search(text) else \"Non-AI\")\n\n# Filter out rows with null or zero salary and outliers \ndf_filtered_1 = df[\n    (df[\"SALARY\"].notnull()) &\n    (df[\"SALARY\"] &gt; 0)\n]\nq1 = df_filtered_1[\"SALARY\"].quantile(0.01)\nq99 = df_filtered_1[\"SALARY\"].quantile(0.99)\ndf_filtered_1 = df_filtered_1[(df_filtered_1[\"SALARY\"] &gt;= q1) & (df_filtered_1[\"SALARY\"] &lt;= q99)]\n\nprint(df_filtered_1[\"is_ai_job\"].value_counts())\n\n\nis_ai_job\nNon-AI    15948\nAI         5273\nName: count, dtype: int64\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nax3=sns.boxplot(\n    data=df_filtered_1, \n    x=\"is_ai_job\", \n    y=\"SALARY\",\n    hue=\"is_ai_job\",\n    legend=False,\n    palette=\"Set3\",\n    width=0.4\n)\n\nfor patch in ax3.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Job Type\", fontsize=9)\nplt.ylabel(\"Salary ($)\", fontsize=9)\nplt.tight_layout()\nout_path = \"figures/AI_v_nonAI_salary_boxplot.png\"\nplt.savefig(out_path, dpi=150)\nplt.show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# KDE Plot\nplt.figure(figsize=(8, 5))\nax4=sns.kdeplot(\n    data=df_filtered_1, \n    x=\"SALARY\", \n    hue=\"is_ai_job\", \n    common_norm=False,\n    linewidth=3,\n    palette=\"Set2\",\n    alpha=0.8\n)\n\nfor patch in ax4.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Density: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Salary ($)\", fontsize=9)\nplt.ylabel(\"Density\", fontsize=9)\nplt.tight_layout()\nout_path = \"figures/AI_v_nonAI_salary_KDE.png\"\nplt.savefig(out_path, dpi=150)\nplt.show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n0.2.3 code block for skill_gap_analysis.qmd:\n\n0.2.3.1 Team-based Skill Dataframe\n\n\nCode\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 1, 3],\n    \"SQL\": [3, 3, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 3, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 2, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n0.2.3.2 Compare team skills to industry requirements\n\n\n0.2.3.3 NLP Processing Code Block\n\n\nCode\n# ## Extract most in-demand skills from JD \n\n# import pandas as pd\n# import re\n# from collections import Counter\n# from nltk.corpus import stopwords\n# from pathlib import Path\n# import nltk\n\n# nltk.data.path.append(str(Path.home() / \"nltk_data\"))\n\n# stop_words = stopwords.words(\"english\")\n\n# # Pull description from job postings and convert into strings\n# job_descriptions = df[\"BODY\"].dropna().astype(str).tolist()\n\n# ## NLP processing\n# # Combine all JD strings into one string and convert all to lowercase \n# print(\"Combining job descriptions...\")\n# all_text = \" \".join(job_descriptions).lower()\n\n# # Extract only alphabetical and excludes punctuation, numeric values, symbols (Tokenizing)\n# print(\"Running regex to extract words...\")\n# words = re.findall(r'\\b[a-zA]+\\b', all_text)\n\n# # Filter to remove common stopwords \n# print(\"Filtering out stopwords...\")\n# words_filtered = [word for word in words if word not in stopwords.words(\"english\")]\n\n# # Count the frequency of each word\n# print(\"Counting word frequencies...\")\n# words_count = Counter(words_filtered)\n\n# # Define a list of skills: \n# skills_list = {\"python\", \"sql\", \"aws\", \"docker\", \"tableau\", \"excel\", \"pandas\", \"numpy\", \"power\", \"spark\", \"machine\", \"learning\", \"nlp\", \"cloud\", \"computing\"}\n\n# # Extract the predefined skills that actually appear in the job postings text blob; \n# skills_filtered = {\n#   skill: words_count[skill]\n#   for skill in skills_list\n#   if skill in words_count\n# }\n\n# print(\"Top data analytics skills from job description\")\n# for skill, count in skills_filtered.items():\n#   print(f\"{skill}:{count}\")\n\n\n\n\n0.2.3.4 Compare team skills to industry requirements 2.0\n\n\n0.2.3.5 OPTIMIZED NLP Processing Code Block\n\n\nCode\n## Extract most in-demand skills from JD (optimized)\n\nimport re\nimport os\nimport unicodedata\nfrom collections import Counter\n\n# Import stopwords (Angelina note: switched from NLTK to sklearn's built-in stopwords,\n# which avoids downloads and runs faster)\ntry:\n    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n    stop_words = set(ENGLISH_STOP_WORDS)\nexcept Exception:\n    # Minimal fallback if sklearn is missing\n    stop_words = {\n        \"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\",\n        \"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"were\",\"will\",\"with\"\n    }\n\n# Helper function to normalize text\n# (Angelina note: ensures Unicode normalized, casefolded, and whitespace trimmed)\ndef nfc_casefold_trim(s: str) -&gt; str:\n    s = unicodedata.normalize(\"NFC\", str(s)).casefold()\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n# Compile regex once (Angelina note: faster than re-compiling each loop)\nword_re = re.compile(r\"[a-z]+\")\n\n# Pull description from job postings and count words (streaming, no giant string build)\nprint(\"Scanning job descriptions and counting tokens (streaming)...\")\nwords_count = Counter()\n\nfor txt in df[\"BODY\"].dropna().astype(str):\n    t = nfc_casefold_trim(txt)\n    words_count.update(w for w in word_re.findall(t) if w not in stop_words)\n\n# Define a list of skills\nskills_list = {\n    \"python\", \"sql\", \"aws\", \"docker\", \"tableau\", \"excel\",\n    \"pandas\", \"numpy\", \"spark\", \"machine\", \"learning\",\n    \"nlp\", \"cloud\", \"computing\", \"power\"\n}\n\n# Extract only the predefined skills that appear in job postings\nskills_filtered = {s: words_count[s] for s in skills_list if words_count.get(s, 0) &gt; 0}\n\n# Print results, sorted by most frequent\nprint(\"Top data analytics skills from job descriptions\")\nfor skill, count in sorted(skills_filtered.items(), key=lambda kv: (-kv[1], kv[0])):\n    print(f\"{skill}:{count}\")\n\n\n    # --------- added simple bar chart ---------\nif skills_filtered:\n    os.makedirs(\"figures\", exist_ok=True)  \n    items = sorted(skills_filtered.items(), key=lambda kv: kv[1], reverse=True)\n    labels = [k for k, _ in items]\n    values = [v for _, v in items]\n\n    plt.figure(figsize=(8, 4.5))\n    plt.bar(labels, values)\n    plt.title(\"Most In-Demand Skills from Job Descriptions\")\n    plt.xlabel(\"Skill\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    out_path = \"figures/jd_top_skills.png\"\n    plt.savefig(out_path, dpi=150)\n    plt.show()\n    print(f\"[Saved chart] {out_path}\")\n\n\nScanning job descriptions and counting tokens (streaming)...\nTop data analytics skills from job descriptions\ncloud:42787\nsql:35871\npower:21894\nexcel:19874\nlearning:16361\ntableau:14609\npython:13402\naws:8510\nmachine:5592\ncomputing:2588\nspark:1496\ndocker:613\npandas:378\nnlp:293\nnumpy:187\n\n\n\n\n\n\n\n\n\n[Saved chart] figures/jd_top_skills.png\n\n\n\n\n\n0.2.4 3.1.3 Propose an Improvement Plan\nJob postings show high demand for Cloud, SQL, Python, ML, and AWS. Our team is strong in visualization (Excel, Power BI, Tableau) but weaker in Cloud, ML, and NLP. This plan aligns our learning with market needs, provides specific resources, and ensures collaboration strategies so the whole team can close the gap together.\nWhich skills should each member prioritize learning?\n\nAngelina – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).\n\nDevin – Solid in Excel (5) and Tableau (3), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.\n\nLeo – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions).\n\nWhat courses or resources can help?\n\nCloud & AWS – free cloud provider tutorials, AWS Educate, and cloud labs for hands-on practice.\n\nMachine Learning & NLP – scikit-learn tutorials, Kaggle competitions, and university modules on ML/NLP.\n\nPython & SQL – interactive platforms (Jupyter notebooks, LeetCode SQL), and official documentation.\n\nDocker & Spark – short online workshops, Spark quickstarts, and Docker “getting started” labs.\n\nHow can the team collaborate to bridge skill gaps?\n\nRole rotation: assign rotating leads (“cloud lead,” “ML/NLP lead,” “Python/SQL lead”) for mini-projects so each teammate practices outside their strengths.\n\nLightning talks: weekly 15-minute sessions where one teammate teaches a concept or tool they just learned.\n\nPair programming: match stronger members (for example, Angelina for visualization) with weaker ones (for example, Devin on Python) to share knowledge in real time.\n\nShared resources: maintain a team wiki with reusable queries, cloud setup notes, and code snippets."
  },
  {
    "objectID": "data_analysis.html#kmeans-clustering-analysis",
    "href": "data_analysis.html#kmeans-clustering-analysis",
    "title": "Project Working File",
    "section": "1.1 KMeans Clustering Analysis",
    "text": "1.1 KMeans Clustering Analysis\nWe performed KMeans clustering on job postings using core features (salary, minimum and maximum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n1.1.1 Fit KMeans and Assign Clusters - Data Prep\nWe use KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nCode\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n\n\nCode\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\nKMeans labels column ready: True\nUnique clusters: 5\n\n\n\n\nCode\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\ndisplay(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\nprint(\"\\nMost common label per cluster:\")\ndisplay(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\ndisplay(ct_share.head(50))\n\n\n\n\n\n\n\n\n\nkmeans_labels\nNAICS_2022_6_NAME\ncount\n\n\n\n\n369\n0\nUnclassified Industry\n593\n\n\n118\n0\nEmployment Placement Agencies\n390\n\n\n90\n0\nCustom Computer Programming Services\n232\n\n\n4\n0\nAdministrative Management and General Manageme...\n229\n\n\n95\n0\nDirect Health and Medical Insurance Carriers\n220\n\n\n63\n0\nCommercial Banking\n214\n\n\n71\n0\nComputer Systems Design Services\n179\n\n\n340\n0\nSoftware Publishers\n120\n\n\n61\n0\nColleges, Universities, and Professional Schools\n109\n\n\n359\n0\nTemporary Help Services\n92\n\n\n148\n0\nGeneral Medical and Surgical Hospitals\n89\n\n\n15\n0\nAll Other Miscellaneous Ambulatory Health Care...\n87\n\n\n172\n0\nInsurance Agencies and Brokerages\n74\n\n\n271\n0\nOther Scientific and Technical Consulting Serv...\n61\n\n\n372\n0\nWarehouse Clubs and Supercenters\n60\n\n\n24\n0\nAll Other Professional, Scientific, and Techni...\n59\n\n\n75\n0\nConsumer Lending\n58\n\n\n119\n0\nEngineering Services\n57\n\n\n247\n0\nOther Computer Related Services\n57\n\n\n254\n0\nOther General Government Support\n57\n\n\n262\n0\nOther Management Consulting Services\n47\n\n\n102\n0\nDrugs and Druggists' Sundries Merchant Wholesa...\n38\n\n\n74\n0\nComputing Infrastructure Providers, Data Proce...\n37\n\n\n227\n0\nOffice Administrative Services\n37\n\n\n9\n0\nAircraft Manufacturing\n34\n\n\n176\n0\nInvestment Banking and Securities Intermediation\n33\n\n\n326\n0\nSearch, Detection, Navigation, Guidance, Aeron...\n31\n\n\n28\n0\nAll Other Support Services\n30\n\n\n293\n0\nPortfolio Management and Investment Advice\n30\n\n\n129\n0\nFinancial Transactions Processing, Reserve, an...\n26\n\n\n287\n0\nPharmacies and Drug Retailers\n26\n\n\n235\n0\nOffices of Physicians (except Mental Health Sp...\n25\n\n\n216\n0\nNatural Gas Distribution\n24\n\n\n249\n0\nOther Electric Power Generation\n22\n\n\n286\n0\nPharmaceutical Preparation Manufacturing\n22\n\n\n237\n0\nOffices of Real Estate Agents and Brokers\n21\n\n\n352\n0\nSurgical and Medical Instrument Manufacturing\n21\n\n\n153\n0\nGuided Missile and Space Vehicle Manufacturing\n20\n\n\n374\n0\nWeb Search Portals and All Other Information S...\n20\n\n\n11\n0\nAll Other Business Support Services\n19\n\n\n230\n0\nOffices of Certified Public Accountants\n19\n\n\n356\n0\nTelemarketing Bureaus and Other Contact Centers\n19\n\n\n20\n0\nAll Other Miscellaneous Retailers\n18\n\n\n23\n0\nAll Other Personal Services\n18\n\n\n88\n0\nCredit Unions\n17\n\n\n97\n0\nDirect Property and Casualty Insurance Carriers\n17\n\n\n209\n0\nMotion Picture and Video Production\n17\n\n\n87\n0\nCredit Card Issuing\n16\n\n\n122\n0\nExecutive Search Services\n16\n\n\n330\n0\nSemiconductor and Related Device Manufacturing\n16\n\n\n\n\n\n\n\n\nMost common label per cluster:\n\n\n\n\n\n\n\n\n\nkmeans_labels\nNAICS_2022_6_NAME\ncount\n\n\n\n\n0\n0\nUnclassified Industry\n593\n\n\n1\n1\nComputer Systems Design Services\n412\n\n\n2\n2\nUnclassified Industry\n681\n\n\n3\n3\nAdministrative Management and General Manageme...\n389\n\n\n4\n4\nUnclassified Industry\n165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkmeans_labels\nNAICS_2022_6_NAME\ncount\ncluster_total\nshare\n\n\n\n\n369\n0\nUnclassified Industry\n593\n4890\n0.121\n\n\n118\n0\nEmployment Placement Agencies\n390\n4890\n0.080\n\n\n90\n0\nCustom Computer Programming Services\n232\n4890\n0.047\n\n\n4\n0\nAdministrative Management and General Manageme...\n229\n4890\n0.047\n\n\n95\n0\nDirect Health and Medical Insurance Carriers\n220\n4890\n0.045\n\n\n63\n0\nCommercial Banking\n214\n4890\n0.044\n\n\n71\n0\nComputer Systems Design Services\n179\n4890\n0.037\n\n\n340\n0\nSoftware Publishers\n120\n4890\n0.025\n\n\n61\n0\nColleges, Universities, and Professional Schools\n109\n4890\n0.022\n\n\n359\n0\nTemporary Help Services\n92\n4890\n0.019\n\n\n148\n0\nGeneral Medical and Surgical Hospitals\n89\n4890\n0.018\n\n\n15\n0\nAll Other Miscellaneous Ambulatory Health Care...\n87\n4890\n0.018\n\n\n172\n0\nInsurance Agencies and Brokerages\n74\n4890\n0.015\n\n\n271\n0\nOther Scientific and Technical Consulting Serv...\n61\n4890\n0.012\n\n\n372\n0\nWarehouse Clubs and Supercenters\n60\n4890\n0.012\n\n\n24\n0\nAll Other Professional, Scientific, and Techni...\n59\n4890\n0.012\n\n\n75\n0\nConsumer Lending\n58\n4890\n0.012\n\n\n119\n0\nEngineering Services\n57\n4890\n0.012\n\n\n247\n0\nOther Computer Related Services\n57\n4890\n0.012\n\n\n254\n0\nOther General Government Support\n57\n4890\n0.012\n\n\n262\n0\nOther Management Consulting Services\n47\n4890\n0.010\n\n\n102\n0\nDrugs and Druggists' Sundries Merchant Wholesa...\n38\n4890\n0.008\n\n\n74\n0\nComputing Infrastructure Providers, Data Proce...\n37\n4890\n0.008\n\n\n227\n0\nOffice Administrative Services\n37\n4890\n0.008\n\n\n9\n0\nAircraft Manufacturing\n34\n4890\n0.007\n\n\n176\n0\nInvestment Banking and Securities Intermediation\n33\n4890\n0.007\n\n\n326\n0\nSearch, Detection, Navigation, Guidance, Aeron...\n31\n4890\n0.006\n\n\n28\n0\nAll Other Support Services\n30\n4890\n0.006\n\n\n293\n0\nPortfolio Management and Investment Advice\n30\n4890\n0.006\n\n\n129\n0\nFinancial Transactions Processing, Reserve, an...\n26\n4890\n0.005\n\n\n287\n0\nPharmacies and Drug Retailers\n26\n4890\n0.005\n\n\n235\n0\nOffices of Physicians (except Mental Health Sp...\n25\n4890\n0.005\n\n\n216\n0\nNatural Gas Distribution\n24\n4890\n0.005\n\n\n249\n0\nOther Electric Power Generation\n22\n4890\n0.004\n\n\n286\n0\nPharmaceutical Preparation Manufacturing\n22\n4890\n0.004\n\n\n237\n0\nOffices of Real Estate Agents and Brokers\n21\n4890\n0.004\n\n\n352\n0\nSurgical and Medical Instrument Manufacturing\n21\n4890\n0.004\n\n\n153\n0\nGuided Missile and Space Vehicle Manufacturing\n20\n4890\n0.004\n\n\n374\n0\nWeb Search Portals and All Other Information S...\n20\n4890\n0.004\n\n\n11\n0\nAll Other Business Support Services\n19\n4890\n0.004\n\n\n230\n0\nOffices of Certified Public Accountants\n19\n4890\n0.004\n\n\n356\n0\nTelemarketing Bureaus and Other Contact Centers\n19\n4890\n0.004\n\n\n20\n0\nAll Other Miscellaneous Retailers\n18\n4890\n0.004\n\n\n23\n0\nAll Other Personal Services\n18\n4890\n0.004\n\n\n88\n0\nCredit Unions\n17\n4890\n0.003\n\n\n97\n0\nDirect Property and Casualty Insurance Carriers\n17\n4890\n0.003\n\n\n209\n0\nMotion Picture and Video Production\n17\n4890\n0.003\n\n\n87\n0\nCredit Card Issuing\n16\n4890\n0.003\n\n\n122\n0\nExecutive Search Services\n16\n4890\n0.003\n\n\n330\n0\nSemiconductor and Related Device Manufacturing\n16\n4890\n0.003\n\n\n\n\n\n\n\n\n\nCode\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) &gt; 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nplt.show()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n\n\n\n\n\n\n\n\n\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n\n\n\n\n1.1.2 Top 5 Job Titles for Each Cluster\nCluster 0:\n- data analyst — 3,009\n- senior data analyst — 627\n- business intelligence analyst — 501\n- enterprise architect — 389\n- unlock your future career path in data analytics — 238\nCluster 1:\n- enterprise architect cleared — 71\n- senior data analyst navista — 50\n- sr data analyst strategy and business intelligence — 40\n- security senior solution architect — 33\n- data analyst — 32\nCluster 2:\n- enterprise architect — 319\n- erp senior business analyst fp a — 197\n- data analyst — 132\n- zero trust architect — 89\n- senior data analyst — 69\nCluster 3:\n- data analyst — 1,386\n- business intelligence analyst — 220\n- data and reporting professional — 100\n- retail data analyst — 81\n- part sales data analyst — 59\nCluster 4:\n- data engineer analytics — 117\n- enterprise architect — 105\n- oracle hcm cloud manager — 87\n- sap functional implementation lead payroll module — 58\n- data engineer analytics technical leadership — 57\n\n1.1.2.1 Figure: Top 5 Job Titles per Cluster\n\n\nCode\n# --- Bar chart for the top 5 titles in Cluster 0 (change cluster_num as needed) ---\n\ncluster_num = 0\ntop_titles = df[df['KMEANS_CLUSTER'] == cluster_num]['TITLE_CLEAN'].value_counts().head(5)\n\nplt.figure(figsize=(8, 5))\nbars = top_titles.plot(kind='bar', color='skyblue')\nplt.title(f'Top 5 Job Titles in Cluster {cluster_num}')\nplt.xlabel('Job Title')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right') \n\n# Add the counts on top of the bars\nfor idx, value in enumerate(top_titles.values):\n    plt.text(idx, value + max(top_titles.values)*0.01, str(value), ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure: Top 5 Job Titles per Cluster\n\n\n1.1.2.2 Analysis for Business Relevance\nThe clusters reveal how salary and experience interact across industries. When linked with NAICS reference codes, lower-salary clusters correspond primarily to administrative support and retail services, while higher-salary clusters (1, 2, 4) align with technology, finance, and professional-services sectors. This pattern confirms that advanced technical and analytical roles command higher compensation even at similar experience levels.\nFor job seekers, understanding which industries occupy higher clusters can guide upskilling toward data engineering, cloud, or analytics leadership roles that return stronger salary growth.\nFor employers, these clusters act as market benchmarks, helping calibrate compensation and experience requirements to remain competitive in high-demand technical fields.\n\n\nCode\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)\ncrosstab\n\n\n\n\n\n\n\n\nNAICS_2022_6_NAME\nAbrasive Product Manufacturing\nAdhesive Manufacturing\nAdministration of Air and Water Resource and Solid Waste Management Programs\nAdministration of Education Programs\nAdministration of General Economic Programs\nAdministration of Housing Programs\nAdministration of Human Resource Programs (except Education, Public Health, and Veterans' Affairs Programs)\nAdministration of Public Health Programs\nAdministration of Veterans' Affairs\nAdministrative Management and General Management Consulting Services\n...\nWater and Sewer Line and Related Structures Construction\nWeb Search Portals and All Other Information Services\nWelding and Soldering Equipment Manufacturing\nWholesale Trade Agents and Brokers\nWine and Distilled Alcoholic Beverage Merchant Wholesalers\nWineries\nWired Telecommunications Carriers\nWireless Telecommunications Carriers (except Satellite)\nWood Kitchen Cabinet and Countertop Manufacturing\nWood Window and Door Manufacturing\n\n\nKMEANS_CLUSTER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n0\n0\n1\n0\n3\n0\n0\n7\n1\n229\n...\n2\n20\n0\n13\n6\n0\n0\n10\n1\n0\n\n\n1.0\n3\n0\n0\n0\n0\n0\n0\n1\n0\n86\n...\n1\n3\n0\n4\n0\n0\n0\n3\n1\n0\n\n\n2.0\n1\n1\n2\n3\n2\n1\n2\n10\n1\n118\n...\n1\n8\n1\n10\n3\n1\n1\n6\n0\n1\n\n\n3.0\n2\n0\n0\n0\n0\n0\n0\n2\n0\n389\n...\n0\n88\n0\n5\n0\n1\n0\n19\n0\n0\n\n\n4.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n83\n...\n0\n23\n0\n1\n0\n1\n0\n8\n0\n0\n\n\n\n\n5 rows × 565 columns\n\n\n\nFigure: KMeans clusters for job postings, plotted by salary and minimum years experience."
  },
  {
    "objectID": "data_analysis.html#regression-predicting-salary",
    "href": "data_analysis.html#regression-predicting-salary",
    "title": "Project Working File",
    "section": "1.2 Regression – Predicting Salary",
    "text": "1.2 Regression – Predicting Salary\nWe chose Random Forest over Linear Regression because the relationship between experience and salary is non-linear, and the model captures complex interactions more effectively.\nThe goal of this model is to predict job posting salary using experience and employment type features. The model uses an 80/20 train–test split and evaluates performance using RMSE and R² metrics.\n\n\nCode\n# Feature Selection and Data Preparation\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Use minimum years of experience, maximum years of experience, and employment type as features\nfeatures = ['MIN_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']\n\n# One-hot encode employment type\ndf_encoded = pd.get_dummies(df, columns=['REMOTE_TYPE_NAME'], drop_first=True)\n\nX = df_encoded[[col for col in df_encoded.columns if col in features or col.startswith('REMOTE_TYPE_NAME')]]\ny = df_encoded['SALARY']\n\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train/test split (70/30)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Drop rows where y (salary) is missing\nmask = ~y_train.isna()\nX_train_filtered = X_train.loc[mask]\ny_train_filtered = y_train.loc[mask]\n\nmask_test = ~y_test.isna()\nX_test_filtered = X_test.loc[mask_test]\ny_test_filtered = y_test.loc[mask_test]\n\n# Train a random forest regression model to predict salary\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_filtered, y_train_filtered)\n\n# Predict salaries for the test set\ny_pred = rf.predict(X_test_filtered)\n\n# Calculate RMSE (Root Mean Squared Error) and R² (coefficient of determination)\nmse = mean_squared_error(y_test_filtered, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_filtered, y_pred)\n\nprint(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\nprint(f'R² Score: {r2:.3f}')\n\n\nRoot Mean Squared Error (RMSE): 34744.38\nR² Score: 0.336\n\n\n\n\nCode\n# Visual: Predicted vs. Actual Salary\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_actual, y_pred_series, alpha=0.5)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Predicted vs. Actual Salary (Random Forest Regression)\")\nlo, hi = float(y_actual.min()), float(y_actual.max())\nplt.plot([lo, hi], [lo, hi], 'r--', label=\"Perfect Prediction\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure: Predicted vs. actual salaries from the regression model. Points closer to the red dashed line indicate more accurate predictions.\n\n\nCode\n# Feature importance: shows which variables most influence salary prediction\nimportances = rf.feature_importances_\nfeature_names = X.columns\n\nfeat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 features influencing salary prediction:\")\nfor name, imp in feat_imp[:5]:\n    print(f\"{name}: {imp:.3f}\")\n\n\nTop 5 features influencing salary prediction:\nMIN_YEARS_EXPERIENCE: 0.953\nREMOTE_TYPE_NAME_Not Remote: 0.017\nREMOTE_TYPE_NAME_[None]: 0.015\nREMOTE_TYPE_NAME_Remote: 0.014\nREMOTE_TYPE_NAME_Unknown: 0.000\n\n\n\n\nCode\n# Visual: Feature Importance Bar Chart\nimport pandas as pd\n\nfeat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\nfeat_imp_df = feat_imp_df.sort_values('importance', ascending=False).head(5)\n\nplt.figure(figsize=(8, 5))\nplt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Top 5 Feature Importances (Random Forest Regression)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure: Top 5 features influencing salary predictions, as measured by the random forest model.\n\n\nCode\n# Calculate absolute prediction error for each job\nimport pandas as pd\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\nerrors = (y_actual - y_pred_series).abs()\noutlier_indices = errors.nlargest(5).index\n\n# Show the original job posting rows for these outliers\ncols = ['SALARY', 'MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE_NAME', 'REMOTE_TYPE_NAME']\ncols = [c for c in cols if c in df.columns]\nprint(df.loc[outlier_indices, cols])\n\n\n         SALARY  MIN_YEARS_EXPERIENCE REMOTE_TYPE_NAME\n12166  312500.0                   1.0           [None]\n23659  312000.0                   1.0           [None]\n39897  328600.0                   5.0           [None]\n2586   305000.0                   3.0           [None]\n37324  338750.0                   7.0           [None]\n\n\n\n\nCode\n# Visual: Highlight Outliers in Predicted vs. Actual Salary\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nemp_col = next((c for c in [\"EMPLOYMENT_TYPE_NAME\",\"REMOTE_TYPE_NAME\",\"STATE_NAME\",\"MSA_NAME\"] if c in df.columns), None)\n\nsubset_cols = [\"SALARY\",\"MIN_YEARS_EXPERIENCE\"] + ([emp_col] if emp_col else [])\nsubset = df.loc[outlier_indices, subset_cols].copy()\n\ndef _fmt_sal(x): \n    return f\"${int(x):,}\" if pd.notna(x) else \"N/A\"\n\ndef _fmt_exp(x): \n    return f\"{int(x)}yr min\" if pd.notna(x) else \"n/a\"\n\ndef _fmt_emp(x): \n    return str(x) if pd.notna(x) else \"n/a\"\n\nif emp_col:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\\n{_fmt_emp(e)}\"\n        for s, m, e in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"], subset[emp_col])\n    ]\nelse:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\"\n        for s, m in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"])\n    ]\n\nplt.figure(figsize=(10, 5))\nplt.bar(labels, subset[\"SALARY\"].fillna(0))\nplt.ylabel(\"Actual Salary\")\nplt.title(\"Top 5 Outlier Job Salaries\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Classification: Predicting Remote vs On-Site Jobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Prepare data\nemp_col = \"EMPLOYMENT_TYPE_NAME\" if \"EMPLOYMENT_TYPE_NAME\" in df.columns else (\"STATE_NAME\" if \"STATE_NAME\" in df.columns else (\"MSA_NAME\" if \"MSA_NAME\" in df.columns else None))\nif emp_col is not None:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", emp_col, \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nelse:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nclf_df[\"IS_REMOTE\"] = clf_df[\"REMOTE_TYPE_NAME\"].str.contains(\"Remote\", case=False, na=False).astype(int)\n\nX = clf_df[[\"MIN_YEARS_EXPERIENCE\", emp_col]].copy() if emp_col is not None else clf_df[[\"MIN_YEARS_EXPERIENCE\"]].copy() # Removed MAX_YEARS_EXPERIENCE\ny = clf_df[\"IS_REMOTE\"]\n\n# One-hot encode employment type\nif emp_col is not None:\n    preprocessor = ColumnTransformer([\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [emp_col]),\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\nelse:\n    preprocessor = ColumnTransformer([\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Model\nclf = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n])\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\n# Metrics\nacc = accuracy_score(y_test, pred)\nf1 = f1_score(y_test, pred)\nprint(f\"Accuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n\n# Confusion matrix\nConfusionMatrixDisplay(confusion_matrix(y_test, pred)).plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix — Remote vs On-Site Classification\")\nplt.show()\n\n\nAccuracy: 0.619, F1 Score: 0.328\n\n\n\n\n\n\n\n\n\nFigure: Actual salaries of the five largest outlier jobs. Job experience and employment type are shown in labels.\nOutlier Analysis\nA review of the top 5 largest prediction errors shows several jobs with extremely high actual salaries (all above $300,000), yet typical or even low minimum/maximum years of experience, and standard employment types. Examples include:\n\n\n\n\n\n\n\n\n\nActual Salary\nMin Experience\nMax Experience\nEmployment Type\n\n\n\n\n$470,000\n5\n3\nFull-time (&gt; 32 hours)\n\n\n$437,500\n15\n3\nFull-time (&gt; 32 hours)\n\n\n$347,075\n5\n3\nPart-time / full-time\n\n\n$353,500\n8\n3\nFull-time (&gt; 32 hours)\n\n\n$328,600\n5\n3\nFull-time (&gt; 32 hours)\n\n\n\nThese outlier jobs may represent rare executive roles, data entry issues, or atypical salary reporting.\nTheir presence skews the model’s RMSE and makes prediction more difficult. In a business context, further investigation or filtering of these outliers would improve overall model accuracy and provide more relevant insights for most job seekers.\n\n1.2.0.1 Regression Results and Interpretation\nA random forest regression model was trained to predict salary using minimum years of experience, maximum years of experience, and employment type as features. The model was evaluated with a 70/30 train-test split.\n\nRoot Mean Squared Error (RMSE): 27,598.44\nR² Score: 0.115\n\nThe R² score shows that a limited amount of salary variation is explained by these features, indicating that other factors (not included in the model) may also influence salary.\nTop features influencing salary prediction: 1. Minimum years of experience (0.910) 2. Employment type: Part-time (≤ 32 hours) (0.044) 3. Maximum years of experience (0.029) 4. Employment type: Part-time / full-time (0.016) 5. Employment type: Unknown (0.000)\nInterpretation for business and job-seeker implications: - Minimum years of experience is the strongest driver of salary in this dataset. - Employment type and maximum years of experience also play a smaller role. - Job seekers should highlight their relevant experience to increase their salary potential. - Employers should consider how clearly defined experience requirements and employment types may impact pay and candidate attraction.\nNote on outliers:\nSeveral extreme outliers (very high salaries with typical experience and standard employment types) were found. These can affect the model’s accuracy and may reflect unusual roles, reporting issues, or data entry errors. In a real business scenario, further investigation or filtering of these outliers would be recommended to improve model reliability."
  },
  {
    "objectID": "data_analysis.html#classification-predicting-remote-vs-on-site-job",
    "href": "data_analysis.html#classification-predicting-remote-vs-on-site-job",
    "title": "Project Working File",
    "section": "1.3 Classification – Predicting Remote vs On-Site Job",
    "text": "1.3 Classification – Predicting Remote vs On-Site Job\n\n\nCode\n# # Classification: Predicting Remote vs On-Site Jobs\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# import matplotlib.pyplot as plt\n\n# # Prepare data\n# clf_df = df[[\"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\", \"EMPLOYMENT_TYPE_NAME\", \"REMOTE_TYPE_NAME\"]].dropna().copy()\n# clf_df[\"IS_REMOTE\"] = clf_df[\"REMOTE_TYPE_NAME\"].str.contains(\"Remote\", case=False).astype(int)\n\n# X = clf_df[[\"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\", \"EMPLOYMENT_TYPE_NAME\"]]\n# y = clf_df[\"IS_REMOTE\"]\n\n# # One-hot encode employment type\n# preprocessor = ColumnTransformer([\n#     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"EMPLOYMENT_TYPE_NAME\"]),\n#     (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"])\n# ])\n\n# # Train/test split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# # Model\n# clf = Pipeline([\n#     (\"prep\", preprocessor),\n#     (\"model\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n# ])\n# clf.fit(X_train, y_train)\n# pred = clf.predict(X_test)\n\n# # Metrics\n# acc = accuracy_score(y_test, pred)\n# f1 = f1_score(y_test, pred)\n# print(f\"Accuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n\n# # Confusion matrix\n# ConfusionMatrixDisplay(confusion_matrix(y_test, pred)).plot(cmap=\"Blues\")\n# plt.title(\"Confusion Matrix — Remote vs On-Site Classification\")\n# plt.show()\n\n\n\n1.3.1 Classification Results and Interpretation\nWe trained a logistic-regression model to predict whether a job posting is remote or on-site using years of experience and employment type. After applying class_weight=‘balanced’ to handle class imbalance, model performance reached Accuracy ≈ 0.60 and F1 ≈ 0.29.\nThe confusion matrix shows that while most on-site roles are correctly detected, roughly one-third of remote jobs are now correctly identified. These results suggest that experience level and employment type provide some predictive power, but additional features such as job title, industry, or skills would likely improve remote-work detection."
  },
  {
    "objectID": "data_analysis.html#conclusive-key-insights-and-recommendations-for-job-seekers",
    "href": "data_analysis.html#conclusive-key-insights-and-recommendations-for-job-seekers",
    "title": "Project Working File",
    "section": "1.4 Conclusive Key Insights and Recommendations for Job Seekers",
    "text": "1.4 Conclusive Key Insights and Recommendations for Job Seekers\n\nIndustry segmentation (KMeans) shows distinct clusters by salary and experience; tech and professional-services roles dominate higher-pay clusters.\n\nSalary modeling (Regression) confirms that years of experience and job type drive pay levels.\n\nRemote-work classification indicates that higher experience and full-time roles increase the likelihood of remote opportunities.\n\nImplication: Job seekers should focus on building experience and targeting high-skill industries to improve salary potential and remote flexibility."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1 Introduction & Research Rationale",
    "section": "",
    "text": "1 Introduction & Research Rationale\nThe job market in 2024 is undergoing a major transformation with the rise of artificial intelligence and widespread acceptance of long-distance/remote work. This analysis will examine the following key topics to uncover the factors that influence salary trends:\nThe restructuring of compensation due to AI, inflation, and remote work.\nThe traditional pay structure is being reshaped as companies adapt to automation, rising costs of living, and a distributed workforce. These forces are pushing employers to rethink how they reward skills, experience, and location.\nGrowing pay disparities across regions, industries, and job types in 2024.\nHigh-paying jobs are increasingly concentrated in specific tech-driven sectors and urban hubs. Meanwhile, many essential or service-based roles are seeing slower wage growth, deepening inequality across the workforce.\nShifting salary patterns based on remote flexibility, job type, and sector growth.\nRemote roles now often offer comparable or even higher compensation due to talent shortages and broader applicant pools. Industries like tech and healthcare are setting new standards, while others struggle to keep pace.\n\n\n2 Literature Review Summary\nThe rise of Artificial Intelligence (AI) has reshaped the job market across the US, with demand for AI-related skills increasing dramatically. According to a study by PWC, jobs that require AI specialist skills are growing 3.5 times faster than all other job markets, with skilled AI workers being paid up to 25% more in some sectors (PwC 2024). However, this increase in compensation is not limited to workers specialized in AI. Non-AI roles that require complementary skills such as digital literacy, analytical thinking, and teamwork are also seeing a 5–10% wage increase (Mäkelä and Stephany 2024). According to US job vacancy data from 2018–2024, AI-related jobs are significantly more likely to include non-monetary benefits as part of the compensation package, including parental leave and remote working options (Stephany, Mira, and Bone 2025).\nBeyond the benefit of not having to commute and the ability to work from anywhere, long-distance and remote roles are often sought after by job seekers due to faster wage growth. In a study comparing the pay trends of remote versus in-office workers in the same occupation, remote workers experienced 4.4% faster annual wage growth, especially in professional and technical sectors (Pabilonia and Vernon 2025). Furthermore, workers who transitioned into remote roles with the same employer saw up to 16 percentage points higher wage growth than their counterparts who remained local (Romem 2024). This demonstrates that switching into remote work can lead to significantly higher wage growth, even within the same company and/or job category.\n\n\n\n\n\nReferences\n\nMäkelä, E., and F. Stephany. 2024: “Complement or substitute? How AI increases the demand for human skills,” arXiv preprint arXiv:2412.19754,.\n\n\nPabilonia, S. W., and V. Vernon. 2025: “Remote work, wages, and hours worked in the united states,” SSRN Electronic Journal,.\n\n\nPwC. 2024: “PwC 2024 Global AI Jobs Barometer: Demand, Wages, and Disruption,”\n\n\nRomem, I. 2024: “Long-Distance Work and Compensation,”ADP Research Institute.\n\n\nStephany, F., A. Mira, and M. Bone. 2025: “Beyond pay: AI skills reward more job benefits,” arXiv preprint arXiv:2507.20410,."
  },
  {
    "objectID": "introduction.html#introduction-research-rationale",
    "href": "introduction.html#introduction-research-rationale",
    "title": "Job Market Analysis 2024",
    "section": "",
    "text": "The job market in 2024 is undergoing a major transformation with the rise of artificial intelligence and widespread acceptance of long-distance/remote work. This analysis will examine the following key topics to uncover the factors that influence salary trends:\nThe restructuring of compensation due to AI, inflation, and remote work.\nThe traditional pay structure is being reshaped as companies adapt to automation, rising costs of living, and a distributed workforce. These forces are pushing employers to rethink how they reward skills, experience, and location.\nGrowing pay disparities across regions, industries, and job types in 2024.\nHigh-paying jobs are increasingly concentrated in specific tech-driven sectors and urban hubs. Meanwhile, many essential or service-based roles are seeing slower wage growth, deepening inequality across the workforce.\nShifting salary patterns based on remote flexibility, job type, and sector growth.\nRemote roles now often offer comparable or even higher compensation due to talent shortages and broader applicant pools. Industries like tech and healthcare are setting new standards, while others struggle to keep pace."
  },
  {
    "objectID": "introduction.html#literature-review-summary",
    "href": "introduction.html#literature-review-summary",
    "title": "Job Market Analysis 2024",
    "section": "2 Literature Review Summary",
    "text": "2 Literature Review Summary\nThe rise of Artificial Intelligence (AI) has reshaped the job market across the US, with demand for AI-related skills increasing dramatically. According to a study by PWC, jobs that require AI specialist skills are growing 3.5 times faster than all other job markets, with skilled AI workers being paid up to 25% more in some sectors [@pwc2024]. However, this increase in compensation is not limited to workers specialized in AI. Non-AI roles that require complementary skills such as digital literacy, analytical thinking, and teamwork are also seeing a 5–10% wage increase [@makela2024]. According to US job vacancy data from 2018–2024, AI-related jobs are significantly more likely to include non-monetary benefits as part of the compensation package, including parental leave and remote working options [@stephany2025].\nBeyond the benefit of not having to commute and the ability to work from anywhere, long-distance and remote roles are often sought after by job seekers due to faster wage growth. In a study comparing the pay trends of remote versus in-office workers in the same occupation, remote workers experienced 4.4% faster annual wage growth, especially in professional and technical sectors [@pabilonia2025]. Furthermore, workers who transitioned into remote roles with the same employer saw up to 16 percentage points higher wage growth than their counterparts who remained local [@romem2024]. This demonstrates that switching into remote work can lead to significantly higher wage growth, even within the same company and/or job category."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA Overview",
    "section": "",
    "text": "This section explores job market trends and the restructuring of compensation through a series of visualizations. Each exploratory data analysis (EDA) was chosen to reveal specific patterns in compensation to highlight the impact of AI and remote work.\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")"
  },
  {
    "objectID": "eda.html#salary-by-remote-work-type",
    "href": "eda.html#salary-by-remote-work-type",
    "title": "EDA Overview",
    "section": "1 Salary by Remote Work Type",
    "text": "1 Salary by Remote Work Type\nThe purpose of this EDA is to visually compare not just the average pay, but also the range and consistency of salaries across different work arrangements. This visualization could reveal that remote roles have a wider salary range, indicating that companies are paying a premium for top talent regardless of location.\n\n\nCode\n# 5.1.1 Visual - Compensation\nimport plotly.express as px\nimport pandas as pd\n\nvalues_to_exclude = ['Unknown', '[None]']\ndf_filtered = df[~df['REMOTE_TYPE_NAME'].isin(values_to_exclude)]\n\nfig1 = px.box(\n    df_filtered,\n    x=\"REMOTE_TYPE_NAME\",\n    y=\"SALARY\",\n    title=\"Salary Distribution by Work Arrangement\",\n    labels={\"REMOTE_TYPE_NAME\": \"Work Arrangement\", \"SALARY\": \"Annual Salary ($)\"},\n    width=800,\n    height=600,\n    color=\"REMOTE_TYPE_NAME\",                         \n    color_discrete_sequence=px.colors.qualitative.Set2\n)\n#fig1.show()\nfig1.write_image(\"figures/salary_by_work_arrangement.png\", scale=2)\n\n\n\n\n\nFigure 2: Salary Distribution by Work Arrangement\n\n\nKey Insights: The median salaries across all work arrangements are similar, clustering around $115,000. However, both Remote and Hybrid Remote roles exhibit a much wider salary range and more high-paying outlier positions, with some remote roles exceeding $350,000. This suggests that while typical pay is comparable, remote-friendly positions offer significantly greater potential for top-end compensation."
  },
  {
    "objectID": "eda.html#top-skills-vs.-average-salary",
    "href": "eda.html#top-skills-vs.-average-salary",
    "title": "EDA Overview",
    "section": "2 Top Skills vs. Average Salary",
    "text": "2 Top Skills vs. Average Salary\nThe purpose of this EDA is to identify which specific skills are most financially valuable in the current job market, connecting AI-related skills to compensation. This could reveal that skills related to AI/ML platforms command a significantly higher salary, even if they aren’t the most frequently requested skills overall.\n\n\nCode\n# 5.1.2 Visual - Skills vs. Salary\nimport plotly.express as px\nimport ast\n\n# This function safely converts the string of a list into an actual list\ndef parse_skills(skill_list_str):\n    try:\n        return ast.literal_eval(skill_list_str)\n    except (ValueError, SyntaxError):\n        return []\n\n# Create a new column with the cleaned lists of skills\ndf['SKILLS_LIST'] = df['SKILLS_NAME'].apply(parse_skills)\n\n# Create a new DataFrame where each skill gets its own row\ndf_skills_exploded = df.explode('SKILLS_LIST')\n\n# --- Now, create the chart using the cleaned data ---\ntop_10_skills_by_count = df_skills_exploded['SKILLS_LIST'].value_counts().nlargest(10).index\ndf_top_skills = df_skills_exploded[df_skills_exploded['SKILLS_LIST'].isin(top_10_skills_by_count)]\n\navg_salary_for_top_skills = df_top_skills.groupby('SKILLS_LIST')['SALARY'].mean().reset_index()\n\nfig2 = px.bar(\n    avg_salary_for_top_skills,\n    x='SKILLS_LIST',\n    y=\"SALARY\",\n    title=\"Average Salary for Top 10 Skills\",\n    labels={'SKILLS_LIST': \"Skill\", \"SALARY\": \"Average Annual Salary ($)\"},\n    width=800,\n    height=600,                       \n    color_discrete_sequence=px.colors.qualitative.Set2,\n    opacity=0.7\n)\n#fig2.show()\nfig2.write_image(\"figures/topskills_salary.png\", scale=2)\n\n\n\n\n\nFigure 3: Average Salary for Top 10 Skills most in Demand\n\n\nKey Insights: Among the 10 most in-demand skills, Data Analysis commands the highest average salary at over $120,000. While compensation for most top skills is competitive, technical skills like Data Analysis and Business Process show a slight financial advantage over the lowest-paid skill in this group, Management, which averages approximately $110,000."
  },
  {
    "objectID": "eda.html#salary-trends-by-top-industries",
    "href": "eda.html#salary-trends-by-top-industries",
    "title": "EDA Overview",
    "section": "3 Salary Trends by Top Industries",
    "text": "3 Salary Trends by Top Industries\nThe purpose of this EDA is to explore how compensation varies across economic sectors and identify which industries offer the highest earning potential. Using the 2024 job posting data and grouping by industry (NAICS 2022 Level-6 codes), this analysis aims to compare the median salary and salary distribution across highest-paying industries within the job market.\n\n\nCode\n## Query Setup\n# Convert the POSTED date from string to date format\ndf[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\n# Filter for job postings from 2024, specifically looking at Salary and Industry. Exclude unknowns, nulls, and zeros. Exclude 'Unclassified Industry' \ndf_jp_2024 = df[\n  (df[\"POSTED\"].dt.year==2024) & \n  (df[\"SALARY\"] &gt; 0) & \n  (df[\"SALARY\"].notnull()) &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unknown\") &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unclassified Industry\")\n]\n\n## Further filter to exclude industries that have an insignificant number of job postings\n# count the number of rows per industry  \nindustry_jp_count = df_jp_2024[\"NAICS_2022_6_NAME\"].value_counts()\n\n# summarize the distribution of job counts per industry\nindustry_jp_count.describe()\n\n# Set minimum threshold at 100 job postings to ensure statistical significance\ntop_jp_industries = industry_jp_count[industry_jp_count &gt; 100].index\n\n# Update df to only show top job posting industries\ndf_jp_2024 = df_jp_2024[df_jp_2024[\"NAICS_2022_6_NAME\"].isin(top_jp_industries)]\n\n\n\n\nCode\n## Plot: Analyze Median Salary by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# group by industry name and calculate median salary, sort by descending order\ntop_industry_salary_order = (\n    df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"]\n    .median()\n    .sort_values(ascending=False)\n    .head(12)\n)\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax1=sns.barplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    hue=\"NAICS_2022_6_NAME\",\n    order=industry_order,\n    orient='h',\n    palette=\"Set3\",\n    estimator=np.median,\n    errorbar=None,\n    legend=False,\n    alpha=0.8\n)\n\nfor patch in ax1.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Median Salary by Industry\")\nplt.xlabel(\"Median Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\nout_path = \"figures/median_salary_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 4: Median Salary by Industry\n\n\n\n\nCode\n## Plot: Analyze Salary Distribution by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# # determine IQRs by industry:\n# q25 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.25)\n# q75 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.75)\n# # sort by the middle 50% (Q3 - Q1) and name that as the new sorting order\n# iqr = (q75 - q25).sort_values(ascending=False).head(12)\n\n#iqr_order = iqr.index.to_list()\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax2 = sns.boxplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    order=industry_order,\n    palette=\"Set3\",\n    width=0.6,\n    fliersize=2.5,  \n    linewidth=0.8,\n)\n\nfor patch in ax2.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution by Industry\")\nplt.xlabel(\"Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\n\n# Save and display the figure\nout_path = \"figures/salary_distribution_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 5: Salary Distribution by Industry\n\n\nKey Insights: The analysis of median salary across industries shows that Web Search Portals and All Other Information Services leads with the highest median pay, exceeding $160,000. This is likely due to the increasing demand on specialized digital infrastructure and AI-driven information services. The box plot shows a wide interquartile range (IQR), indicating a significant pay gap between entry-level and more senior/specialized roles in the industry. This trend is also observed in the Computing Infrastructure Providers, Data Processing, Web Hosting, and Related Services category, which is indicative of the range of roles in demand for those industries. Although other technology and information industries such as Software Publishers and Computer System Design Services reported slightly lower median salaries compared to the top-paying sectors, the large amount of high-end outlier values suggests that these industries still offer high-earning potential for more senior/specialized roles with most positions clustering around mid-range salaries.\nNotably, non-technology/information-related sectors such as All Other Miscellaneous Retailers also reported high median salaries, around $160,000. The broad definition of these sectors likely encompasses various types of retail firms that may include roles and platforms beyond the traditional frontline retail and sales roles. Another sector in which compensation data may be inflated by data categorization is Offices of Certified Public Accountants (CPAs), which reported a median salary of around $145,000. Based on the salary distribution highlighted in the box plot, the positively-skewed distribution indicates that while most CPA roles earn around the median, some specialized roles are earning substantially more. This aligns with how CPA firms are typically structured, where a small number of senior partners and high-level executives driving the high-end outliers.\nThese findings show that companies in technology, professional services, and consulting industries offer the highest median salaries and exhibit a larger dispersion in compensation, which is reflective of a more diverse workforce structure that may include entry level, technical/specialist, and senior leadership roles. On the other hand, administrative and support service sectors demonstrate lower median but tighter distributions in salary, demonstrating a more standardized workforce and wage structure in comparison."
  },
  {
    "objectID": "eda.html#ai-vs.-non-ai-salary-comparison",
    "href": "eda.html#ai-vs.-non-ai-salary-comparison",
    "title": "EDA Overview",
    "section": "4 AI vs. Non-AI Salary Comparison",
    "text": "4 AI vs. Non-AI Salary Comparison\nThe motivation for this analysis was to investigate how specialization in AI-related skills influence salary in today’s job market. This analysis differentiates AI vs. non-AI jobs by identifying AI-related keywords such as machine learning, data science, computer vision, and natural language processing in job postings. We then compare the median and distribution of salaries between the two groups to determine whether AI roles command higher compensation than non-AI roles.\n\n\nCode\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a lowercase version of the BODY column for keyword searching\ndf[\"BODY\"] = df[\"BODY\"].astype(str).str.lower()\n\n# identify AI related keywords \nai_keywords = [\n    \"machine learn\",  # matches 'machine learning', 'machine learner'\n    \"data scien\",     # matches 'data scientist', 'data science'\n    \"artificial intel\",  # matches 'artificial intelligence'\n    \"deep learn\",  \n    \"ml engineer\",            \n    \"data engineer\",\n    \"computer vision\", \n    \"natural language\", \n    \"nlp\",\n    \"big data\",\n    \"cloud data\"\n]\n\n# Create a regex pattern that matches any of the keywords, case-insensitive\nai_pattern = re.compile(r\"|\".join([re.escape(k) for k in ai_keywords]), flags=re.IGNORECASE)\n\n# Assign a new column is_ai_job to label job postings with AI or Non-AI based on keyword presence in the BODY text\ndf[\"is_ai_job\"] = df[\"BODY\"].apply(lambda text: \"AI\" if ai_pattern.search(text) else \"Non-AI\")\n\n# Filter out rows with null or zero salary and outliers \ndf_filtered_1 = df[\n    (df[\"SALARY\"].notnull()) &\n    (df[\"SALARY\"] &gt; 0)\n]\nq1 = df_filtered_1[\"SALARY\"].quantile(0.01)\nq99 = df_filtered_1[\"SALARY\"].quantile(0.99)\ndf_filtered_1 = df_filtered_1[(df_filtered_1[\"SALARY\"] &gt;= q1) & (df_filtered_1[\"SALARY\"] &lt;= q99)]\nprint(df_filtered_1[\"is_ai_job\"].value_counts())\n\n\nis_ai_job\nNon-AI    15948\nAI         5273\nName: count, dtype: int64\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nax3=sns.boxplot(\n    data=df_filtered_1, \n    x=\"is_ai_job\", \n    y=\"SALARY\",\n    hue=\"is_ai_job\",\n    legend=False,\n    palette=\"Set3\",\n    width=0.4\n)\n\nfor patch in ax3.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Job Type\", fontsize=9)\nplt.ylabel(\"Salary ($)\", fontsize=9)\nplt.tight_layout()\n#plt.show()\nout_path = \"figures/AI_v_nonAI_salary_boxplot.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\n\n\n\n\nFigure 6: Salary Distribution comparison between AI vs. Non-AI Jobs\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# KDE Plot\nplt.figure(figsize=(8, 5))\nax4=sns.kdeplot(\n    data=df_filtered_1, \n    x=\"SALARY\", \n    hue=\"is_ai_job\", \n    common_norm=False,\n    linewidth=3,\n    palette=\"Set2\",\n    alpha=0.8\n)\n\nfor patch in ax4.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Density: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Salary ($)\", fontsize=9)\nplt.ylabel(\"Density\", fontsize=9)\nplt.tight_layout()\n#plt.show()\nout_path = \"figures/AI_v_nonAI_salary_KDE.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\n\n\n\n\nFigure 7: Salary Density comparison between AI vs. Non-AI Jobs\n\n\nKey Insights: The salary comparison between AI and non-AI job postings shows a wage premium associated with roles that require AI-related skill sets. Analyzing 5273 AI roles and 15949 non-AI roles, the box plot shows that AI roles have a higher median salary ($115,000-$120,000) than non-AI roles ($105,000-$110,000). In terms of IQRs - both Q1 and Q3 for AI roles is positioned higher in comparison to non-AI roles. This suggests that roles that require expertise in AI-related skills generally pay better than the rest of the job market.\nThe KDE plot shows that AI roles have a steep upward slope from $20,000-$80,000 with job postings mostly clustering between $80,000-$120,000. This pattern likely means that there are less entry-level roles in AI and most roles are concentrated in the mid-salary range. This suggests that while there aren’t many jobs that reflect entry level salaries, AI-related job postings reflect a higher salary floor driven by a higher barrier of entry."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "To evaluate the current technical skill set for each team member, each team member performed a self-rating for a set of core competencies for data analytics and data science, which included Python, SQL, Power BI, Tableau, Excel, Machine Learning, Natural Language Processing (NLP), Cloud Computing, and AWS. The assessment was made on a five-point scale, and the rating values were then compiled into a dataframe for further analysis. We then leveraged a Seaborn heatmap to identify where our strengths concentrated and pinpoint where the gaps existed.\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 1, 3],\n    \"SQL\": [3, 3, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 3, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 2, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 8: Team Skill Levels Heatmap\n\n\nWhich skills should each member prioritize learning?\n\nAngelina – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).\n\nDevin – Solid in Excel (5) and Tableau (3), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.\n\nLeo – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions)."
  },
  {
    "objectID": "skill_gap_analysis.html#external-skills-assessment-using-natural-language-processing-techniques",
    "href": "skill_gap_analysis.html#external-skills-assessment-using-natural-language-processing-techniques",
    "title": "Skill Gap Analysis",
    "section": "2 External Skills Assessment using Natural Language Processing techniques",
    "text": "2 External Skills Assessment using Natural Language Processing techniques\nTo identify the most in-demand skills in the analytics job market, we analyzed the job descriptions from the Lightcast dataset by leveraging Natural Language Processing (NLP) techniques to process the BODY column which contained detailed job descriptions. Below is a summary of the data preparation and extraction process:\n\nText normalization: To ensure consistency, job descriptions were converted to lowercase, Unicode-normalized format, and whitespaces were stripped.\nTokenization: Numbers and punctuation were filtered out to ensure words can be captured and extracted properly.\nStopword removal: To further filter and concentrate only on meaningful technical words, common English stopwords were removed using Scikit-learn’s built-in stopword list.\nKeyword filtering: A predefined list of relevant analytics skills (e.g., Python, SQL, AWS, Tableau, Excel, Pandas, Spark, Machine Learning, NLP, Cloud Computing) was used to identify and count occurrences within the job descriptions.\nFrequency analysis: Using Python’s Counter() function, we tallied the frequency of each skill keyword and visualized in a column chart to understand their significance in the job market.\n\n\n\nCode\n## Extract most in-demand skills from JD (optimized)\nimport re\nimport os\nimport unicodedata\nfrom collections import Counter\n\n# Import stopwords (Angelina note: switched from NLTK to sklearn's built-in stopwords,\n# which avoids downloads and runs faster)\ntry:\n    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n    stop_words = set(ENGLISH_STOP_WORDS)\nexcept Exception:\n    # Minimal fallback if sklearn is missing\n    stop_words = {\n        \"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\",\n        \"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"were\",\"will\",\"with\"\n    }\n\n# Helper function to normalize text\n# (Angelina note: ensures Unicode normalized, casefolded, and whitespace trimmed)\ndef nfc_casefold_trim(s: str) -&gt; str:\n    s = unicodedata.normalize(\"NFC\", str(s)).casefold()\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n# Compile regex once (Angelina note: faster than re-compiling each loop)\nword_re = re.compile(r\"[a-z]+\")\n\n# Pull description from job postings and count words (streaming, no giant string build)\nprint(\"Scanning job descriptions and counting tokens (streaming)...\")\nwords_count = Counter()\n\nfor txt in df[\"BODY\"].dropna().astype(str):\n    t = nfc_casefold_trim(txt)\n    words_count.update(w for w in word_re.findall(t) if w not in stop_words)\n\n# Define a list of skills\nskills_list = {\n    \"python\", \"sql\", \"aws\", \"docker\", \"tableau\", \"excel\",\n    \"pandas\", \"numpy\", \"spark\", \"machine\", \"learning\",\n    \"nlp\", \"cloud\", \"computing\", \"power\"\n}\n\n# Extract only the predefined skills that appear in job postings\nskills_filtered = {s: words_count[s] for s in skills_list if words_count.get(s, 0) &gt; 0}\n\n# Print results, sorted by most frequent\nprint(\"Top data analytics skills from job descriptions\")\nfor skill, count in sorted(skills_filtered.items(), key=lambda kv: (-kv[1], kv[0])):\n    print(f\"{skill}:{count}\")\n\n\n    # --------- added simple bar chart ---------\nif skills_filtered:\n    os.makedirs(\"figures\", exist_ok=True)  \n    items = sorted(skills_filtered.items(), key=lambda kv: kv[1], reverse=True)\n    labels = [k for k, _ in items]\n    values = [v for _, v in items]\n\n    plt.figure(figsize=(8, 4.5))\n    plt.bar(labels, values)\n    plt.title(\"Most In-Demand Skills from Job Descriptions\")\n    plt.xlabel(\"Skill\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    out_path = \"figures/jd_top_skills.png\"\n    plt.savefig(out_path, dpi=150)\n    #plt.show()\n    plt.close()\n    #print(f\"[Saved chart] {out_path}\")\n\n\nScanning job descriptions and counting tokens (streaming)...\nTop data analytics skills from job descriptions\ncloud:42787\nsql:35871\npower:21894\nexcel:19874\nlearning:16361\ntableau:14609\npython:13402\naws:8510\nmachine:5592\ncomputing:2588\nspark:1496\ndocker:613\npandas:378\nnlp:293\nnumpy:187\n\n\n\n\n\nFigure 9: Most In-demand skills from Job Descriptions"
  },
  {
    "objectID": "skill_gap_analysis.html#propose-an-improvement-plan",
    "href": "skill_gap_analysis.html#propose-an-improvement-plan",
    "title": "Skill Gap Analysis",
    "section": "3 Propose an Improvement Plan",
    "text": "3 Propose an Improvement Plan\nAccording to our analysis, job postings show high demand for Cloud, SQL, Python, ML, and AWS. Our team is strong in visualization (Excel, Power BI, Tableau) but weaker in Cloud, ML, and NLP. This plan aligns our learning with market needs, provides specific resources, and ensures collaboration strategies so the whole team can close the gap together.\nWhat courses or resources can help?\n\nCloud & AWS – free cloud provider tutorials, AWS Educate, and cloud labs for hands-on practice.\n\nMachine Learning & NLP – scikit-learn tutorials, Kaggle competitions, and university modules on ML/NLP.\n\nPython & SQL – interactive platforms (Jupyter notebooks, LeetCode SQL), and official documentation.\n\nDocker & Spark – short online workshops, Spark quickstarts, and Docker “getting started” labs.\n\nHow can the team collaborate to bridge skill gaps?\n\nRole rotation: assign rotating leads (“cloud lead,” “ML/NLP lead,” “Python/SQL lead”) for mini-projects so each teammate practices outside their strengths.\n\nLightning talks: weekly 15-minute sessions where one teammate teaches a concept or tool they just learned.\n\nPair programming: match stronger members (for example, Angelina for visualization) with weaker ones (for example, Devin on Python) to share knowledge in real time.\n\nShared resources: maintain a team wiki with reusable queries, cloud setup notes, and code snippets."
  },
  {
    "objectID": "skill_gap_analysis.html#internal-skills-assessment",
    "href": "skill_gap_analysis.html#internal-skills-assessment",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "To evaluate the current technical skill set for each team member, each team member performed a self-rating for a set of core competencies for data analytics and data science, which included Python, SQL, Power BI, Tableau, Excel, Machine Learning, Natural Language Processing (NLP), Cloud Computing, and AWS. The assessment was made on a five-point scale, and the rating values were then compiled into a dataframe for further analysis. We then leveraged a Seaborn heatmap to identify where our strengths concentrated and pinpoint where the gaps existed.\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 1, 3],\n    \"SQL\": [3, 3, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 3, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 2, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 8: Team Skill Levels Heatmap\n\n\nWhich skills should each member prioritize learning?\n\nAngelina – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).\n\nDevin – Solid in Excel (5) and Tableau (3), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.\n\nLeo – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions)."
  },
  {
    "objectID": "final_analytics.html",
    "href": "final_analytics.html",
    "title": "Regression, Classification, and Topic Insights",
    "section": "",
    "text": "We performed KMeans clustering on job postings using core features (salary, minimum and maximum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n\nWe used KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n\n\nFigure 10: KMeans Clusters by Salary and Min Years Experience\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\n#display(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\n#print(\"\\nMost common label per cluster:\")\n#display(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\n#display(ct_share.head(50))\n\n\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) &gt; 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nout_path = \"figures/KMeans_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n\n\nKMeans labels column ready: True\nUnique clusters: 5\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n\n\n\n\n\nClustering on salary and minimum experience produced three clear market segments:\n\nEntry-level / lower salary\nMid-career / moderate salary\nSenior specialist / high salary\n\nThis segmentation aligns with career ladders and confirms the broad, positive relationship between experience and compensation.\nBusiness relevance\n\nJob seekers: Use cluster patterns to target industries/roles occupying higher-pay segments and to plan upskilling.\nEmployers: Map openings to cluster ranges to calibrate pay bands against the external market and reduce attrition risk.\n\n\n\n\nCode\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)"
  },
  {
    "objectID": "final_analytics.html#kmeans-clustering-analysis",
    "href": "final_analytics.html#kmeans-clustering-analysis",
    "title": "Regression, Classification, and Topic Insights",
    "section": "",
    "text": "We performed KMeans clustering on job postings using core features (salary, minimum and maximum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n\nWe used KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n\n\nFigure 10: KMeans Clusters by Salary and Min Years Experience\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\n#display(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\n#print(\"\\nMost common label per cluster:\")\n#display(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\n#display(ct_share.head(50))\n\n\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) &gt; 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nout_path = \"figures/KMeans_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n\n\nKMeans labels column ready: True\nUnique clusters: 5\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n\n\n\n\n\nClustering on salary and minimum experience produced three clear market segments:\n\nEntry-level / lower salary\nMid-career / moderate salary\nSenior specialist / high salary\n\nThis segmentation aligns with career ladders and confirms the broad, positive relationship between experience and compensation.\nBusiness relevance\n\nJob seekers: Use cluster patterns to target industries/roles occupying higher-pay segments and to plan upskilling.\nEmployers: Map openings to cluster ranges to calibrate pay bands against the external market and reduce attrition risk.\n\n\n\n\nCode\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)"
  },
  {
    "objectID": "final_analytics.html#regression-predicting-salary",
    "href": "final_analytics.html#regression-predicting-salary",
    "title": "Regression, Classification, and Topic Insights",
    "section": "2 Regression – Predicting Salary",
    "text": "2 Regression – Predicting Salary\nWe chose Random Forest over Linear Regression because the relationship between experience and salary is non-linear, and the model captures complex interactions more effectively.\nThe goal of this model is to predict job posting salary using experience and employment type features. The model uses an 80/20 train–test split and evaluates performance using RMSE and R² metrics.\n\n\nCode\n# Feature Selection and Data Preparation\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Use minimum years of experience, employment type (or closest available), and employment type as features\n\nfeatures = ['MIN_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']\n\n# One-hot encode employment type\ndf_encoded = pd.get_dummies(df, columns=['REMOTE_TYPE_NAME'], drop_first=True)\n\nX = df_encoded[[col for col in df_encoded.columns if col in features or col.startswith('REMOTE_TYPE_NAME_')]]\ny = df_encoded['SALARY']\n\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train/test split (70/30)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Drop rows where y (salary) is missing\nmask = ~y_train.isna()\nX_train_filtered = X_train.loc[mask]\ny_train_filtered = y_train.loc[mask]\n\nmask_test = ~y_test.isna()\nX_test_filtered = X_test.loc[mask_test]\ny_test_filtered = y_test.loc[mask_test]\n\n# Train a random forest regression model to predict salary\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_filtered, y_train_filtered)\n\n# Predict salaries for the test set\ny_pred = rf.predict(X_test_filtered)\n\n# Calculate RMSE (Root Mean Squared Error) and R² (coefficient of determination)\nmse = mean_squared_error(y_test_filtered, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_filtered, y_pred)\n\nprint(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\nprint(f'R² Score: {r2:.3f}')\n\n\nRoot Mean Squared Error (RMSE): 34744.38\nR² Score: 0.336\n\n\n\n2.1 Regression Results and Interpretation\nWe modeled salary using Minimum Years of Experience (MIN_YEARS_EXPERIENCE) and Remote Type (REMOTE_TYPE_NAME). We excluded MAX_YEARS_EXPERIENCE (&gt;85% missing) to protect statistical integrity; this reduced sample size but improved reliability.\nModel performance\n\nRMSE: 34,744.38\nR²: 0.336\n\nWhat this means An R² of 0.336 indicates experience and remote status explain a meaningful, but incomplete share of salary variation. That’s consistent with cross-industry labor data, where compensation is also driven by occupation, industry, location, and scarce technical capabilities.\nBusiness relevance\n\nFor job seekers: Experience contributes to higher earnings, but targeted specialization (e.g., analytics, cloud, data engineering) and industry selection are decisive for larger pay jumps.\nFor employers: Pricing talent solely by tenure can misalign offers in high-skill roles. Clear remote/on-site definitions in postings improve candidate signal and reduce noise in market benchmarking.\n\n\n\nCode\n# Visual: Predicted vs. Actual Salary\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_actual, y_pred_series, alpha=0.5)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Predicted vs. Actual Salary (Random Forest Regression)\")\nlo, hi = float(y_actual.min()), float(y_actual.max())\nplt.plot([lo, hi], [lo, hi], 'r--', label=\"Perfect Prediction\")\nplt.legend()\nplt.tight_layout()\nout_path = \"figures/Predicted_vs_Actual_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nPredicted vs. Actual Salary (Random Forest Regression)\n\n\n\n\nCode\n# Feature importance: shows which variables most influence salary prediction\nimportances = rf.feature_importances_\nfeature_names = X.columns\n\nfeat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 features influencing salary prediction:\")\nfor name, imp in feat_imp[:5]:\n    print(f\"{name}: {imp:.3f}\")\n\n\nTop 5 features influencing salary prediction:\nMIN_YEARS_EXPERIENCE: 0.953\nREMOTE_TYPE_NAME_Not Remote: 0.017\nREMOTE_TYPE_NAME_[None]: 0.015\nREMOTE_TYPE_NAME_Remote: 0.014\nREMOTE_TYPE_NAME_Unknown: 0.000\n\n\n\n\nCode\n# Visual: Feature Importance Bar Chart\nimport pandas as pd\n\nfeat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\nfeat_imp_df = feat_imp_df.sort_values('importance', ascending=False).head(5)\n\nplt.figure(figsize=(8, 5))\nplt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Top 5 Feature Importances (Random Forest Regression)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nout_path = \"figures/Top_5_Feature_Importances_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nTop 5 Feature Importances (Random Forest Regression)\n\n\n\n\nCode\n# Calculate absolute prediction error for each job\nimport pandas as pd\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\nerrors = (y_actual - y_pred_series).abs()\noutlier_indices = errors.nlargest(5).index\n\n# Show the original job posting rows for these outliers\ncols = ['SALARY', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE_NAME', 'REMOTE_TYPE_NAME']\ncols = [c for c in cols if c in df.columns]\nprint(df.loc[outlier_indices, cols])\n\n\n         SALARY  MIN_YEARS_EXPERIENCE REMOTE_TYPE_NAME\n10189  312500.0                   1.0           [None]\n19209  312000.0                   1.0           [None]\n31552  328600.0                   5.0           [None]\n2258   305000.0                   3.0           [None]\n29554  338750.0                   7.0           [None]\n\n\n\n\nCode\n# Visual: Highlight Outliers in Predicted vs. Actual Salary\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nemp_col = next((c for c in [\"EMPLOYMENT_TYPE_NAME\",\"REMOTE_TYPE_NAME\",\"STATE_NAME\",\"MSA_NAME\"] if c in df.columns), None)\n\nsubset_cols = [\"SALARY\",\"MIN_YEARS_EXPERIENCE\"] + ([emp_col] if emp_col else [])\nsubset = df.loc[outlier_indices, subset_cols].copy()\n\ndef _fmt_sal(x): \n    return f\"${int(x):,}\" if pd.notna(x) else \"N/A\"\n\ndef _fmt_exp(x): \n    return f\"{int(x)}yr min\" if pd.notna(x) else \"n/a\"\n\ndef _fmt_emp(x): \n    return str(x) if pd.notna(x) else \"n/a\"\n\nif emp_col:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\\n{_fmt_emp(e)}\"\n        for s, m, e in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"], subset[emp_col])\n    ]\nelse:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\"\n        for s, m in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"])\n    ]\n\nplt.figure(figsize=(10, 5))\nplt.bar(labels, subset[\"SALARY\"].fillna(0))\nplt.ylabel(\"Actual Salary\")\nplt.title(\"Top 5 Outlier Job Salaries\")\nplt.tight_layout()\nout_path = \"figures/Top_5_Outlier_Job_Salaries.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n2.2 Outlier Jobs and Market Signals\nThe largest residuals include postings above $300,000 with only 1–5 years minimum experience, often missing a remote/on-site label. These cases point to specialized, high-impact roles where compensation is decoupled from tenure (e.g., advanced analytics, cloud/platform, niche leadership).\nA small number of postings show very high pay at low experience with unspecified remote type. These likely reflect specialized roles or incomplete records. In practice, validate or isolate these cases before setting salary bands or training production models.\n\n\n\nTop 5 Outlier Job Salaries\n\n\nBusiness relevance\n\nEmployers: Standardize and QA high-pay, low-tenure postings, unclear fields and atypical mixes should be reviewed before publication.\nJob seekers: Outliers highlight skill paths where focused upskilling can command premium pay earlier in a career."
  },
  {
    "objectID": "final_analytics.html#outlier-jobs-and-market-signals",
    "href": "final_analytics.html#outlier-jobs-and-market-signals",
    "title": "Regression, Classification, and Topic Insights",
    "section": "3 Outlier Jobs and Market Signals",
    "text": "3 Outlier Jobs and Market Signals\nThe largest residuals include postings above $300,000 with only 1–5 years minimum experience, often missing a remote/on-site label. These cases point to specialized, high-impact roles where compensation is decoupled from tenure (e.g., advanced analytics, cloud/platform, niche leadership).\nBusiness relevance\n\nEmployers: Standardize and QA high-pay, low-tenure postings, unclear fields and atypical mixes should be reviewed before publication.\nJob seekers: Outliers highlight skill paths where focused upskilling can command premium pay earlier in a career."
  },
  {
    "objectID": "final_analytics.html#classification-predicting-remote-vs-on-site-job",
    "href": "final_analytics.html#classification-predicting-remote-vs-on-site-job",
    "title": "Regression, Classification, and Topic Insights",
    "section": "3 Classification – Predicting Remote vs On-Site Job",
    "text": "3 Classification – Predicting Remote vs On-Site Job\nWe trained a logistic regression to predict Remote vs On-Site using MIN_YEARS_EXPERIENCE plus an available categorical descriptor (e.g., employment type or state).\n\n\nCode\n# Classification: Predicting Remote vs On-Site Jobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Prepare data\nemp_col = \"EMPLOYMENT_TYPE_NAME\" if \"EMPLOYMENT_TYPE_NAME\" in df.columns else (\"STATE_NAME\" if \"STATE_NAME\" in df.columns else (\"MSA_NAME\" if \"MSA_NAME\" in df.columns else None))\nif emp_col is not None:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", emp_col, \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nelse:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nclf_df[\"IS_REMOTE\"] = clf_df[\"REMOTE_TYPE_NAME\"].str.contains(\"Remote\", case=False, na=False).astype(int)\n\nX = clf_df[[\"MIN_YEARS_EXPERIENCE\", emp_col]].copy() if emp_col is not None else clf_df[[\"MIN_YEARS_EXPERIENCE\"]].copy() # Removed MAX_YEARS_EXPERIENCE\ny = clf_df[\"IS_REMOTE\"]\n\n# One-hot encode employment type\nif emp_col is not None:\n    preprocessor = ColumnTransformer([\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [emp_col]),\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\nelse:\n    preprocessor = ColumnTransformer([\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Model\nclf = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n])\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\n# Metrics\nacc = accuracy_score(y_test, pred)\nf1 = f1_score(y_test, pred)\nprint(f\"Accuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n\n# Confusion matrix\nConfusionMatrixDisplay(confusion_matrix(y_test, pred)).plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix — Remote vs On-Site Classification\")\nout_path = \"figures/Confusion_Matrix_Remote_vs_On-Site.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\nAccuracy: 0.619, F1 Score: 0.328\n\n\n\n\n\nConfusion Matrix — Remote vs On-Site Classification\n\n\n\n3.1 Classification Results and Interpretation\nPerformance:\n\nAccuracy: 0.622\nF1 Score: 0.327\n\nWhat this means: The model distinguishes remote vs. on-site at a moderate level, consistent with the idea that remote status is policy/role-design driven, not primarily a function of required experience.\nBusiness relevance:\n\nEmployers: Remote flexibility can be offered across experience levels without materially distorting supply. If remote status is strategic, emphasize role/industry signals rather than tenure in postings.\nJob seekers: Remote options exist from entry to senior levels, broadening geographic reach and negotiation leverage."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "1 Conclusion & Recommendation",
    "section": "",
    "text": "This project, which analyzed the 2024 job market, provided key insights into the factors influencing compensation and future workforce development. Our analysis confirms that the employment landscape is being fundamentally reshaped by technology and the demand for flexible work, creating both challenges and opportunities for job seekers and employers.\n\n\nSkill-Driven Wage Premiums Our Natural Language Processing (NLP) analysis identified a clear difference between the team’s strengths and current market demand. While the team performs well in visualization tools such as Excel, Power BI, and Tableau, the job market places greater emphasis on Cloud Computing, SQL, Python, and Machine Learning (ML). The frequency of these skills in job postings shows that focused development in these areas supports higher compensation and continued competitiveness in analytics roles. Experience as the Primary Salary Driver The Random Forest Regression model achieved an RMSE of 34,744.38 and an R² of 0.336, identifying Minimum Years of Experience as the most influential feature in predicting salary, with a feature importance of approximately 0.953. This result indicates that, while technical specialization enhances access to well-paid roles, accumulated experience continues to be the main factor influencing salary progression. The Segmented Job Market The KMeans Clustering model grouped job postings into five clusters, summarized into three main tiers—Entry-Level (lower salary), Mid-Career (moderate salary), and Senior Specialist (higher salary). This pattern aligns with traditional career development paths and suggests that job seekers should aim for higher-tier roles by combining experience with proficiency in high-demand skills such as Cloud Computing, Python, and Machine Learning. Remote Work Status is Not Experience-Dependent The Logistic Regression model achieved an accuracy of 0.622 and an F1 score of 0.327, showing moderate predictive strength when using experience to classify remote versus on-site jobs. This suggests that remote flexibility depends more on company policy or role design than on years of experience. Remote opportunities appear across all experience tiers, providing both entry-level and senior professionals with broader access to roles.\n\n\n\nBased on these findings, the following strategies are recommended for both job seekers (the team) and employers: Continue Upskilling in Cloud and Machine Learning Efforts should focus on further developing Cloud/AWS and Machine Learning/NLP skills. These areas show the greatest opportunity for alignment with market needs and can improve access to higher-tier roles. Standardize High-Value Job Postings Employers should review and standardize postings with high salaries and lower experience requirements. Accurate labeling of REMOTE_TYPE and clear definitions of required technical skills will improve market consistency, strengthen salary benchmarking, and attract well-matched candidates. Use Remote Flexibility to Expand Talent Pools Since remote status does not strongly depend on experience, employers can apply flexible work policies to reach qualified candidates at all career levels. This approach supports broader hiring while maintaining consistency in pay structures."
  },
  {
    "objectID": "conclusion.html#key-findings-and-market-dynamics",
    "href": "conclusion.html#key-findings-and-market-dynamics",
    "title": "1 Conclusion & Recommendation",
    "section": "",
    "text": "Skill-Driven Wage Premiums Our Natural Language Processing (NLP) analysis identified a clear difference between the team’s strengths and current market demand. While the team performs well in visualization tools such as Excel, Power BI, and Tableau, the job market places greater emphasis on Cloud Computing, SQL, Python, and Machine Learning (ML). The frequency of these skills in job postings shows that focused development in these areas supports higher compensation and continued competitiveness in analytics roles. Experience as the Primary Salary Driver The Random Forest Regression model achieved an RMSE of 34,744.38 and an R² of 0.336, identifying Minimum Years of Experience as the most influential feature in predicting salary, with a feature importance of approximately 0.953. This result indicates that, while technical specialization enhances access to well-paid roles, accumulated experience continues to be the main factor influencing salary progression. The Segmented Job Market The KMeans Clustering model grouped job postings into five clusters, summarized into three main tiers—Entry-Level (lower salary), Mid-Career (moderate salary), and Senior Specialist (higher salary). This pattern aligns with traditional career development paths and suggests that job seekers should aim for higher-tier roles by combining experience with proficiency in high-demand skills such as Cloud Computing, Python, and Machine Learning. Remote Work Status is Not Experience-Dependent The Logistic Regression model achieved an accuracy of 0.622 and an F1 score of 0.327, showing moderate predictive strength when using experience to classify remote versus on-site jobs. This suggests that remote flexibility depends more on company policy or role design than on years of experience. Remote opportunities appear across all experience tiers, providing both entry-level and senior professionals with broader access to roles."
  },
  {
    "objectID": "conclusion.html#strategic-recommendations",
    "href": "conclusion.html#strategic-recommendations",
    "title": "1 Conclusion & Recommendation",
    "section": "",
    "text": "Based on these findings, the following strategies are recommended for both job seekers (the team) and employers: Continue Upskilling in Cloud and Machine Learning Efforts should focus on further developing Cloud/AWS and Machine Learning/NLP skills. These areas show the greatest opportunity for alignment with market needs and can improve access to higher-tier roles. Standardize High-Value Job Postings Employers should review and standardize postings with high salaries and lower experience requirements. Accurate labeling of REMOTE_TYPE and clear definitions of required technical skills will improve market consistency, strengthen salary benchmarking, and attract well-matched candidates. Use Remote Flexibility to Expand Talent Pools Since remote status does not strongly depend on experience, employers can apply flexible work policies to reach qualified candidates at all career levels. This approach supports broader hiring while maintaining consistency in pay structures."
  }
]