---
title: "Data Cleaning & Exploration"
execute:
  kernel: ad688-venv
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---

# Methodology
Below are the steps we took to prepare the dataset to ensure accuracy and statistical integrity in our exploratory data analysis (EDA). 

## Handling of missing values 
We addressed missing values based on field type and the amount of data missing per field: 

### Numerical Fields
Missing values in key numerical fields were imputed with the median of each respective fields. Since these numerical fields are key for grouping and visualizing trends, simply removing empty rows could reduce the diversity of the dataset and introduce biases. The rationale for imputing the median rather than the mean is that the latter tends to be influenced by outliers and skewed distributions, which is often exhibited in fields like "SALARY. Based on the downstream EDA, additional filtering may be applied to exclude imputed values altogether to prevent distorition and ensure statistical integrity in the trends observed. 

### Categorical Fields
Missing values in key categorical fields were imputed with the placeholder value "Unknown" in order to prevent those rows of data being dropped, leading to unnecessary data loss. By imputing a neutral label like "Unknown", this strategy ensures that data integrity is retained without introducing false assumptions and biases into the downstream analysis. 

### Columns containing Majority Missing Data
While the above addressed the rationale for imputing missing values, having to impute the majority of a column's values can also create noise, which provides insignificant information and analytical value to the downstream analysis. Therefore, columns containing more than 50% missing values were excluded from the dataset. 
  
## Remove Unnecessary Columns
Columns containing redundant and irrelevant information such as outdated NAICS/SOC codes and tracking metadata were excluded from the dataset. Since the scope of this analysis is focused on job market trends in 2024, it is best practice to remove any outdated NAICS/SOC fields to prevent confusion and duplication. Similarly, metadata fields that could introduce ambiguity and do not add any meaningful to downstream analysis are excluded. 

## Remove Duplicates 
To eliminate true duplicates from the dataset, job listings that have identical values in all the fields listed below were removed to prevent distortion and ensure statistical integrity: 
  - Job Title
  - Company Name
  - Location
  - Posting Date
  - Skill requirements
  - Employment type

## Code Block
```{python}
import pandas as pd

# Load lightcast_job_postings.csv 
df = pd.read_csv("data/lightcast_job_postings.csv")
df.head()
df.columns.tolist()

# Drop columns
columns_to_drop = [
  # tracking & other metadata
    "ID", "LAST_UPDATED_DATE", "LAST_UPDATED_TIMESTAMP", "DUPLICATES",
    "SOURCE_TYPES", "SOURCES", "URL", "ACTIVE_URLS", "ACTIVE_SOURCES_INFO", "MODELED_EXPIRED", "MODELED_DURATION", "TITLE_RAW",
  # outdated NAICS and SOC codes
    "NAICS2", "NAICS2_NAME", "NAICS3", "NAICS3_NAME",
    "NAICS4", "NAICS4_NAME", "NAICS5", "NAICS5_NAME",
    "NAICS6", "NAICS6_NAME", 
    "SOC_2", "SOC_2_NAME", "SOC_3", "SOC_3_NAME",
    "SOC_4", "SOC_4_NAME", "SOC_5", "SOC_5_NAME",
    "SOC_2021_2", "SOC_2021_2_NAME", "SOC_2021_3", "SOC_2021_3_NAME",
    "SOC_2021_5", "SOC_2021_5_NAME",
    "NAICS_2022_2", "NAICS_2022_2_NAME", "NAICS_2022_3", "NAICS_2022_3_NAME",
    "NAICS_2022_4", "NAICS_2022_4_NAME", "NAICS_2022_5", "NAICS_2022_5_NAME"
]
df.drop(columns=columns_to_drop, inplace=True)
df.info()
```

```{python}
import missingno as msno
import matplotlib.pyplot as plt

# Visualize missing data using missingno bar graph
plt.figure(figsize=(12, 6))
msno.bar(df)
plt.title("Missing Data Bar Chart")
plt.tight_layout()
plt.show()

# Identify columns that have a significant amount of missing values and sort by the percentage of missing values
missing_values_pct = (df.isna().mean() * 100).sort_values(ascending=False).reset_index()
missing_values_pct.columns = ["Column", "Missing %"]
print(missing_values_pct.to_string(index=False))
```

```{python}
# Fill in missing values for SALARY, INDUSTRY, and other relevant columns
# Fill categorical columns with "Unknown"
fill_col_unk = [
    "EXPIRED", "MSA_INCOMING", "MSA_NAME_INCOMING", "MSA", "MSA_OUTGOING", "MSA_NAME", "COMPANY_RAW", "TITLE_CLEAN", "TITLE", "TITLE_NAME", "COMPANY_NAME", "COMPANY_IS_STAFFING", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "EDUCATION_LEVELS_NAME", "MIN_EDULEVELS_NAME", "SKILLS_NAME", "SPECIALIZED_SKILLS_NAME", "CERTIFICATIONS_NAME", "STATE_NAME", "CITY_NAME", "COUNTY_NAME"
]
# Loop through and fill missing values
for col in fill_col_unk:
    df[col] = df[col].fillna("Unknown")

# Do the same for relevant numerical columns, but fill with median
fill_col_median = [
    "SALARY", "SALARY_FROM", "SALARY_TO", "DURATION", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE"
]
for col in fill_col_median:
    df[col] = df[col].fillna(df[col].median())

# Drop columns with >50% missing values
df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)
df.info()
```

```{python}
# Remove duplicate
df=df.drop_duplicates(subset=["TITLE_CLEAN", "COMPANY_NAME", "LOCATION", "POSTED", "EMPLOYMENT_TYPE_NAME", "REMOTE_TYPE_NAME", "SKILLS_NAME"], keep="first")

# Preview new df
df.shape
df.head()
```
