{
  "hash": "985697c6137ff0ea99a05d80aa4509b1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Regression, Classification, and Topic Insights\n---\n\n\n\n## KMeans Clustering Analysis\nWe performed KMeans clustering on job postings using core features (salary, minimum and maximum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n### Fit KMeans and Assign Clusters - Data Prep\nWe used KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n![Figure 10: KMeans Clusters by Salary and Min Years Experience](figures/KMeans_Cluster.png)\n\n\n::: {#ec7fb868 .cell execution_count=1}\n``` {.python .cell-code}\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n```\n:::\n\n\n::: {#a39ff82c .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n```\n:::\n\n\n::: {#23a8fb6f .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\n#display(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\n#print(\"\\nMost common label per cluster:\")\n#display(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\n#display(ct_share.head(50))\n\n\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) > 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nout_path = \"figures/KMeans_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKMeans labels column ready: True\nUnique clusters: 5\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n```\n:::\n:::\n\n\n### K-Means Clustering Summary\nClustering on salary and minimum experience produced three clear market segments:\n\n- Entry-level / lower salary\n- Mid-career / moderate salary\n- Senior specialist / high salary\n\nThis segmentation aligns with career ladders and confirms the broad, positive relationship between experience and compensation.\n\n**Business relevance**\n\n- **Job seekers**: Use cluster patterns to target industries/roles occupying higher-pay segments and to plan upskilling.\n- **Employers**: Map openings to cluster ranges to calibrate pay bands against the external market and reduce attrition risk.\n\n<!-- ```{python}\n# --- Bar chart for the top 5 titles in Cluster 0 (change cluster_num as needed) ---\ncluster_num = 0\ntop_titles = df[df['KMEANS_CLUSTER'] == cluster_num]['TITLE_CLEAN'].value_counts().head(5)\n\nplt.figure(figsize=(8, 5))\nbars = top_titles.plot(kind='bar', color='skyblue')\nplt.title(f'Top 5 Job Titles in Cluster {cluster_num}')\nplt.xlabel('Job Title')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right') \nout_path = \"figures/Top_5_Job_Titles_in_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n\n# Add the counts on top of the bars\nfor idx, value in enumerate(top_titles.values):\n    plt.text(idx, value + max(top_titles.values)*0.01, str(value), ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\n#plt.show()\nplt.close()\n#![Top 5 Job Titles in Cluster 0](figures/Top_5_Job_Titles_in_Cluster.png)\n``` -->\n\n::: {#ad10b414 .cell execution_count=4}\n``` {.python .cell-code}\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)\n```\n:::\n\n\n## Regression – Predicting Salary\nWe chose Random Forest over Linear Regression because the relationship between experience and salary is non-linear, and the model captures complex interactions more effectively.\n\nThe goal of this model is to predict job posting salary using experience and employment type features. The model uses an 80/20 train–test split and evaluates performance using RMSE and R² metrics.\n\n::: {#15dd8b10 .cell execution_count=5}\n``` {.python .cell-code}\n# Feature Selection and Data Preparation\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Use minimum years of experience, employment type (or closest available), and employment type as features\n\nfeatures = ['MIN_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']\n\n# One-hot encode employment type\ndf_encoded = pd.get_dummies(df, columns=['REMOTE_TYPE_NAME'], drop_first=True)\n\nX = df_encoded[[col for col in df_encoded.columns if col in features or col.startswith('REMOTE_TYPE_NAME_')]]\ny = df_encoded['SALARY']\n\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train/test split (70/30)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n```\n:::\n\n\n::: {#aaa00ab3 .cell execution_count=6}\n``` {.python .cell-code}\n# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Drop rows where y (salary) is missing\nmask = ~y_train.isna()\nX_train_filtered = X_train.loc[mask]\ny_train_filtered = y_train.loc[mask]\n\nmask_test = ~y_test.isna()\nX_test_filtered = X_test.loc[mask_test]\ny_test_filtered = y_test.loc[mask_test]\n\n# Train a random forest regression model to predict salary\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_filtered, y_train_filtered)\n\n# Predict salaries for the test set\ny_pred = rf.predict(X_test_filtered)\n\n# Calculate RMSE (Root Mean Squared Error) and R² (coefficient of determination)\nmse = mean_squared_error(y_test_filtered, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_filtered, y_pred)\n\nprint(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\nprint(f'R² Score: {r2:.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot Mean Squared Error (RMSE): 34744.38\nR² Score: 0.336\n```\n:::\n:::\n\n\n### Regression Results and Interpretation\nWe modeled salary using Minimum Years of Experience (MIN_YEARS_EXPERIENCE) and Remote Type (REMOTE_TYPE_NAME). We excluded MAX_YEARS_EXPERIENCE (>85% missing) to protect statistical integrity; this reduced sample size but improved reliability.\n\n**Model performance**\n\n- **RMSE**: 34,744.38\n- **R²**: 0.336\n\n**What this means**\nAn R² of 0.336 indicates experience and remote status explain a meaningful, but incomplete share of salary variation. That’s consistent with cross-industry labor data, where compensation is also driven by occupation, industry, location, and scarce technical capabilities.\n\n**Business relevance**\n\n- **For job seekers**: Experience contributes to higher earnings, but targeted specialization (e.g., analytics, cloud, data engineering) and industry selection are decisive for larger pay jumps.\n- **For employers**: Pricing talent solely by tenure can misalign offers in high-skill roles. Clear remote/on-site definitions in postings improve candidate signal and reduce noise in market benchmarking.\n\n::: {#51d1646a .cell execution_count=7}\n``` {.python .cell-code}\n# Visual: Predicted vs. Actual Salary\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_actual, y_pred_series, alpha=0.5)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Predicted vs. Actual Salary (Random Forest Regression)\")\nlo, hi = float(y_actual.min()), float(y_actual.max())\nplt.plot([lo, hi], [lo, hi], 'r--', label=\"Perfect Prediction\")\nplt.legend()\nplt.tight_layout()\nout_path = \"figures/Predicted_vs_Actual_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n```\n:::\n\n\n![Predicted vs. Actual Salary (Random Forest Regression)](figures/Predicted_vs_Actual_RF_Regression.png)\n\n::: {#d52ee3fa .cell execution_count=8}\n``` {.python .cell-code}\n# Feature importance: shows which variables most influence salary prediction\nimportances = rf.feature_importances_\nfeature_names = X.columns\n\nfeat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 features influencing salary prediction:\")\nfor name, imp in feat_imp[:5]:\n    print(f\"{name}: {imp:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 5 features influencing salary prediction:\nMIN_YEARS_EXPERIENCE: 0.953\nREMOTE_TYPE_NAME_Not Remote: 0.017\nREMOTE_TYPE_NAME_[None]: 0.015\nREMOTE_TYPE_NAME_Remote: 0.014\nREMOTE_TYPE_NAME_Unknown: 0.000\n```\n:::\n:::\n\n\n::: {#00b1c95c .cell execution_count=9}\n``` {.python .cell-code}\n# Visual: Feature Importance Bar Chart\nimport pandas as pd\n\nfeat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\nfeat_imp_df = feat_imp_df.sort_values('importance', ascending=False).head(5)\n\nplt.figure(figsize=(8, 5))\nplt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Top 5 Feature Importances (Random Forest Regression)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nout_path = \"figures/Top_5_Feature_Importances_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n```\n:::\n\n\n![Top 5 Feature Importances (Random Forest Regression)](figures/Top_5_Feature_Importances_RF_Regression.png)\n\n::: {#5f2ba9bb .cell execution_count=10}\n``` {.python .cell-code}\n# Calculate absolute prediction error for each job\nimport pandas as pd\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\nerrors = (y_actual - y_pred_series).abs()\noutlier_indices = errors.nlargest(5).index\n\n# Show the original job posting rows for these outliers\ncols = ['SALARY', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE_NAME', 'REMOTE_TYPE_NAME']\ncols = [c for c in cols if c in df.columns]\nprint(df.loc[outlier_indices, cols])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         SALARY  MIN_YEARS_EXPERIENCE REMOTE_TYPE_NAME\n10189  312500.0                   1.0           [None]\n19209  312000.0                   1.0           [None]\n31552  328600.0                   5.0           [None]\n2258   305000.0                   3.0           [None]\n29554  338750.0                   7.0           [None]\n```\n:::\n:::\n\n\n::: {#a7cb2dcd .cell execution_count=11}\n``` {.python .cell-code}\n# Visual: Highlight Outliers in Predicted vs. Actual Salary\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nemp_col = next((c for c in [\"EMPLOYMENT_TYPE_NAME\",\"REMOTE_TYPE_NAME\",\"STATE_NAME\",\"MSA_NAME\"] if c in df.columns), None)\n\nsubset_cols = [\"SALARY\",\"MIN_YEARS_EXPERIENCE\"] + ([emp_col] if emp_col else [])\nsubset = df.loc[outlier_indices, subset_cols].copy()\n\ndef _fmt_sal(x): \n    return f\"${int(x):,}\" if pd.notna(x) else \"N/A\"\n\ndef _fmt_exp(x): \n    return f\"{int(x)}yr min\" if pd.notna(x) else \"n/a\"\n\ndef _fmt_emp(x): \n    return str(x) if pd.notna(x) else \"n/a\"\n\nif emp_col:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\\n{_fmt_emp(e)}\"\n        for s, m, e in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"], subset[emp_col])\n    ]\nelse:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\"\n        for s, m in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"])\n    ]\n\nplt.figure(figsize=(10, 5))\nplt.bar(labels, subset[\"SALARY\"].fillna(0))\nplt.ylabel(\"Actual Salary\")\nplt.title(\"Top 5 Outlier Job Salaries\")\nplt.tight_layout()\nout_path = \"figures/Top_5_Outlier_Job_Salaries.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n```\n:::\n\n\n### Outlier Jobs and Market Signals\nThe largest residuals include postings above $300,000 with only 1–5 years minimum experience, often missing a remote/on-site label. These cases point to specialized, high-impact roles where compensation is decoupled from tenure (e.g., advanced analytics, cloud/platform, niche leadership).\n\nA small number of postings show very high pay at low experience with unspecified remote type. These likely reflect specialized roles or incomplete records. In practice, validate or isolate these cases before setting salary bands or training production models.\n\n![Top 5 Outlier Job Salaries](figures/Top_5_Outlier_Job_Salaries.png)\n\n**Business relevance**\n\n- **Employers**: Standardize and QA high-pay, low-tenure postings, unclear fields and atypical mixes should be reviewed before publication.\n- **Job seekers**: Outliers highlight skill paths where focused upskilling can command premium pay earlier in a career.\n\n## Classification – Predicting Remote vs On-Site Job\nWe trained a logistic regression to predict Remote vs On-Site using MIN_YEARS_EXPERIENCE plus an available categorical descriptor (e.g., employment type or state).\n\n::: {#693c30e7 .cell execution_count=12}\n``` {.python .cell-code}\n# Classification: Predicting Remote vs On-Site Jobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Prepare data\nemp_col = \"EMPLOYMENT_TYPE_NAME\" if \"EMPLOYMENT_TYPE_NAME\" in df.columns else (\"STATE_NAME\" if \"STATE_NAME\" in df.columns else (\"MSA_NAME\" if \"MSA_NAME\" in df.columns else None))\nif emp_col is not None:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", emp_col, \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nelse:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nclf_df[\"IS_REMOTE\"] = clf_df[\"REMOTE_TYPE_NAME\"].str.contains(\"Remote\", case=False, na=False).astype(int)\n\nX = clf_df[[\"MIN_YEARS_EXPERIENCE\", emp_col]].copy() if emp_col is not None else clf_df[[\"MIN_YEARS_EXPERIENCE\"]].copy() # Removed MAX_YEARS_EXPERIENCE\ny = clf_df[\"IS_REMOTE\"]\n\n# One-hot encode employment type\nif emp_col is not None:\n    preprocessor = ColumnTransformer([\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [emp_col]),\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\nelse:\n    preprocessor = ColumnTransformer([\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Model\nclf = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n])\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\n# Metrics\nacc = accuracy_score(y_test, pred)\nf1 = f1_score(y_test, pred)\nprint(f\"Accuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n\n# Confusion matrix\nConfusionMatrixDisplay(confusion_matrix(y_test, pred)).plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix — Remote vs On-Site Classification\")\nout_path = \"figures/Confusion_Matrix_Remote_vs_On-Site.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.619, F1 Score: 0.328\n```\n:::\n:::\n\n\n![Confusion Matrix — Remote vs On-Site Classification](figures/Confusion_Matrix_Remote_vs_On-Site.png)\n\n### Classification Results and Interpretation\n**Performance:**\n\n- **Accuracy**: 0.622\n- **F1 Score**: 0.327\n\n**What this means:**\nThe model distinguishes remote vs. on-site at a moderate level, consistent with the idea that remote status is policy/role-design driven, not primarily a function of required experience.\n\n**Business relevance:**\n\n- **Employers:** Remote flexibility can be offered across experience levels without materially distorting supply. If remote status is strategic, emphasize role/industry signals rather than tenure in postings.\n- **Job seekers:** Remote options exist from entry to senior levels, broadening geographic reach and negotiation leverage.\n\n",
    "supporting": [
      "final_analytics_files"
    ],
    "filters": [],
    "includes": {}
  }
}