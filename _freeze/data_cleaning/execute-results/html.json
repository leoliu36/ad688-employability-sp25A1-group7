{
  "hash": "b8ec9b2ff6794d729760094201353cfe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Methodology\n---\n\n\nBelow are the steps we took to prepare the dataset to ensure accuracy and statistical integrity in our exploratory data analysis (EDA). \n\n## Remove Unnecessary Columns\nFirstly, columns containing redundant and irrelevant information were excluded from the dataset. Since the scope of this analysis is focused on job market trends in 2024, it is best practice to remove any outdated NAICS/SOC fields to prevent confusion and duplication. Similarly, metadata fields or duplicate fields that could introduce ambiguity and do not add any meaningful to downstream analysis are excluded. To summarize, unnecessary columns containing the following information were dropped:\n\n- Meta/tracking\n- Duplicated location info\n- Raw/duplicate title & body\n- Duplicated employment info\n- Education code columns\n- Redundant NAICS/SOC versions\n- LOT/V6 occupation hierarchy\n- ONET & CIP codes \n- Sectors\n\n## Handling of missing values \nWe addressed missing values based on field type and the amount of data missing per field: \n\n### Numerical Fields\nMissing values in key numerical fields were imputed with the median of each respective fields. Since these numerical fields are key for grouping and visualizing trends, simply removing empty rows could reduce the diversity of the dataset and introduce biases. The rationale for imputing the median rather than the mean is that the latter tends to be influenced by outliers and skewed distributions, which is often exhibited in fields like \"SALARY. Based on the downstream EDA, additional filtering may be applied to exclude imputed values altogether to prevent distorition and ensure statistical integrity in the trends observed. \n\n### Categorical Fields\nMissing values in key categorical fields were imputed with the placeholder value \"Unknown\" in order to prevent those rows of data being dropped, leading to unnecessary data loss. By imputing a neutral label like \"Unknown\", this strategy ensures that data integrity is retained without introducing false assumptions and biases into the downstream analysis. \n\n### Columns containing Majority Missing Data\nWhile the above addressed the rationale for imputing missing values, having to impute the majority of a column's values can also create noise, which provides insignificant information and analytical value to the downstream analysis. Therefore, columns containing more than 50% missing values were excluded from the dataset. \n\n![Figure 1: Non-null Data](figures/non-null_data.png)\n\n\n::: {#d01ed07e .cell execution_count=1}\n``` {.python .cell-code}\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n```\n:::\n\n\n::: {#f5d8999e .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load lightcast_job_postings.csv \ndf = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\n# Show the first 5 rows \n#print(df.head(5).to_string())\n\n# Drop columns\ncolumns_to_drop = [\n    # Meta/tracking\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\", \"URL\",\n    \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Duplicated location info\n    \"LOCATION\", \"CITY\", \"STATE\", \"COUNTY\", \"COUNTY_NAME\",\n    \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_NAME_OUTGOING\", \"MSA_INCOMING\", \"MSA_NAME_INCOMING\",\n\n    # Raw/duplicate title & body\n    \"TITLE_RAW\", \"TITLE_NAME\",\n\n    # Duplicated employment info\n    \"EMPLOYMENT_TYPE\", \"EMPLOYMENT_TYPE_NAME\",\n\n    # Education code columns\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\", \"MIN_EDULEVELS\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS/SOC versions\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\",\n    \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_4\", \"SOC_4_NAME\",\n    \"SOC_5\",  # keep SOC_5_NAME, drop code\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\",\n    \"SOC_2021_4\", \"SOC_2021_4_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n\n    # LOT/V6 occupation hierarchy (keep only 1 specialized name field)\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n\n    # ONET & CIP codes \n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"CIP2\", \"CIP2_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP6\", \"CIP6_NAME\",\n\n    # Sectors\n    \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\",\n\n    # NAICS 2022 lower-level codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"NAICS_2022_4\", \"NAICS_2022_4_NAME\", \"NAICS_2022_5\", \"NAICS_2022_5_NAME\",\n    \"NAICS_2022_6\",  # drop code, keep name\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n#df.info()\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Identify columns that have a significant amount of missing values and sort df by the percentage of missing values\nmissing_percent = df.isnull().mean().sort_values(ascending=False)*100\ndf_sorted = df[missing_percent.index]\n\n# Visualize missing data using missingno bar chart \nplt.figure(figsize=(12, 6))\nmsno.bar(df_sorted)\nplt.title(\"Non-null Data Bar Chart\")\nplt.tight_layout()\nout_path = \"figures/non-null_data.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\nmissing_values_pct = (missing_percent.reset_index().rename(columns={\"index\": \"Column\", 0: \"Missing %\"}))\nprint(missing_values_pct.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Column  Missing %\n           MAX_YEARS_EXPERIENCE  88.372093\n             MAX_EDULEVELS_NAME  77.495931\n                         SALARY  57.505035\n                      SALARY_TO  55.311871\n            ORIGINAL_PAY_PERIOD  55.311871\n                    SALARY_FROM  55.311871\n                       DURATION  37.678281\n           MIN_YEARS_EXPERIENCE  31.926398\n               MODELED_DURATION  26.607631\n                MODELED_EXPIRED  21.264035\n                        EXPIRED  10.819609\n                       MSA_NAME   5.447047\n                    COMPANY_RAW   0.746227\n                    TITLE_CLEAN   0.193109\n                     SOC_5_NAME   0.060691\n             SPECIALIZED_SKILLS   0.060691\n        SPECIALIZED_SKILLS_NAME   0.060691\n                 CERTIFICATIONS   0.060691\nLOT_SPECIALIZED_OCCUPATION_NAME   0.060691\n            CERTIFICATIONS_NAME   0.060691\n     LOT_SPECIALIZED_OCCUPATION   0.060691\n             COMMON_SKILLS_NAME   0.060691\n                    SKILLS_NAME   0.060691\n                SOFTWARE_SKILLS   0.060691\n           SOFTWARE_SKILLS_NAME   0.060691\n                  COMMON_SKILLS   0.060691\n              NAICS_2022_6_NAME   0.060691\n                         SKILLS   0.060691\n                          TITLE   0.060691\n                     STATE_NAME   0.060691\n                      CITY_NAME   0.060691\n               REMOTE_TYPE_NAME   0.060691\n                    REMOTE_TYPE   0.060691\n                  IS_INTERNSHIP   0.060691\n             MIN_EDULEVELS_NAME   0.060691\n            COMPANY_IS_STAFFING   0.060691\n                   COMPANY_NAME   0.060691\n                        COMPANY   0.060691\n                           BODY   0.060691\n                         POSTED   0.030346\n```\n:::\n:::\n\n\n## Remove Duplicates \nTo eliminate true duplicates from the dataset, job listings that have identical values in all the fields listed below were removed to prevent distortion and ensure statistical integrity: \n\n  - Job Title\n  - Company Name\n  - Location\n  - Posting Date\n  - Skill requirements\n  - Employment type\n\n<!-- ```{python}\n# ADDING IN MORE LIBRARIES\n# -------------------------------------------------------------------\n# Global Plotly Styling — applies to all visualizations in this report\n# -------------------------------------------------------------------\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# pio.templates.default = \"plotly_white\"\n# pio.templates[\"plotly_white\"].layout.font.family = \"Arial\"\n# pio.templates[\"plotly_white\"].layout.font.size = 14\n# pio.templates[\"plotly_white\"].layout.title.font.size = 18\n\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\", color=\"#000000\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2,\n        xaxis=dict(\n            showgrid=False,\n            gridcolor=\"#c0c0c0\",  # same as Matplotlib grid\n            zeroline=False,\n            linecolor=\"black\",\n            mirror=True,\n            ticks=\"outside\",\n            title_font=dict(size=13, family=\"Arial\"),\n            tickfont=dict(size=11, family=\"Arial\")\n        ),\n        yaxis=dict(\n            showgrid=True,\n            gridcolor=\"#c0c0c0\",\n            zeroline=False,\n            linecolor=\"black\",\n            mirror=True,\n            ticks=\"outside\",\n            title_font=dict(size=13, family=\"Arial\"),\n            tickfont=dict(size=11, family=\"Arial\")\n        )\n    )\n)\n\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-v0_8-whitegrid\") \nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"axes.edgecolor\": \"black\",\n    \"axes.linewidth\": 0.8,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150,\n    \"axes.grid\": True,\n    \"grid.color\": \"#c0c0c0\"\n})\nsns.set_palette(\"Set2\")\n``` -->\n\n::: {#5bf50273 .cell execution_count=3}\n``` {.python .cell-code}\n# Drop columns with >50% missing values\ncols_to_drop_missing = [\n    \"MAX_YEARS_EXPERIENCE\",\n    \"MAX_EDULEVELS_NAME\",\n    \"SALARY_FROM\",\n    \"SALARY_TO\",\n    \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_DURATION\",\n    \"MODELED_EXPIRED\",\n    \"EXPIRED\"\n]\ndf.drop(columns=cols_to_drop_missing, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\nfill_col_unk = [\n    # Company info\n    \"COMPANY_NAME\", \"COMPANY_IS_STAFFING\",\n    \n    # Job titles\n    \"TITLE\", \"TITLE_CLEAN\",\n    \n    # Occupation/industry (kept name fields)\n    \"SOC_5_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \"NAICS_2022_6_NAME\",\n    \n    # Remote type\n    \"REMOTE_TYPE_NAME\",\n    \n    # Education level (names, not codes)\n    \"MIN_EDULEVELS_NAME\", \n    \n    # Location info\n    \"STATE_NAME\", \"CITY_NAME\", \"MSA_NAME\",\n\n    # Skills/certifications (optional — only if you plan to analyze skills)\n    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \"COMMON_SKILLS_NAME\", \"CERTIFICATIONS_NAME\"\n]\n\n# Loop through and fill missing values\nfor col in fill_col_unk:\n    df[col] = df[col].fillna(\"Unknown\")\n\n# Create a cleaned version for SALARY with median imputation\ndf[\"SALARY_CLEANED\"] = df[\"SALARY\"].copy()\nmedian_salary = df[\"SALARY\"].median()\ndf[\"SALARY_CLEANED\"] = df[\"SALARY_CLEANED\"].fillna(median_salary)\n\n# Remove duplicate\ndf=df.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"POSTED\", \"REMOTE_TYPE_NAME\", \"SKILLS_NAME\"], keep=\"first\")\n\n# Preview new df\ndf.shape\n\nimport os\n# Ensure the data directory exists\nos.makedirs(\"data\", exist_ok=True)\n\n# Export cleaned dataset for reuse in other analyses\noutput_path = \"data/cleaned_job_postings.csv\"\ndf.to_csv(output_path, index=False)\n\n#print(f\"Cleaned dataset saved successfully: {output_path}\")\n#qprint(f\"Total rows: {len(df):,}, columns: {len(df.columns)}\")\n```\n:::\n\n\n",
    "supporting": [
      "data_cleaning_files"
    ],
    "filters": [],
    "includes": {}
  }
}