{
  "hash": "8f78d20a507f943fd0b212159fa12186",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Skill Gap Analysis\n---\n\n\n\n## Internal Skills Assessment\nTo evaluate the current technical skill set for each team member, each team member performed a self-rating for a set of core competencies for data analytics and data science, which included Python, SQL, Power BI, Tableau, Excel, Machine Learning, Natural Language Processing (NLP), Cloud Computing, and AWS. The assessment was made on a five-point scale, and the rating values were then compiled into a dataframe for further analysis. We then leveraged a Seaborn heatmap to identify where our strengths concentrated and pinpoint where the gaps existed. \n\n\n::: {#c3f98aae .cell execution_count=1}\n``` {.python .cell-code}\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n```\n:::\n\n\n::: {#2ac4de5d .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n```\n:::\n\n\n::: {#3ddcc84a .cell execution_count=3}\n``` {.python .cell-code}\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 2, 3],\n    \"SQL\": [3, 4, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 5, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 1, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\n#plt.show()\nplt.close()\n```\n:::\n\n\n![Figure 8: Team Skill Levels Heatmap](figures/Team_Skills_Heatmap.png)\n\n**Which skills should each member prioritize learning?**\n\n- **Angelina** – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).  \n- **Devin** – Solid in Excel (5) and Tableau (5), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.  \n- **Leo** – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions).  \n\n## External Skills Assessment using Natural Language Processing techniques\nTo identify the most in-demand skills in the analytics job market, we analyzed the job descriptions from the Lightcast dataset by leveraging Natural Language Processing (NLP) techniques to process the BODY column which contained detailed job descriptions. Below is a summary of the data preparation and extraction process:\n\n- **Text normalization:** To ensure consistency, job descriptions were converted to lowercase, Unicode-normalized format, and whitespaces were stripped. \n- **Tokenization:** Numbers and punctuation were filtered out to ensure words can be captured and extracted properly.\n- **Stopword removal:** To further filter and concentrate only on meaningful technical words, common English stopwords were removed using Scikit-learn's built-in stopword list. \n- **Keyword filtering:** A predefined list of relevant analytics skills (e.g., Python, SQL, AWS, Tableau, Excel, Pandas, Spark, Machine Learning, NLP, Cloud Computing) was used to identify and count occurrences within the job descriptions.\n- **Frequency analysis:** Using Python's Counter() function, we tallied the frequency of each skill keyword and visualized in a column chart to understand their significance in the job market. \n\n::: {#fb3eaffc .cell execution_count=4}\n``` {.python .cell-code}\n## Extract most in-demand skills from JD (optimized)\nimport re\nimport os\nimport unicodedata\nfrom collections import Counter\n\n# Import stopwords (Angelina note: switched from NLTK to sklearn's built-in stopwords,\n# which avoids downloads and runs faster)\ntry:\n    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n    stop_words = set(ENGLISH_STOP_WORDS)\nexcept Exception:\n    # Minimal fallback if sklearn is missing\n    stop_words = {\n        \"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\",\n        \"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"were\",\"will\",\"with\"\n    }\n\n# Helper function to normalize text\n# (Angelina note: ensures Unicode normalized, casefolded, and whitespace trimmed)\ndef nfc_casefold_trim(s: str) -> str:\n    s = unicodedata.normalize(\"NFC\", str(s)).casefold()\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n# Compile regex once (Angelina note: faster than re-compiling each loop)\nword_re = re.compile(r\"[a-z]+\")\n\n# Pull description from job postings and count words (streaming, no giant string build)\nprint(\"Scanning job descriptions and counting tokens (streaming)...\")\nwords_count = Counter()\n\nfor txt in df[\"BODY\"].dropna().astype(str):\n    t = nfc_casefold_trim(txt)\n    words_count.update(w for w in word_re.findall(t) if w not in stop_words)\n\n# Define a list of skills\nskills_list = {\n    \"python\", \"sql\", \"aws\", \"docker\", \"tableau\", \"excel\",\n    \"pandas\", \"numpy\", \"spark\", \"machine\", \"learning\",\n    \"nlp\", \"cloud\", \"computing\", \"power\"\n}\n\n# Extract only the predefined skills that appear in job postings\nskills_filtered = {s: words_count[s] for s in skills_list if words_count.get(s, 0) > 0}\n\n# Print results, sorted by most frequent\nprint(\"Top data analytics skills from job descriptions\")\nfor skill, count in sorted(skills_filtered.items(), key=lambda kv: (-kv[1], kv[0])):\n    print(f\"{skill}:{count}\")\n\n\n    # --------- added simple bar chart ---------\nif skills_filtered:\n    os.makedirs(\"figures\", exist_ok=True)  \n    items = sorted(skills_filtered.items(), key=lambda kv: kv[1], reverse=True)\n    labels = [k for k, _ in items]\n    values = [v for _, v in items]\n\n    plt.figure(figsize=(8, 4.5))\n    plt.bar(labels, values)\n    plt.title(\"Most In-Demand Skills from Job Descriptions\")\n    plt.xlabel(\"Skill\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    out_path = \"figures/jd_top_skills.png\"\n    plt.savefig(out_path, dpi=150)\n    #plt.show()\n    plt.close()\n    #print(f\"[Saved chart] {out_path}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScanning job descriptions and counting tokens (streaming)...\nTop data analytics skills from job descriptions\ncloud:42787\nsql:35871\npower:21894\nexcel:19874\nlearning:16361\ntableau:14609\npython:13402\naws:8510\nmachine:5592\ncomputing:2588\nspark:1496\ndocker:613\npandas:378\nnlp:293\nnumpy:187\n```\n:::\n:::\n\n\n![Figure 9: Most In-demand skills from Job Descriptions](figures/jd_top_skills.png)\n\n## Propose an Improvement Plan\nAccording to our analysis, job postings show high demand for Cloud, SQL, Python, ML, and AWS. Our team is strong in visualization (Excel, Power BI, Tableau) but weaker in Cloud, ML, and NLP. This plan aligns our learning with market needs, provides specific resources, and ensures collaboration strategies so the whole team can close the gap together.\n\n**What courses or resources can help?**\n\n- **Cloud & AWS** – free cloud provider tutorials, AWS Educate, and cloud labs for hands-on practice.  \n- **Machine Learning & NLP** – scikit-learn tutorials, Kaggle competitions, and university modules on ML/NLP.  \n- **Python & SQL** – interactive platforms (Jupyter notebooks, LeetCode SQL), and official documentation.  \n- **Docker & Spark** – short online workshops, Spark quickstarts, and Docker “getting started” labs.  \n\n\n**How can the team collaborate to bridge skill gaps?**\n\n- **Role rotation:** assign rotating leads (“cloud lead,” “ML/NLP lead,” “Python/SQL lead”) for mini-projects so each teammate practices outside their strengths.  \n- **Lightning talks:** weekly 15-minute sessions where one teammate teaches a concept or tool they just learned.  \n- **Pair programming:** match stronger members (for example, Angelina for visualization) with weaker ones (for example, Devin on Python) to share knowledge in real time.  \n- **Shared resources:** maintain a team wiki with reusable queries, cloud setup notes, and code snippets.  \n\n",
    "supporting": [
      "skill_gap_analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}