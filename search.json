[
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "To evaluate the current technical skill set for each team member, each team member performed a self-rating for a set of core competencies for data analytics and data science, which included Python, SQL, Power BI, Tableau, Excel, Machine Learning, Natural Language Processing (NLP), Cloud Computing, and AWS. The assessment was made on a five-point scale, and the rating values were then compiled into a dataframe for further analysis. We then leveraged a Seaborn heatmap to identify where our strengths concentrated and pinpoint where the gaps existed.\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 2, 3],\n    \"SQL\": [3, 4, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 5, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 1, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 7: Team Skill Levels Heatmap\n\n\nWhich skills should each member prioritize learning?\n\nAngelina – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).\n\nDevin – Solid in Excel (5) and Tableau (5), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.\n\nLeo – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions)."
  },
  {
    "objectID": "skill_gap_analysis.html#internal-skills-assessment",
    "href": "skill_gap_analysis.html#internal-skills-assessment",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "To evaluate the current technical skill set for each team member, each team member performed a self-rating for a set of core competencies for data analytics and data science, which included Python, SQL, Power BI, Tableau, Excel, Machine Learning, Natural Language Processing (NLP), Cloud Computing, and AWS. The assessment was made on a five-point scale, and the rating values were then compiled into a dataframe for further analysis. We then leveraged a Seaborn heatmap to identify where our strengths concentrated and pinpoint where the gaps existed.\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\n# Create list of relevant analytics skills and rate each member from 1-5\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Angelina\", \"Devin\", \"Leo\"],\n    \"Python\": [3, 2, 3],\n    \"SQL\": [3, 4, 3],\n    \"Power BI\": [5, 4, 4],\n    \"Tableau\": [4, 5, 2],\n    \"Excel\": [5, 5, 4],\n    \"Machine Learning\": [2, 1, 1],\n    \"NLP\": [2, 1, 1],\n    \"Cloud Computing\": [1, 1, 1],\n    \"AWS\": [1, 1, 1]\n}\n\n# Convert to dataframe \ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n# Plot df as a heatmap to visualize skill distribution\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 7: Team Skill Levels Heatmap\n\n\nWhich skills should each member prioritize learning?\n\nAngelina – Strong in visualization tools (Power BI 5, Tableau 4, Excel 5). Next priorities: Cloud, AWS, Machine Learning, and NLP. These are high-demand in job postings (Cloud: 64k+, AWS: 10k+, ML/NLP combined: 23k+).\n\nDevin – Solid in Excel (5) and Tableau (5), but weakest in Python, ML, NLP, and AWS. Needs to raise Cloud as well.\n\nLeo – Stronger in Power BI (4) and Excel (4), but very low in ML, NLP, Cloud, and AWS. Should also build up Python and SQL to meet market demand (Python: 17k+, SQL: 43k+ mentions)."
  },
  {
    "objectID": "skill_gap_analysis.html#external-skills-assessment-using-natural-language-processing-techniques",
    "href": "skill_gap_analysis.html#external-skills-assessment-using-natural-language-processing-techniques",
    "title": "Skill Gap Analysis",
    "section": "2 External Skills Assessment using Natural Language Processing techniques",
    "text": "2 External Skills Assessment using Natural Language Processing techniques\nTo identify the most in-demand skills in the analytics job market, we analyzed the job descriptions from the Lightcast dataset by leveraging Natural Language Processing (NLP) techniques to process the BODY column which contained detailed job descriptions. Below is a summary of the data preparation and extraction process:\n\nText normalization: To ensure consistency, job descriptions were converted to lowercase, Unicode-normalized format, and whitespaces were stripped.\nTokenization: Numbers and punctuation were filtered out to ensure words can be captured and extracted properly.\nStopword removal: To further filter and concentrate only on meaningful technical words, common English stopwords were removed using Scikit-learn’s built-in stopword list.\nKeyword filtering: A predefined list of relevant analytics skills (e.g., Python, SQL, AWS, Tableau, Excel, Pandas, Spark, Machine Learning, NLP, Cloud Computing) was used to identify and count occurrences within the job descriptions.\nFrequency analysis: Using Python’s Counter() function, we tallied the frequency of each skill keyword and visualized in a column chart to understand their significance in the job market.\n\n\n\nCode\n## Extract most in-demand skills from JD (optimized)\nimport re\nimport os\nimport unicodedata\nfrom collections import Counter\n\n# Import stopwords (Angelina note: switched from NLTK to sklearn's built-in stopwords,\n# which avoids downloads and runs faster)\ntry:\n    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n    stop_words = set(ENGLISH_STOP_WORDS)\nexcept Exception:\n    # Minimal fallback if sklearn is missing\n    stop_words = {\n        \"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\",\n        \"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"were\",\"will\",\"with\"\n    }\n\n# Helper function to normalize text\n# (Angelina note: ensures Unicode normalized, casefolded, and whitespace trimmed)\ndef nfc_casefold_trim(s: str) -&gt; str:\n    s = unicodedata.normalize(\"NFC\", str(s)).casefold()\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n# Compile regex once (Angelina note: faster than re-compiling each loop)\nword_re = re.compile(r\"[a-z]+\")\n\n# Pull description from job postings and count words (streaming, no giant string build)\nprint(\"Scanning job descriptions and counting tokens (streaming)...\")\nwords_count = Counter()\n\nfor txt in df[\"BODY\"].dropna().astype(str):\n    t = nfc_casefold_trim(txt)\n    words_count.update(w for w in word_re.findall(t) if w not in stop_words)\n\n# Define a list of skills\nskills_list = {\n    \"python\", \"sql\", \"aws\", \"docker\", \"tableau\", \"excel\",\n    \"pandas\", \"numpy\", \"spark\", \"machine\", \"learning\",\n    \"nlp\", \"cloud\", \"computing\", \"power\"\n}\n\n# Extract only the predefined skills that appear in job postings\nskills_filtered = {s: words_count[s] for s in skills_list if words_count.get(s, 0) &gt; 0}\n\n# Print results, sorted by most frequent\nprint(\"Top data analytics skills from job descriptions\")\nfor skill, count in sorted(skills_filtered.items(), key=lambda kv: (-kv[1], kv[0])):\n    print(f\"{skill}:{count}\")\n\n\n    # --------- added simple bar chart ---------\nif skills_filtered:\n    os.makedirs(\"figures\", exist_ok=True)  \n    items = sorted(skills_filtered.items(), key=lambda kv: kv[1], reverse=True)\n    labels = [k for k, _ in items]\n    values = [v for _, v in items]\n\n    plt.figure(figsize=(8, 4.5))\n    plt.bar(labels, values)\n    plt.title(\"Most In-Demand Skills from Job Descriptions\")\n    plt.xlabel(\"Skill\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    out_path = \"figures/jd_top_skills.png\"\n    plt.savefig(out_path, dpi=150)\n    #plt.show()\n    plt.close()\n    #print(f\"[Saved chart] {out_path}\")\n\n\nScanning job descriptions and counting tokens (streaming)...\nTop data analytics skills from job descriptions\ncloud:42787\nsql:35871\npower:21894\nexcel:19874\nlearning:16361\ntableau:14609\npython:13402\naws:8510\nmachine:5592\ncomputing:2588\nspark:1496\ndocker:613\npandas:378\nnlp:293\nnumpy:187\n\n\n\n\n\nFigure 8: Most In-demand skills from Job Descriptions"
  },
  {
    "objectID": "skill_gap_analysis.html#propose-an-improvement-plan",
    "href": "skill_gap_analysis.html#propose-an-improvement-plan",
    "title": "Skill Gap Analysis",
    "section": "3 Propose an Improvement Plan",
    "text": "3 Propose an Improvement Plan\nAccording to our analysis, job postings show high demand for Cloud, SQL, Python, ML, and AWS. Our team is strong in visualization (Excel, Power BI, Tableau) but weaker in Cloud, ML, and NLP. This plan aligns our learning with market needs, provides specific resources, and ensures collaboration strategies so the whole team can close the gap together.\nWhat courses or resources can help?\n\nCloud & AWS – free cloud provider tutorials, AWS Educate, and cloud labs for hands-on practice.\n\nMachine Learning & NLP – scikit-learn tutorials, Kaggle competitions, and university modules on ML/NLP.\n\nPython & SQL – interactive platforms (Jupyter notebooks, LeetCode SQL), and official documentation.\n\nDocker & Spark – short online workshops, Spark quickstarts, and Docker “getting started” labs.\n\nHow can the team collaborate to bridge skill gaps?\n\nRole rotation: assign rotating leads (“cloud lead,” “ML/NLP lead,” “Python/SQL lead”) for mini-projects so each teammate practices outside their strengths.\n\nLightning talks: weekly 15-minute sessions where one teammate teaches a concept or tool they just learned.\n\nPair programming: match stronger members (for example, Angelina for visualization) with weaker ones (for example, Devin on Python) to share knowledge in real time.\n\nShared resources: maintain a team wiki with reusable queries, cloud setup notes, and code snippets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Research 2024",
    "section": "",
    "text": "1 Welcome\nThis project explores how Artificial Intelligence (AI), remote work, work experience and other emerging labor-market factors are transforming compensation structures across the U.S. job market. Our team analyzed over 70,000 job postings from the Lightcast database to uncover patterns in:\n\nRemote vs. in-office work arrangements\n\nSkill demand trends and emerging gaps\n\nSalary distribution across AI vs. non-AI roles\n\nThe results provide insight into how job seekers can realign their skillset to achieve higher compensation in response to changes in the labor market.\n\n\n\n2 Explore the Report\nUse the navigation bar above to view each section of the analysis, including data cleaning, exploratory analytics, skill gap analysis, and final insights."
  },
  {
    "objectID": "final_analytics.html",
    "href": "final_analytics.html",
    "title": "Regression, Classification, and Topic Insights",
    "section": "",
    "text": "We performed KMeans clustering on job postings using core features (salary, minimum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n\nWe used KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n\n\nFigure 9: KMeans Clusters by Salary and Min Years Experience\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\n#display(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\n#print(\"\\nMost common label per cluster:\")\n#display(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\n#display(ct_share.head(50))\n\n\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) &gt; 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nout_path = \"figures/KMeans_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n\n\nKMeans labels column ready: True\nUnique clusters: 5\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n\n\n\n\n\nClustering on salary and minimum experience produced three clear market segments:\n\nEntry-level / lower salary\nMid-career / moderate salary\nSenior specialist / high salary\n\nThis segmentation aligns with career ladders and confirms the broad, positive relationship between experience and compensation.\nBusiness relevance\n\nJob seekers: Use cluster patterns to target industries/roles occupying higher-pay segments and to plan upskilling.\nEmployers: Map openings to cluster ranges to calibrate pay bands against the external market and reduce attrition risk.\n\n\n\n\nCode\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)"
  },
  {
    "objectID": "final_analytics.html#kmeans-clustering-analysis",
    "href": "final_analytics.html#kmeans-clustering-analysis",
    "title": "Regression, Classification, and Topic Insights",
    "section": "",
    "text": "We performed KMeans clustering on job postings using core features (salary, minimum years of experience). This analysis seeks to segment jobs into groups with similar compensation and experience profiles, and to interpret these clusters using industry categories (NAICS).\n\n\nWe used KMeans clustering to segment jobs into five groups, using standardized salary and experience as inputs. Each posting is assigned to a cluster.\n\n\n\nFigure 9: KMeans Clusters by Salary and Min Years Experience\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Select features for clustering\ncluster_features = ['SALARY', 'MIN_YEARS_EXPERIENCE']\nX = df[cluster_features].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans model\nn_clusters = 5  # Assignment recommends 5, we must justify if we change\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Add cluster labels to DataFrame\ndf.loc[X.index, 'KMEANS_CLUSTER'] = clusters\n\n\n# === Create a label column from the trained KMeans model ===\ntry:\n    df[\"kmeans_labels\"] = pd.NA\n    df.loc[X.index, \"kmeans_labels\"] = kmeans.labels_\nexcept AttributeError:\n    \n    if \"kmeans_labels\" not in df.columns:\n        df[\"kmeans_labels\"] = kmeans.fit_predict(X_scaled)\n\nprint(\"KMeans labels column ready:\", \"kmeans_labels\" in df.columns)\nprint(\"Unique clusters:\", df[\"kmeans_labels\"].nunique())\n\n\n# === KMeans Cluster Reference Label Analysis ===\ncluster_col = \"kmeans_labels\"        \nlabel_col   = \"NAICS_2022_6_NAME\"    # required 'reference label' for interpretation\n\ncrosstab = (\n    df.groupby([cluster_col, label_col])\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values([cluster_col, \"count\"], ascending=[True, False])\n)\n#display(crosstab.head(50))\n\n# Most common label per cluster\ntop_per_cluster = crosstab.loc[crosstab.groupby(cluster_col)[\"count\"].idxmax()].reset_index(drop=True)\n#print(\"\\nMost common label per cluster:\")\n#display(top_per_cluster)\n\n# Percent share per cluster\nct_share = (\n    crosstab\n    .join(crosstab.groupby(cluster_col)[\"count\"].transform(\"sum\").rename(\"cluster_total\"))\n    .assign(share=lambda d: (d[\"count\"] / d[\"cluster_total\"]).round(3))\n    .sort_values([cluster_col, \"share\"], ascending=[True, False])\n)\n#display(ct_share.head(50))\n\n\n# KMeans Cluster Visualization with Top Job Titles\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# --- SCATTERPLOT OF KMEANS CLUSTERS ---\n\nplot_sample = df.loc[X.index].sample(n=5000, random_state=42) if len(X) &gt; 5000 else df.loc[X.index]\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    data=plot_sample,\n    x='SALARY',\n    y='MIN_YEARS_EXPERIENCE',\n    hue='KMEANS_CLUSTER',\n    palette='tab10'\n)\nplt.title('KMeans Clusters by Salary and Min Years Experience')\nplt.xlabel('Salary')\nplt.ylabel('Minimum Years Experience')\nplt.legend(title='Cluster')\nplt.tight_layout()\nout_path = \"figures/KMeans_Cluster.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n# --- TOP 5 JOB TITLES PER CLUSTER ---\n\nprint(\"Top 5 Job Titles for Each Cluster:\\n\")\nfor cluster in sorted(df['KMEANS_CLUSTER'].dropna().unique()):\n    print(f\"\\nCluster {int(cluster)}:\")\n    print(df[df['KMEANS_CLUSTER'] == cluster]['TITLE_CLEAN'].value_counts().head(5))\n\n\nKMeans labels column ready: True\nUnique clusters: 5\nTop 5 Job Titles for Each Cluster:\n\n\nCluster 0:\nTITLE_CLEAN\ndata analyst                     393\nsenior data analyst               94\nbusiness intelligence analyst     88\nsr data analyst                   37\ndata analyst iii                  26\nName: count, dtype: int64\n\nCluster 1:\nTITLE_CLEAN\ndata analyst            81\nenterprise architect    54\nsenior data analyst     23\nsolution architect      18\ndata modeler            17\nName: count, dtype: int64\n\nCluster 2:\nTITLE_CLEAN\ndata analyst                     681\nbusiness intelligence analyst    106\ndata analyst ii                   42\nsenior data analyst               35\nresearch data analyst             26\nName: count, dtype: int64\n\nCluster 3:\nTITLE_CLEAN\ndata analyst                                              68\nenterprise architect                                      59\nsenior data analyst                                       44\ndata engineer analytics                                   30\noracle cloud supply chain management senior consultant    19\nName: count, dtype: int64\n\nCluster 4:\nTITLE_CLEAN\nenterprise architect                            56\nsolution architect                              18\noracle cloud supply chain management manager    14\nprincipal enterprise architect                  14\nenterprise platform architect                   12\nName: count, dtype: int64\n\n\n\n\n\nClustering on salary and minimum experience produced three clear market segments:\n\nEntry-level / lower salary\nMid-career / moderate salary\nSenior specialist / high salary\n\nThis segmentation aligns with career ladders and confirms the broad, positive relationship between experience and compensation.\nBusiness relevance\n\nJob seekers: Use cluster patterns to target industries/roles occupying higher-pay segments and to plan upskilling.\nEmployers: Map openings to cluster ranges to calibrate pay bands against the external market and reduce attrition risk.\n\n\n\n\nCode\n# Cross-tab clusters by industry to interpret groupings\ncrosstab = pd.crosstab(\n    df.loc[X.index, 'KMEANS_CLUSTER'],\n    df.loc[X.index, 'NAICS_2022_6_NAME']\n)"
  },
  {
    "objectID": "final_analytics.html#regression-predicting-salary",
    "href": "final_analytics.html#regression-predicting-salary",
    "title": "Regression, Classification, and Topic Insights",
    "section": "2 Regression – Predicting Salary",
    "text": "2 Regression – Predicting Salary\nWe chose Random Forest over Linear Regression because the relationship between experience and salary is non-linear, and the model captures complex interactions more effectively.\nThe goal of this model is to predict job posting salary using experience and employment type features. The model uses an 80/20 train–test split and evaluates performance using RMSE and R² metrics.\n\n\nCode\n# Feature Selection and Data Preparation\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Use minimum years of experience, employment type (or closest available), and employment type as features\n\nfeatures = ['MIN_YEARS_EXPERIENCE', 'REMOTE_TYPE_NAME']\n\n# One-hot encode employment type\ndf_encoded = pd.get_dummies(df, columns=['REMOTE_TYPE_NAME'], drop_first=True)\n\nX = df_encoded[[col for col in df_encoded.columns if col in features or col.startswith('REMOTE_TYPE_NAME_')]]\ny = df_encoded['SALARY']\n\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train/test split (70/30)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Drop rows where y (salary) is missing\nmask = ~y_train.isna()\nX_train_filtered = X_train.loc[mask]\ny_train_filtered = y_train.loc[mask]\n\nmask_test = ~y_test.isna()\nX_test_filtered = X_test.loc[mask_test]\ny_test_filtered = y_test.loc[mask_test]\n\n# Train a random forest regression model to predict salary\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_filtered, y_train_filtered)\n\n# Predict salaries for the test set\ny_pred = rf.predict(X_test_filtered)\n\n# Calculate RMSE (Root Mean Squared Error) and R² (coefficient of determination)\nmse = mean_squared_error(y_test_filtered, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_filtered, y_pred)\n\nprint(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\nprint(f'R² Score: {r2:.3f}')\n\n\nRoot Mean Squared Error (RMSE): 34744.38\nR² Score: 0.336\n\n\n\n2.1 Regression Results and Interpretation\nWe modeled salary using Minimum Years of Experience (MIN_YEARS_EXPERIENCE) and Remote Type (REMOTE_TYPE_NAME). We excluded MAX_YEARS_EXPERIENCE (&gt;85% missing) to protect statistical integrity; this reduced sample size but improved reliability.\nModel performance\n\nRMSE: 34,744.38\nR²: 0.336\n\nWhat this means An R² of 0.336 indicates experience and remote status explain a meaningful, but incomplete share of salary variation. That’s consistent with cross-industry labor data, where compensation is also driven by occupation, industry, location, and scarce technical capabilities.\nBusiness relevance\n\nFor job seekers: Experience contributes to higher earnings, but targeted specialization (e.g., analytics, cloud, data engineering) and industry selection are decisive for larger pay jumps.\nFor employers: Pricing talent solely by tenure can misalign offers in high-skill roles. Clear remote/on-site definitions in postings improve candidate signal and reduce noise in market benchmarking.\n\n\n\nCode\n# Visual: Predicted vs. Actual Salary\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_actual, y_pred_series, alpha=0.5)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Predicted vs. Actual Salary (Random Forest Regression)\")\nlo, hi = float(y_actual.min()), float(y_actual.max())\nplt.plot([lo, hi], [lo, hi], 'r--', label=\"Perfect Prediction\")\nplt.legend()\nplt.tight_layout()\nout_path = \"figures/Predicted_vs_Actual_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 10: Predicted vs. Actual Salary (Random Forest Regression)\n\n\n\n\nCode\n# Feature importance: shows which variables most influence salary prediction\nimportances = rf.feature_importances_\nfeature_names = X.columns\n\nfeat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 features influencing salary prediction:\")\nfor name, imp in feat_imp[:5]:\n    print(f\"{name}: {imp:.3f}\")\n\n\nTop 5 features influencing salary prediction:\nMIN_YEARS_EXPERIENCE: 0.953\nREMOTE_TYPE_NAME_Not Remote: 0.017\nREMOTE_TYPE_NAME_[None]: 0.015\nREMOTE_TYPE_NAME_Remote: 0.014\nREMOTE_TYPE_NAME_Unknown: 0.000\n\n\n\n\nCode\n# Visual: Feature Importance Bar Chart\nimport pandas as pd\n\nfeat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\nfeat_imp_df = feat_imp_df.sort_values('importance', ascending=False).head(5)\n\nplt.figure(figsize=(8, 5))\nplt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Top 5 Feature Importances (Random Forest Regression)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nout_path = \"figures/Top_5_Feature_Importances_RF_Regression.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 11: Top 5 Feature Importances (Random Forest Regression)\n\n\n\n\nCode\n# Calculate absolute prediction error for each job\nimport pandas as pd\ny_actual = y_test_filtered if 'y_test_filtered' in globals() else y_test\ny_pred_series = pd.Series(y_pred, index=y_actual.index[:len(y_pred)])\nerrors = (y_actual - y_pred_series).abs()\noutlier_indices = errors.nlargest(5).index\n\n# Show the original job posting rows for these outliers\ncols = ['SALARY', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE_NAME', 'REMOTE_TYPE_NAME']\ncols = [c for c in cols if c in df.columns]\nprint(df.loc[outlier_indices, cols])\n\n\n         SALARY  MIN_YEARS_EXPERIENCE REMOTE_TYPE_NAME\n10189  312500.0                   1.0           [None]\n19209  312000.0                   1.0           [None]\n31552  328600.0                   5.0           [None]\n2258   305000.0                   3.0           [None]\n29554  338750.0                   7.0           [None]\n\n\n\n\nCode\n# Visual: Highlight Outliers in Predicted vs. Actual Salary\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nemp_col = next((c for c in [\"EMPLOYMENT_TYPE_NAME\",\"REMOTE_TYPE_NAME\",\"STATE_NAME\",\"MSA_NAME\"] if c in df.columns), None)\n\nsubset_cols = [\"SALARY\",\"MIN_YEARS_EXPERIENCE\"] + ([emp_col] if emp_col else [])\nsubset = df.loc[outlier_indices, subset_cols].copy()\n\ndef _fmt_sal(x): \n    return f\"${int(x):,}\" if pd.notna(x) else \"N/A\"\n\ndef _fmt_exp(x): \n    return f\"{int(x)}yr min\" if pd.notna(x) else \"n/a\"\n\ndef _fmt_emp(x): \n    return str(x) if pd.notna(x) else \"n/a\"\n\nif emp_col:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\\n{_fmt_emp(e)}\"\n        for s, m, e in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"], subset[emp_col])\n    ]\nelse:\n    labels = [\n        f\"{_fmt_sal(s)}\\n{_fmt_exp(m)}\"\n        for s, m in zip(subset[\"SALARY\"], subset[\"MIN_YEARS_EXPERIENCE\"])\n    ]\n\nplt.figure(figsize=(10, 5))\nplt.bar(labels, subset[\"SALARY\"].fillna(0))\nplt.ylabel(\"Actual Salary\")\nplt.title(\"Top 5 Outlier Job Salaries\")\nplt.tight_layout()\nout_path = \"figures/Top_5_Outlier_Job_Salaries.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n2.2 Outlier Jobs and Market Signals\nThe largest residuals include postings above $300,000 with only 1–5 years minimum experience, often missing a remote/on-site label. These cases point to specialized, high-impact roles where compensation is decoupled from tenure (e.g., advanced analytics, cloud/platform, niche leadership).\nA small number of postings show very high pay at low experience with unspecified remote type. These likely reflect specialized roles or incomplete records. In practice, validate or isolate these cases before setting salary bands or training production models.\n\n\n\nFigure 12: Top 5 Outlier Job Salaries\n\n\nBusiness relevance\n\nEmployers: Standardize and QA high-pay, low-tenure postings, unclear fields and atypical mixes should be reviewed before publication.\nJob seekers: Outliers highlight skill paths where focused upskilling can command premium pay earlier in a career."
  },
  {
    "objectID": "final_analytics.html#classification-predicting-remote-vs-on-site-job",
    "href": "final_analytics.html#classification-predicting-remote-vs-on-site-job",
    "title": "Regression, Classification, and Topic Insights",
    "section": "3 Classification – Predicting Remote vs On-Site Job",
    "text": "3 Classification – Predicting Remote vs On-Site Job\nWe trained a logistic regression to predict Remote vs On-Site using MIN_YEARS_EXPERIENCE plus an available categorical descriptor (e.g., employment type or state).\n\n\nCode\n# Classification: Predicting Remote vs On-Site Jobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Prepare data\nemp_col = \"EMPLOYMENT_TYPE_NAME\" if \"EMPLOYMENT_TYPE_NAME\" in df.columns else (\"STATE_NAME\" if \"STATE_NAME\" in df.columns else (\"MSA_NAME\" if \"MSA_NAME\" in df.columns else None))\nif emp_col is not None:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", emp_col, \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nelse:\n    clf_df = df[[\"MIN_YEARS_EXPERIENCE\", \"REMOTE_TYPE_NAME\"]].dropna().copy() # Removed MAX_YEARS_EXPERIENCE\nclf_df[\"IS_REMOTE\"] = clf_df[\"REMOTE_TYPE_NAME\"].str.contains(\"Remote\", case=False, na=False).astype(int)\n\nX = clf_df[[\"MIN_YEARS_EXPERIENCE\", emp_col]].copy() if emp_col is not None else clf_df[[\"MIN_YEARS_EXPERIENCE\"]].copy() # Removed MAX_YEARS_EXPERIENCE\ny = clf_df[\"IS_REMOTE\"]\n\n# One-hot encode employment type\nif emp_col is not None:\n    preprocessor = ColumnTransformer([\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [emp_col]),\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\nelse:\n    preprocessor = ColumnTransformer([\n        (\"num\", \"passthrough\", [\"MIN_YEARS_EXPERIENCE\"]) # Removed MAX_YEARS_EXPERIENCE\n    ])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Model\nclf = Pipeline([\n    (\"prep\", preprocessor),\n    (\"model\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n])\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\n# Metrics\nacc = accuracy_score(y_test, pred)\nf1 = f1_score(y_test, pred)\nprint(f\"Accuracy: {acc:.3f}, F1 Score: {f1:.3f}\")\n\n# Confusion matrix\nConfusionMatrixDisplay(confusion_matrix(y_test, pred)).plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix — Remote vs On-Site Classification\")\nout_path = \"figures/Confusion_Matrix_Remote_vs_On-Site.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\nAccuracy: 0.619, F1 Score: 0.328\n\n\n\n\n\nFigure 13: Confusion Matrix — Remote vs On-Site Classification\n\n\n\n3.1 Classification Results and Interpretation\nPerformance:\n\nAccuracy: 0.622\nF1 Score: 0.327\n\nWhat this means: The model distinguishes remote vs. on-site at a moderate level, consistent with the idea that remote status is policy/role-design driven, not primarily a function of required experience.\nBusiness relevance:\n\nEmployers: Remote flexibility can be offered across experience levels without materially distorting supply. If remote status is strategic, emphasize role/industry signals rather than tenure in postings.\nJob seekers: Remote options exist from entry to senior levels, broadening geographic reach and negotiation leverage."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Methodology",
    "section": "",
    "text": "Below are the steps we took to prepare the dataset to ensure accuracy and statistical integrity in our exploratory data analysis (EDA)."
  },
  {
    "objectID": "data_cleaning.html#remove-unnecessary-columns",
    "href": "data_cleaning.html#remove-unnecessary-columns",
    "title": "Methodology",
    "section": "1 Remove Unnecessary Columns",
    "text": "1 Remove Unnecessary Columns\nFirstly, columns containing redundant and irrelevant information were excluded from the dataset. Since the scope of this analysis is focused on job market trends in 2024, it is best practice to remove any outdated NAICS/SOC fields to prevent confusion and duplication. Similarly, metadata fields or duplicate fields that could introduce ambiguity and do not add any meaningful to downstream analysis are excluded. To summarize, unnecessary columns containing the following information were dropped:\n\nMeta/tracking\nDuplicated location info\nRaw/duplicate title & body\nDuplicated employment info\nEducation code columns\nRedundant NAICS/SOC versions\nLOT/V6 occupation hierarchy\nONET & CIP codes\nSectors"
  },
  {
    "objectID": "data_cleaning.html#handling-of-missing-values",
    "href": "data_cleaning.html#handling-of-missing-values",
    "title": "Methodology",
    "section": "2 Handling of missing values",
    "text": "2 Handling of missing values\nWe addressed missing values based on field type and the amount of data missing per field:\n\n2.1 Numerical Fields\nMissing values in key numerical fields were imputed with the median of each respective fields. Since these numerical fields are key for grouping and visualizing trends, simply removing empty rows could reduce the diversity of the dataset and introduce biases. The rationale for imputing the median rather than the mean is that the latter tends to be influenced by outliers and skewed distributions, which is often exhibited in fields like “SALARY. Based on the downstream EDA, additional filtering may be applied to exclude imputed values altogether to prevent distorition and ensure statistical integrity in the trends observed.\n\n\n2.2 Categorical Fields\nMissing values in key categorical fields were imputed with the placeholder value “Unknown” in order to prevent those rows of data being dropped, leading to unnecessary data loss. By imputing a neutral label like “Unknown”, this strategy ensures that data integrity is retained without introducing false assumptions and biases into the downstream analysis.\n\n\n2.3 Columns containing Majority Missing Data\nWhile the above addressed the rationale for imputing missing values, having to impute the majority of a column’s values can also create noise, which provides insignificant information and analytical value to the downstream analysis. Therefore, columns containing more than 50% missing values were excluded from the dataset.\n\n\n\nFigure 1: Non-null Data\n\n\n\n\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\n\n\n\n\nCode\nimport pandas as pd\n\n# Load lightcast_job_postings.csv \ndf = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\n# Show the first 5 rows \n#print(df.head(5).to_string())\n\n# Drop columns\ncolumns_to_drop = [\n    # Meta/tracking\n    \"ID\", \"LAST_UPDATED_DATE\", \"LAST_UPDATED_TIMESTAMP\", \"DUPLICATES\", \"URL\",\n    \"ACTIVE_URLS\", \"ACTIVE_SOURCES_INFO\", \"SOURCE_TYPES\", \"SOURCES\",\n\n    # Duplicated location info\n    \"LOCATION\", \"CITY\", \"STATE\", \"COUNTY\", \"COUNTY_NAME\",\n    \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_NAME_OUTGOING\", \"MSA_INCOMING\", \"MSA_NAME_INCOMING\",\n\n    # Raw/duplicate title & body\n    \"TITLE_RAW\", \"TITLE_NAME\",\n\n    # Duplicated employment info\n    \"EMPLOYMENT_TYPE\", \"EMPLOYMENT_TYPE_NAME\",\n\n    # Education code columns\n    \"EDUCATION_LEVELS\", \"EDUCATION_LEVELS_NAME\", \"MIN_EDULEVELS\", \"MAX_EDULEVELS\",\n\n    # Redundant NAICS/SOC versions\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \"NAICS4\", \"NAICS4_NAME\",\n    \"NAICS5\", \"NAICS5_NAME\", \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_4\", \"SOC_4_NAME\",\n    \"SOC_5\",  # keep SOC_5_NAME, drop code\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\",\n    \"SOC_2021_4\", \"SOC_2021_4_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n\n    # LOT/V6 occupation hierarchy (keep only 1 specialized name field)\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n\n    # ONET & CIP codes \n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"CIP2\", \"CIP2_NAME\", \"CIP4\", \"CIP4_NAME\", \"CIP6\", \"CIP6_NAME\",\n\n    # Sectors\n    \"LIGHTCAST_SECTORS\", \"LIGHTCAST_SECTORS_NAME\",\n\n    # NAICS 2022 lower-level codes\n    \"NAICS_2022_2\", \"NAICS_2022_2_NAME\", \"NAICS_2022_3\", \"NAICS_2022_3_NAME\",\n    \"NAICS_2022_4\", \"NAICS_2022_4_NAME\", \"NAICS_2022_5\", \"NAICS_2022_5_NAME\",\n    \"NAICS_2022_6\",  # drop code, keep name\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\n#df.info()\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Identify columns that have a significant amount of missing values and sort df by the percentage of missing values\nmissing_percent = df.isnull().mean().sort_values(ascending=False)*100\ndf_sorted = df[missing_percent.index]\n\n# Visualize missing data using missingno bar chart \nplt.figure(figsize=(12, 6))\nmsno.bar(df_sorted)\nplt.title(\"Non-null Data Bar Chart\")\nplt.tight_layout()\nout_path = \"figures/non-null_data.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\nmissing_values_pct = (missing_percent.reset_index().rename(columns={\"index\": \"Column\", 0: \"Missing %\"}))\nprint(missing_values_pct.to_string(index=False))\n\n\n                         Column  Missing %\n           MAX_YEARS_EXPERIENCE  88.372093\n             MAX_EDULEVELS_NAME  77.495931\n                         SALARY  57.505035\n                      SALARY_TO  55.311871\n            ORIGINAL_PAY_PERIOD  55.311871\n                    SALARY_FROM  55.311871\n                       DURATION  37.678281\n           MIN_YEARS_EXPERIENCE  31.926398\n               MODELED_DURATION  26.607631\n                MODELED_EXPIRED  21.264035\n                        EXPIRED  10.819609\n                       MSA_NAME   5.447047\n                    COMPANY_RAW   0.746227\n                    TITLE_CLEAN   0.193109\n                     SOC_5_NAME   0.060691\n             SPECIALIZED_SKILLS   0.060691\n        SPECIALIZED_SKILLS_NAME   0.060691\n                 CERTIFICATIONS   0.060691\nLOT_SPECIALIZED_OCCUPATION_NAME   0.060691\n            CERTIFICATIONS_NAME   0.060691\n     LOT_SPECIALIZED_OCCUPATION   0.060691\n             COMMON_SKILLS_NAME   0.060691\n                    SKILLS_NAME   0.060691\n                SOFTWARE_SKILLS   0.060691\n           SOFTWARE_SKILLS_NAME   0.060691\n                  COMMON_SKILLS   0.060691\n              NAICS_2022_6_NAME   0.060691\n                         SKILLS   0.060691\n                          TITLE   0.060691\n                     STATE_NAME   0.060691\n                      CITY_NAME   0.060691\n               REMOTE_TYPE_NAME   0.060691\n                    REMOTE_TYPE   0.060691\n                  IS_INTERNSHIP   0.060691\n             MIN_EDULEVELS_NAME   0.060691\n            COMPANY_IS_STAFFING   0.060691\n                   COMPANY_NAME   0.060691\n                        COMPANY   0.060691\n                           BODY   0.060691\n                         POSTED   0.030346"
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Methodology",
    "section": "3 Remove Duplicates",
    "text": "3 Remove Duplicates\nTo eliminate true duplicates from the dataset, job listings that have identical values in all the fields listed below were removed to prevent distortion and ensure statistical integrity:\n\nJob Title\nCompany Name\nLocation\nPosting Date\nSkill requirements\nEmployment type\n\n\n\n\nCode\n# Drop columns with &gt;50% missing values\ncols_to_drop_missing = [\n    \"MAX_YEARS_EXPERIENCE\",\n    \"MAX_EDULEVELS_NAME\",\n    \"SALARY_FROM\",\n    \"SALARY_TO\",\n    \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_DURATION\",\n    \"MODELED_EXPIRED\",\n    \"EXPIRED\"\n]\ndf.drop(columns=cols_to_drop_missing, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\nfill_col_unk = [\n    # Company info\n    \"COMPANY_NAME\", \"COMPANY_IS_STAFFING\",\n    \n    # Job titles\n    \"TITLE\", \"TITLE_CLEAN\",\n    \n    # Occupation/industry (kept name fields)\n    \"SOC_5_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \"NAICS_2022_6_NAME\",\n    \n    # Remote type\n    \"REMOTE_TYPE_NAME\",\n    \n    # Education level (names, not codes)\n    \"MIN_EDULEVELS_NAME\", \n    \n    # Location info\n    \"STATE_NAME\", \"CITY_NAME\", \"MSA_NAME\",\n\n    # Skills/certifications (optional — only if you plan to analyze skills)\n    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \"COMMON_SKILLS_NAME\", \"CERTIFICATIONS_NAME\"\n]\n\n# Loop through and fill missing values\nfor col in fill_col_unk:\n    df[col] = df[col].fillna(\"Unknown\")\n\n# Create a cleaned version for SALARY with median imputation\ndf[\"SALARY_CLEANED\"] = df[\"SALARY\"].copy()\nmedian_salary = df[\"SALARY\"].median()\ndf[\"SALARY_CLEANED\"] = df[\"SALARY_CLEANED\"].fillna(median_salary)\n\n# Remove duplicate\ndf=df.drop_duplicates(subset=[\"TITLE_CLEAN\", \"COMPANY_NAME\", \"POSTED\", \"REMOTE_TYPE_NAME\", \"SKILLS_NAME\"], keep=\"first\")\n\n# Preview new df\ndf.shape\n\nimport os\n# Ensure the data directory exists\nos.makedirs(\"data\", exist_ok=True)\n\n# Export cleaned dataset for reuse in other analyses\noutput_path = \"data/cleaned_job_postings.csv\"\ndf.to_csv(output_path, index=False)\n\n#print(f\"Cleaned dataset saved successfully: {output_path}\")\n#qprint(f\"Total rows: {len(df):,}, columns: {len(df.columns)}\")"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "1 Conclusion",
    "section": "",
    "text": "This project, which analyzed the 2024 job market, provided key insights into the factors influencing compensation and future workforce development. Our analysis confirms that the employment landscape is being fundamentally reshaped by technology and the demand for flexible work, creating both challenges and opportunities for job seekers and employers.\n\n\nSkill-Driven Wage Premiums: Our Natural Language Processing (NLP) analysis identified a clear difference between the team’s strengths and current market demand. While the team performs well in visualization tools such as Excel, Power BI, and Tableau, the job market places greater emphasis on Cloud Computing, SQL, Python, and Machine Learning (ML). The frequency of these skills in job postings shows that focused development in these areas supports higher compensation and continued competitiveness in analytics roles.\nExperience as the Primary Salary Driver: The Random Forest Regression model achieved an RMSE of 34,744.38 and an R² of 0.336, identifying Minimum Years of Experience as the most influential feature in predicting salary, with a feature importance of approximately 0.953. This result indicates that, while technical specialization enhances access to well-paid roles, accumulated experience continues to be the main factor influencing salary progression.\nThe Segmented Job Market: The KMeans Clustering model grouped job postings into five clusters, summarized into three main tiers—Entry-Level (lower salary), Mid-Career (moderate salary), and Senior Specialist (higher salary). This pattern aligns with traditional career development paths and suggests that job seekers should aim for higher-tier roles by combining experience with proficiency in high-demand skills such as Cloud Computing, Python, and Machine Learning. Remote Work Status is Not Experience-Dependent The Logistic Regression model achieved an accuracy of 0.622 and an F1 score of 0.327, showing moderate predictive strength when using experience to classify remote versus on-site jobs. This suggests that remote flexibility depends more on company policy or role design than on years of experience. Remote opportunities appear across all experience tiers, providing both entry-level and senior professionals with broader access to roles.\n\n\n\nBased on these findings, the following strategies are recommended for both job seekers (the team) and employers: Continue Upskilling in Cloud and Machine Learning Efforts should focus on further developing Cloud/AWS and Machine Learning/NLP skills. These areas show the greatest opportunity for alignment with market needs and can improve access to higher-tier roles. Standardize High-Value Job Postings Employers should review and standardize postings with high salaries and lower experience requirements. Accurate labeling of REMOTE_TYPE and clear definitions of required technical skills will improve market consistency, strengthen salary benchmarking, and attract well-matched candidates. Use Remote Flexibility to Expand Talent Pools Since remote status does not strongly depend on experience, employers can apply flexible work policies to reach qualified candidates at all career levels. This approach supports broader hiring while maintaining consistency in pay structures."
  },
  {
    "objectID": "conclusion.html#key-findings-and-market-dynamics",
    "href": "conclusion.html#key-findings-and-market-dynamics",
    "title": "1 Conclusion",
    "section": "",
    "text": "Skill-Driven Wage Premiums: Our Natural Language Processing (NLP) analysis identified a clear difference between the team’s strengths and current market demand. While the team performs well in visualization tools such as Excel, Power BI, and Tableau, the job market places greater emphasis on Cloud Computing, SQL, Python, and Machine Learning (ML). The frequency of these skills in job postings shows that focused development in these areas supports higher compensation and continued competitiveness in analytics roles.\nExperience as the Primary Salary Driver: The Random Forest Regression model achieved an RMSE of 34,744.38 and an R² of 0.336, identifying Minimum Years of Experience as the most influential feature in predicting salary, with a feature importance of approximately 0.953. This result indicates that, while technical specialization enhances access to well-paid roles, accumulated experience continues to be the main factor influencing salary progression.\nThe Segmented Job Market: The KMeans Clustering model grouped job postings into five clusters, summarized into three main tiers—Entry-Level (lower salary), Mid-Career (moderate salary), and Senior Specialist (higher salary). This pattern aligns with traditional career development paths and suggests that job seekers should aim for higher-tier roles by combining experience with proficiency in high-demand skills such as Cloud Computing, Python, and Machine Learning. Remote Work Status is Not Experience-Dependent The Logistic Regression model achieved an accuracy of 0.622 and an F1 score of 0.327, showing moderate predictive strength when using experience to classify remote versus on-site jobs. This suggests that remote flexibility depends more on company policy or role design than on years of experience. Remote opportunities appear across all experience tiers, providing both entry-level and senior professionals with broader access to roles."
  },
  {
    "objectID": "conclusion.html#strategic-recommendations",
    "href": "conclusion.html#strategic-recommendations",
    "title": "1 Conclusion",
    "section": "",
    "text": "Based on these findings, the following strategies are recommended for both job seekers (the team) and employers: Continue Upskilling in Cloud and Machine Learning Efforts should focus on further developing Cloud/AWS and Machine Learning/NLP skills. These areas show the greatest opportunity for alignment with market needs and can improve access to higher-tier roles. Standardize High-Value Job Postings Employers should review and standardize postings with high salaries and lower experience requirements. Accurate labeling of REMOTE_TYPE and clear definitions of required technical skills will improve market consistency, strengthen salary benchmarking, and attract well-matched candidates. Use Remote Flexibility to Expand Talent Pools Since remote status does not strongly depend on experience, employers can apply flexible work policies to reach qualified candidates at all career levels. This approach supports broader hiring while maintaining consistency in pay structures."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA Overview",
    "section": "",
    "text": "This section explores job market trends and the restructuring of compensation through a series of visualizations. Each exploratory data analysis (EDA) was chosen to reveal specific patterns in compensation to highlight the impact of AI and remote work.\nCode\n# _setup.qmd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Define a consistent theme\ncustom_theme = go.layout.Template(\n    layout=go.Layout(\n        font=dict(family=\"Arial\", size=12, color=\"#000000\"),\n        title=dict(font=dict(size=16, family=\"Arial\")),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"white\",\n        colorway=px.colors.qualitative.Set2\n    )\n)\npio.templates[\"custom_white\"] = custom_theme\npio.templates.default = \"custom_white\"\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams.update({\n    \"font.family\": \"Arial\",\n    \"font.size\": 7,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 9,\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150\n})\nsns.set_palette(\"Set2\")\nCode\nimport pandas as pd\n\n# Load pre-cleaned dataset\ndf = pd.read_csv(\"data/cleaned_job_postings.csv\")"
  },
  {
    "objectID": "eda.html#salary-by-remote-work-type",
    "href": "eda.html#salary-by-remote-work-type",
    "title": "EDA Overview",
    "section": "1 Salary by Remote Work Type",
    "text": "1 Salary by Remote Work Type\nThe purpose of this EDA is to visually compare not just the average pay, but also the range and consistency of salaries across different work arrangements. This visualization could reveal that remote roles have a wider salary range, indicating that companies are paying a premium for top talent regardless of location.\n\n\nCode\n# 5.1.1 Visual - Compensation\nimport plotly.express as px\nimport pandas as pd\n\nvalues_to_exclude = ['Unknown', '[None]']\ndf_filtered = df[~df['REMOTE_TYPE_NAME'].isin(values_to_exclude)]\n\nfig1 = px.box(\n    df_filtered,\n    x=\"REMOTE_TYPE_NAME\",\n    y=\"SALARY\",\n    title=\"Salary Distribution by Work Arrangement\",\n    labels={\"REMOTE_TYPE_NAME\": \"Work Arrangement\", \"SALARY\": \"Annual Salary ($)\"},\n    width=800,\n    height=600,\n    color=\"REMOTE_TYPE_NAME\",                         \n    color_discrete_sequence=px.colors.qualitative.Set2\n)\n#fig1.show()\nfig1.write_image(\"figures/salary_by_work_arrangement.png\", scale=2)\n\n\n\n\n\nFigure 2: Salary Distribution by Work Arrangement\n\n\nKey Insights: The median salaries across all work arrangements are similar, clustering around $115,000. However, both Remote and Hybrid Remote roles exhibit a much wider salary range and more high-paying outlier positions, with some remote roles exceeding $350,000. This suggests that while typical pay is comparable, remote-friendly positions offer significantly greater potential for top-end compensation."
  },
  {
    "objectID": "eda.html#salary-trends-by-top-industries",
    "href": "eda.html#salary-trends-by-top-industries",
    "title": "EDA Overview",
    "section": "2 Salary Trends by Top Industries",
    "text": "2 Salary Trends by Top Industries\nThe motivation for this EDA is to explore how compensation varies across economic sectors and identify which industries offer the highest earning potential. Using the 2024 job posting data and grouping by industry (NAICS 2022 Level-6 codes), this analysis aims to compare the median salary and salary distribution across highest-paying industries within the job market.\n\n\nCode\n## Query Setup\n# Convert the POSTED date from string to date format\ndf[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\n# Filter for job postings from 2024, specifically looking at Salary and Industry. Exclude unknowns, nulls, and zeros. Exclude 'Unclassified Industry' \ndf_jp_2024 = df[\n  (df[\"POSTED\"].dt.year==2024) & \n  (df[\"SALARY\"] &gt; 0) & \n  (df[\"SALARY\"].notnull()) &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unknown\") &\n  (df[\"NAICS_2022_6_NAME\"]!= \"Unclassified Industry\")\n]\n\n## Further filter to exclude industries that have an insignificant number of job postings\n# count the number of rows per industry  \nindustry_jp_count = df_jp_2024[\"NAICS_2022_6_NAME\"].value_counts()\n\n# summarize the distribution of job counts per industry\nindustry_jp_count.describe()\n\n# Set minimum threshold at 100 job postings to ensure statistical significance\ntop_jp_industries = industry_jp_count[industry_jp_count &gt; 100].index\n\n# Update df to only show top job posting industries\ndf_jp_2024 = df_jp_2024[df_jp_2024[\"NAICS_2022_6_NAME\"].isin(top_jp_industries)]\n\n\n\n\nCode\n## Plot: Analyze Median Salary by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# group by industry name and calculate median salary, sort by descending order\ntop_industry_salary_order = (\n    df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"]\n    .median()\n    .sort_values(ascending=False)\n    .head(12)\n)\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax1=sns.barplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    hue=\"NAICS_2022_6_NAME\",\n    order=industry_order,\n    orient='h',\n    palette=\"Set3\",\n    estimator=np.median,\n    errorbar=None,\n    legend=False,\n    alpha=0.8\n)\n\nfor patch in ax1.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Median Salary by Industry\")\nplt.xlabel(\"Median Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\nout_path = \"figures/median_salary_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 3: Median Salary by Industry\n\n\n\n\nCode\n## Plot: Analyze Salary Distribution by Industry (Seaborn)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# # determine IQRs by industry:\n# q25 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.25)\n# q75 = df_jp_2024.groupby(\"NAICS_2022_6_NAME\")[\"SALARY\"].quantile(0.75)\n# # sort by the middle 50% (Q3 - Q1) and name that as the new sorting order\n# iqr = (q75 - q25).sort_values(ascending=False).head(12)\n\n#iqr_order = iqr.index.to_list()\nindustry_order = top_industry_salary_order.index.tolist()\n\nplt.figure(figsize=(12, 5))\nax2 = sns.boxplot(\n    data=df_jp_2024,\n    y=\"NAICS_2022_6_NAME\",\n    x=\"SALARY\",\n    order=industry_order,\n    palette=\"Set3\",\n    width=0.6,\n    fliersize=2.5,  \n    linewidth=0.8,\n)\n\nfor patch in ax2.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution by Industry\")\nplt.xlabel(\"Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.yticks(ha=\"right\", fontsize=9)\nplt.xticks(fontsize=9)\nplt.tight_layout()\n\n# Save and display the figure\nout_path = \"figures/salary_distribution_by_industry.png\"\nplt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n#plt.show()\nplt.close()\n\n\n\n\n\nFigure 4: Salary Distribution by Industry\n\n\nKey Insights: The analysis of median salary across industries shows that Web Search Portals and All Other Information Services leads with the highest median pay, exceeding $160,000. This is likely due to the increasing demand on specialized digital infrastructure and AI-driven information services. The box plot shows a wide interquartile range (IQR), indicating a significant pay gap between entry-level and more senior/specialized roles in the industry. This trend is also observed in the Computing Infrastructure Providers, Data Processing, Web Hosting, and Related Services category, which is indicative of the range of roles in demand for those industries. Although other technology and information industries such as Software Publishers and Computer System Design Services reported slightly lower median salaries compared to the top-paying sectors, the large amount of high-end outlier values suggests that these industries still offer high-earning potential for more senior/specialized roles with most positions clustering around mid-range salaries.\nNotably, non-technology/information-related sectors such as All Other Miscellaneous Retailers also reported high median salaries, around $160,000. The broad definition of these sectors likely encompasses various types of retail firms that may include roles and platforms beyond the traditional frontline retail and sales roles. Another sector in which compensation data may be inflated by data categorization is Offices of Certified Public Accountants (CPAs), which reported a median salary of around $145,000. Based on the salary distribution highlighted in the box plot, the positively-skewed distribution indicates that while most CPA roles earn around the median, some specialized roles are earning substantially more. This aligns with how CPA firms are typically structured, where a small number of senior partners and high-level executives driving the high-end outliers.\nThese findings show that companies in technology, professional services, and consulting industries offer the highest median salaries and exhibit a larger dispersion in compensation, which is reflective of a more diverse workforce structure that may include entry level, technical/specialist, and senior leadership roles. On the other hand, administrative and support service sectors demonstrate lower median but tighter distributions in salary, demonstrating a more standardized workforce and wage structure in comparison."
  },
  {
    "objectID": "eda.html#ai-vs.-non-ai-salary-comparison",
    "href": "eda.html#ai-vs.-non-ai-salary-comparison",
    "title": "EDA Overview",
    "section": "3 AI vs. Non-AI Salary Comparison",
    "text": "3 AI vs. Non-AI Salary Comparison\nThe motivation for this analysis was to investigate how specialization in AI-related skills influence salary in today’s job market. This analysis differentiates AI vs. non-AI jobs by identifying AI-related keywords such as machine learning, data science, computer vision, and natural language processing in job postings. We then compare the median and distribution of salaries between the two groups to determine whether AI roles command higher compensation than non-AI roles.\n\n\nCode\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a lowercase version of the BODY column for keyword searching\ndf[\"BODY\"] = df[\"BODY\"].astype(str).str.lower()\n\n# identify AI related keywords \nai_keywords = [\n    \"machine learn\",  # matches 'machine learning', 'machine learner'\n    \"data scien\",     # matches 'data scientist', 'data science'\n    \"artificial intel\",  # matches 'artificial intelligence'\n    \"deep learn\",  \n    \"ml engineer\",            \n    \"data engineer\",\n    \"computer vision\", \n    \"natural language\", \n    \"nlp\",\n    \"big data\",\n    \"cloud data\"\n]\n\n# Create a regex pattern that matches any of the keywords, case-insensitive\nai_pattern = re.compile(r\"|\".join([re.escape(k) for k in ai_keywords]), flags=re.IGNORECASE)\n\n# Assign a new column is_ai_job to label job postings with AI or Non-AI based on keyword presence in the BODY text\ndf[\"is_ai_job\"] = df[\"BODY\"].apply(lambda text: \"AI\" if ai_pattern.search(text) else \"Non-AI\")\n\n# Filter out rows with null or zero salary and outliers \ndf_filtered_1 = df[\n    (df[\"SALARY\"].notnull()) &\n    (df[\"SALARY\"] &gt; 0)\n]\nq1 = df_filtered_1[\"SALARY\"].quantile(0.01)\nq99 = df_filtered_1[\"SALARY\"].quantile(0.99)\ndf_filtered_1 = df_filtered_1[(df_filtered_1[\"SALARY\"] &gt;= q1) & (df_filtered_1[\"SALARY\"] &lt;= q99)]\nprint(df_filtered_1[\"is_ai_job\"].value_counts())\n\n\nis_ai_job\nNon-AI    15948\nAI         5273\nName: count, dtype: int64\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nax3=sns.boxplot(\n    data=df_filtered_1, \n    x=\"is_ai_job\", \n    y=\"SALARY\",\n    hue=\"is_ai_job\",\n    legend=False,\n    palette=\"Set3\",\n    width=0.4\n)\n\nfor patch in ax3.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Distribution: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Job Type\", fontsize=9)\nplt.ylabel(\"Salary ($)\", fontsize=9)\nplt.tight_layout()\n#plt.show()\nout_path = \"figures/AI_v_nonAI_salary_boxplot.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\n\n\n\n\nFigure 5: Salary Distribution comparison between AI vs. Non-AI Jobs\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# KDE Plot\nplt.figure(figsize=(8, 5))\nax4=sns.kdeplot(\n    data=df_filtered_1, \n    x=\"SALARY\", \n    hue=\"is_ai_job\", \n    common_norm=False,\n    linewidth=3,\n    palette=\"Set2\",\n    alpha=0.8\n)\n\nfor patch in ax4.patches:\n    patch.set_alpha(0.8)\n\nplt.title(\"Salary Density: AI vs. Non-AI Jobs\")\nplt.xlabel(\"Salary ($)\", fontsize=9)\nplt.ylabel(\"Density\", fontsize=9)\nplt.tight_layout()\n#plt.show()\nout_path = \"figures/AI_v_nonAI_salary_KDE.png\"\nplt.savefig(out_path, dpi=150)\nplt.close()\n\n\n\n\n\nFigure 6: Salary Density comparison between AI vs. Non-AI Jobs\n\n\nKey Insights: The salary comparison between AI and non-AI job postings shows a wage premium associated with roles that require AI-related skill sets. Analyzing 5273 AI roles and 15949 non-AI roles, the box plot shows that AI roles have a higher median salary ($115,000-$120,000) than non-AI roles ($105,000-$110,000). In terms of IQRs - both Q1 and Q3 for AI roles is positioned higher in comparison to non-AI roles. This suggests that roles that require expertise in AI-related skills generally pay better than the rest of the job market.\nThe KDE plot shows that AI roles have a steep upward slope from $20,000-$80,000 with job postings mostly clustering between $80,000-$120,000. This pattern likely means that there are less entry-level roles in AI and most roles are concentrated in the mid-salary range. This suggests that while there aren’t many jobs that reflect entry level salaries, AI-related job postings reflect a higher salary floor driven by a higher barrier of entry."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1 Introduction & Research Rationale",
    "section": "",
    "text": "1 Introduction & Research Rationale\nThe job market in 2024 is undergoing a major transformation with the rise of artificial intelligence and widespread acceptance of long-distance/remote work. This analysis will examine the following key topics to uncover the factors that influence salary trends:\nThe restructuring of compensation due to AI, inflation, and remote work.\nThe traditional pay structure is being reshaped as companies adapt to automation, rising costs of living, and a distributed workforce. These forces are pushing employers to rethink how they value skills, experience, and location.\nGrowing pay disparities across industries and job types in 2024.\nHigh-paying jobs are increasingly concentrated in specific tech-driven sectors and urban hubs. Meanwhile, many essential or service-based roles are seeing slower wage growth, deepening inequality across the workforce.\nShifting salary patterns based on remote flexibility, job type, and sector growth.\nRemote roles now often offer comparable or even higher compensation due to talent shortages and broader applicant pools. Industries like tech and healthcare are setting new standards, while others struggle to keep pace.\n\n\n2 Literature Review Summary\nThe rise of Artificial Intelligence (AI) has reshaped the job market across the US, with demand for AI-related skills increasing dramatically. According to a study by PWC, jobs that require AI specialist skills are growing 3.5 times faster than all other job markets, with skilled AI workers being paid up to 25% more in some sectors (PwC 2024). However, this increase in compensation is not limited to workers specialized in AI. Non-AI roles that require complementary skills such as digital literacy, analytical thinking, and teamwork are also seeing a 5–10% wage increase (Mäkelä and Stephany 2024). According to US job vacancy data from 2018–2024, AI-related jobs are significantly more likely to include non-monetary benefits as part of the compensation package, including parental leave and remote working options (Stephany, Mira, and Bone 2025).\nBeyond the benefit of not having to commute and the ability to work from anywhere, long-distance and remote roles are often sought after by job seekers due to faster wage growth. In a study comparing the pay trends of remote versus in-office workers in the same occupation, remote workers experienced 4.4% faster annual wage growth, especially in professional and technical sectors (Pabilonia and Vernon 2025). Furthermore, workers who transitioned into remote roles with the same employer saw up to 16 percentage points higher wage growth than their counterparts who remained local (Romem 2024). This demonstrates that switching into remote work can lead to significantly higher wage growth, even within the same company and/or job category.\n\n\n\n\n\nReferences\n\nMäkelä, E., and F. Stephany. 2024: “Complement or substitute? How AI increases the demand for human skills,” arXiv preprint arXiv:2412.19754,.\n\n\nPabilonia, S. W., and V. Vernon. 2025: “Remote work, wages, and hours worked in the united states,” SSRN Electronic Journal,.\n\n\nPwC. 2024: “PwC 2024 Global AI Jobs Barometer: Demand, Wages, and Disruption,”\n\n\nRomem, I. 2024: “Long-Distance Work and Compensation,”ADP Research Institute.\n\n\nStephany, F., A. Mira, and M. Bone. 2025: “Beyond pay: AI skills reward more job benefits,” arXiv preprint arXiv:2507.20410,."
  }
]